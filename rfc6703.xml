<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">
<?rfc toc="yes"?>
<?rfc tocompact="yes"?>
<?rfc tocdepth="3"?>
<?rfc tocindent="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes"?>
<?rfc comments="yes"?>
<?rfc inline="yes"?>
<?rfc compact="yes"?>
<?rfc subcompact="no"?>
<?rfc rfcedstyle="yes"?>

<rfc category="info" number="6703"
     ipr="pre5378Trust200902" submissionType="IETF" consensus="yes">
  <front>
    <title abbrev="Reporting Metrics">Reporting IP Network Performance
    Metrics: Different Points of View</title>

    <author fullname="Al Morton" initials="A." surname="Morton">
      <organization>AT&amp;T Labs</organization>

      <address>
        <postal>
          <street>200 Laurel Avenue South</street>
          <city>Middletown</city>
          <region>NJ</region>
          <code>07748</code>
          <country>USA</country>
        </postal>
        <phone>+1 732 420 1571</phone>
        <facsimile>+1 732 368 1192</facsimile>
        <email>acmorton@att.com</email>
        <uri>http://home.comcast.net/~acmacm/</uri>
      </address>
    </author>

    <author fullname="Gomathi Ramachandran" initials="G."
            surname="Ramachandran">
      <organization>AT&amp;T Labs</organization>
      <address>
        <postal>
          <street>200 Laurel Avenue South</street>
          <city>Middletown</city>
          <region>New Jersey</region>
          <code>07748</code>
          <country>USA</country>
        </postal>
        <phone>+1 732 420 2353</phone>
        <email>gomathi@att.com</email>
      </address>
    </author>

    <author fullname="Ganga Maguluri" initials="G." surname="Maguluri">
      <organization>AT&amp;T Labs</organization>
      <address>
        <postal>
          <street>200 Laurel Avenue South</street>
          <city>Middletown</city>
          <region>New Jersey</region>
          <code>07748</code>
          <country>USA</country>
        </postal>
        <phone>+1 732 420 2486</phone>
        <email>gmaguluri@att.com</email>
      </address>
    </author>

    <date month="August" year="2012" />

    <abstract>
      <t>Consumers of IP network performance metrics have many
      different uses in mind. This memo provides "long-term" reporting
      considerations (e.g., hours, days, weeks, or months, as opposed to 10
      seconds), based on analysis of the points of view of two key audiences.
      It describes how these audience categories affect the selection of metric
      parameters and options when seeking information that serves their needs.</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction">
      <t>When designing measurements of IP networks and presenting a result,
      knowledge of the audience is a key consideration. To present a useful
      and relevant portrait of network conditions, one must answer the
      following question:</t>

      <t>"How will the results be used?"</t>

      <t>There are two main audience categories for the report of results:</t>

      <t><list style="numbers">
          <t>Network Characterization - describes conditions in an IP network
          for quality assurance, troubleshooting, modeling, Service Level
          Agreements (SLAs), etc. This point of view (POV) looks inward toward the
          network where the report consumer intends their actions.</t>

          <t>Application Performance Estimation - describes the network
          conditions in a way that facilitates determining effects on user
          applications, and ultimately the users themselves. This
          POV looks outward, toward the user(s), accepting the
          network as is. This report consumer intends to estimate a
          network-dependent aspect of performance or design some aspect
          of an
          application's accommodation of the network. (These are *not*
          application metrics; they are defined at the IP layer.)</t>
        </list>This memo considers how these different POVs affect
      both the measurement design (parameters and options of the metrics) and
      statistics reported when serving the report consumer's needs.</t>

      <t>The IP Performance Metrics (IPPM) Framework <xref target="RFC2330"></xref> and other RFCs
      describing IPPM provide a background for this memo.</t>
    </section>

    <section title="Purpose and Scope">
      <t>The purpose of this memo is to clearly delineate two POVs for using measurements and describe their effects on the test
      design, including the selection of metric parameters and reporting the
      results.</t>

      <t>The scope of this memo primarily covers the test design and reporting
      of the loss and delay metrics <xref target="RFC2680"></xref> <xref
      target="RFC2679"></xref>. It will also discuss the delay variation <xref
      target="RFC3393"></xref> and reordering metrics <xref
      target="RFC4737"></xref> where applicable.</t>

      <t>With capacity metrics growing in relevance to the industry, the memo
      also covers POV and reporting considerations for metrics resulting from
      the Bulk Transfer Capacity Framework <xref target="RFC3148"></xref> and
      Network Capacity Definitions <xref target="RFC5136"></xref>. These memos
      effectively describe two different categories of metrics:</t>

      <t><list style="symbols">
          <t>Restricted <xref target="RFC3148"></xref>: includes restrictions
          of congestion control and the notion of unique data bits delivered,
          and</t>

          <t>Raw <xref target="RFC5136"></xref>: uses a definition of raw
          capacity without the restrictions of data uniqueness or congestion
          awareness.</t>
        </list>It might seem, at first glance, that each of these metrics has an
      obvious audience (raw = network characterization, restricted =
      application performance), but reality is more complex and consistent
      with the overall topic of capacity measurement and reporting. For
      example, TCP is usually used in restricted capacity measurement methods,
      while UDP appears in raw capacity measurement. The raw and restricted
      capacity metrics will be treated in separate sections, although they
      share one common reporting issue: representing variability in capacity
      metric results as part of a long-term report.</t>

      <t>Sampling, or the design of the active packet stream that is the basis
      for the measurements, is also discussed.</t>
    </section>

    <section title="Reporting Results">
      <t>This section gives an overview of recommendations, followed by
      additional considerations for reporting results in the "long term",
      based on the discussion and conclusions of the major sections that
      follow.</t>

      <section title="Overview of Metric Statistics">
        <t>This section gives an overview of reporting recommendations for
        all the metrics considered in this memo.</t>

        <t>The minimal report on measurements must include both loss and delay
        metrics.</t>

        <t>For packet loss, the loss ratio defined in <xref
        target="RFC2680"></xref> is a sufficient starting point -- especially
        the existing guidance for setting the loss threshold waiting time. In
        <xref target="loss-net-char"/>, we have calculated a waiting time --
        51 seconds -- that should be sufficient to differentiate between
        packets that are truly lost or have long finite delays under general
        measurement circumstances. Knowledge of specific conditions can help
        to reduce this threshold, and a waiting time of approximately
        50 seconds is considered to be manageable in practice.</t>

        <t>We note that a loss ratio calculated according to <xref
        target="Y.1540"></xref> would exclude errored packets from the
        numerator. In practice, the difference between these two loss metrics
        is small, if any, depending on whether the last link prior to the
        Destination contributes errored packets.</t>

        <t>For packet delay, we recommend providing both the mean delay and
        the median delay with lost packets designated as undefined (as permitted
        by <xref target="RFC2679"></xref>). Both statistics are based on a
        conditional distribution, and the condition is packet arrival prior to
        a waiting time dT, where dT has been set to take maximum packet
        lifetimes into account, as discussed above for loss. Using a long dT
        helps to ensure that delay distributions are not truncated.</t>

        <t>For Packet Delay Variation (PDV), the minimum delay of the
        conditional distribution should be used as the reference delay for
        computing PDV according to <xref target="Y.1540"></xref> or <xref
        target="RFC5481"></xref> and <xref target="RFC3393"></xref>. A useful
        value to report is a "pseudo" range of delay variation based on
        calculating the difference between a high percentile of delay and the
        minimum delay. For example, the 99.9th percentile minus the minimum
        will give a value that can be compared with objectives in <xref
        target="Y.1541"></xref>.</t>

        <t>For both raw capacity and restricted capacity, reporting the variability in
        a useful way is identified as the main challenge. The min, max, and
        range statistics are suggested along with a ratio of max to min and
        moving averages. In the end, a simple plot of the singleton results
        over time may succeed where summary metrics fail or may serve to confirm
        that the summaries are valid.</t>
      </section>

      <section title="Long-Term Reporting Considerations">
        <t><xref target="IPPM-RPT"></xref> describes methods to
        conduct measurements and report the results on a near-immediate time
        scale (10 seconds, which we consider to be "short-term").</t>

        <t>Measurement intervals and reporting intervals need not be the same
        length. Sometimes, the user is only concerned with the performance
        levels achieved over a relatively long interval of time (e.g., days,
        weeks, or months, as opposed to 10 seconds). However, there can be
        risks involved with running a measurement continuously over a long
        period without recording intermediate results:</t>

        <t><list style="symbols">
            <t>Temporary power failure may cause loss of all results to
            date.</t>

            <t>Measurement system timing synchronization signals may
            experience a temporary outage, causing subsets of measurements to
            be in error or invalid.</t>

            <t>Maintenance on the measurement system or on its connectivity
            to the network under test may be necessary.</t>
          </list>For these and other reasons, such as <list style="symbols">
            <t>the constraint to collect measurements on intervals similar to
            user session length, </t>

            <t>the dual use of measurements in monitoring activities where
            results are needed on a period of a few minutes, or</t>

            <t>the ability to inspect results of a single measurement interval
            for deeper analysis,</t>
          </list>there is value in conducting measurements on intervals that
        are much shorter than the reporting interval.</t>

        <t>There are several approaches for aggregating a series of
        measurement results over time in order to make a statement about the
        longer reporting interval. One approach requires the storage of all
        metric singletons collected throughout the reporting interval, even
        though the measurement interval stops and starts many times.</t>

        <t>Another approach is described in <xref target="RFC5835"></xref> as
        "temporal aggregation". This approach would estimate the results for
        the reporting interval based on combining many individual short-term
        measurement interval statistics to yield a long-term result.
        The result would ideally appear in the
        same form as though a continuous measurement had been conducted. A
        memo addressing the details of temporal aggregation is yet to be
        prepared.</t>

        <t>Yet another approach requires a numerical objective for the metric,
        and the results of each measurement interval are compared with the
        objective. Every measurement interval where the results meet the
        objective contribute to the fraction of time with performance as
        specified. When the reporting interval contains many measurement
        intervals, it is possible to present the results as "metric A was less
        than or equal to objective X during Y% of time".</t>

        <t><list>
        <t>NOTE that numerical thresholds of acceptability are not set in IETF
        performance work and are therefore excluded from the scope of this
        memo.</t>
        </list></t>

        <t>In all measurements, it is important to avoid unintended
        synchronization with network events. This topic is treated in <xref
        target="RFC2330"></xref> for Poisson-distributed inter-packet time
        streams and in <xref target="RFC3432"></xref> for Periodic streams.
        Both avoid synchronization by using random start times.</t>

        <t>There are network conditions where it is simply more useful to
        report the connectivity status of the Source-Destination path, and to
        distinguish time intervals where connectivity can be demonstrated from
        other time intervals (where connectivity does not appear to exist).
        <xref target="RFC2678"></xref> specifies a number of one-way and
        two-way connectivity metrics of increasing complexity. In this memo,
        we recommend that long-term reporting of loss, delay, and other
        metrics be limited to time intervals where connectivity can be
        demonstrated, and that other intervals be summarized as the
        percent of time where connectivity does not appear to exist.
        We note that this same
        approach has been adopted in ITU-T Recommendation <xref
        target="Y.1540"></xref> where performance parameters are only valid
        during periods of service "availability" (evaluated according to a
        function based on packet loss, and sustained periods of loss ratio
        greater than a threshold are declared "unavailable").</t>
      </section>
    </section>

    <section title="Effect of POV on the Loss Metric">
      <t>This section describes the ways in which the loss metric can be tuned
      to reflect the preferences of the two audience categories, or different
      POVs. The waiting time before declaring that a packet is lost -- the loss
      threshold -- is one area where there would appear to be a difference,
      but the ability to post-process the results may resolve it.</t>

      <section title="Loss Threshold">
        <t><xref target="RFC2680">RFC 2680</xref> defines the concept of a
        waiting time for packets to arrive, beyond which they are declared
        lost. The text of the RFC declines to recommend a value, instead
        saying that "good engineering, including an understanding of packet
        lifetimes, will be needed in practice". Later, in the methodology,
        they give reasons for waiting "a reasonable period of time" and
        leave the definition of "reasonable" intentionally vague. Below, we
        estimate a practical bound on waiting time.</t>

        <section title="Network Characterization" anchor="loss-net-char">
          <t>Practical measurement experience has shown that unusual network
          circumstances can cause long delays. One such circumstance is when
          routing loops form during IGP re-convergence following a failure or
          drastic link cost change. Packets will loop between two routers
          until new routes are installed or until the IPv4 Time-to-Live (TTL)
          field (or the IPv6 Hop Limit) decrements to zero. Very long delays
          on the order of several seconds have been measured <xref
          target="Casner"></xref> <xref target="Cia03"></xref>.</t>

          <t>Therefore, network characterization activities prefer a long
          waiting time in order to distinguish these events from other causes
          of loss (such as packet discard at a full queue, or tail drop). This
          way, the metric design helps to distinguish more reliably between
          packets that might yet arrive and those that are no longer
          traversing the network.</t>

          <t>It is possible to calculate a worst-case waiting time, assuming
          that a routing loop is the cause. We model the path between Source
          and Destination as a series of delays in links (t) and queues (q),
          as these are the dominant contributors to delay (in active
          measurement, the Source and Destination hosts contribute minimal
          delay). The normal path delay, D, across n queues (where TTL is
          decremented at a node with a queue) and n+1 links without
          encountering a loop, is<figure anchor="eqD"
              title="Normal Path Delay">
              <preamble></preamble>

              <artwork align="center"><![CDATA[Path model with n=5
  Source --- q1 --- q2 --- q3 --- q4 --- q5 --- Destination
         t0     t1     t2     t3     t4     t5

                           n
                          ---
                          \
                D = t  +   >  (t  +  q)
                     0    /     i     i
                          ---
                         i = 1]]></artwork>

              <postamble></postamble>
            </figure></t>

          <t>and the time spent in the loop with L queues is</t>

          <t><figure anchor="eqR" title="Delay Due to Rotations in a Loop">
              <preamble></preamble>

              <artwork align="center"><![CDATA[Path model with n=5 and L=3
Time in one loop = (qx+tx + qy+ty + qz+tz)

                       qy -- qz   
                        |  ?/exit?   
                       qx--/\
  Src --- q1 --- q2 ---/    q3 --- q4 --- q5 --- Dst
      t0     t1     t2         t3     t4     t5

           j + L-1
            ---
            \                          (TTL - n)
     R = C   >  (t  +  q)  where C   = ---------
            /     i     i         max      L
            ---
            i=j           ]]></artwork>

              <postamble></postamble>
            </figure></t>

          <t>where n is the total number of queues in the non-loop path (with
          n+1 links), j is the queue number where the loop begins, C is the
          number of times a packet circles the loop, and TTL is the packet's
          initial Time-to-Live value at the Source (or Hop Count in IPv6).</t>

          <t>If we take the delays of all links and queues as 100 ms each, the
          TTL=255, the number of queues n=5, and the queues in the loop L=4,
          then using C_max:</t>

          <figure><artwork><![CDATA[
   D = 1.1 seconds and R ~= 50 seconds, and D + R ~= 51.1 seconds
          ]]></artwork></figure>

          <t>We note that the link delays of 100 ms would span most continents,
          and a constant queue length of 100 ms is also very generous. When a
          loop occurs, it is almost certain to be resolved in 10 seconds or
          less. The value calculated above is an upper limit for almost any
          real-world circumstance.</t>

          <t>A waiting time threshold parameter, dT, set consistent with this calculation, would not truncate the delay distribution (possibly
          causing a change in its mathematical properties), because the
          packets that might arrive have been given sufficient time to
          traverse the network.</t>

          <t>It is worth noting that packets that are stored and deliberately
          forwarded at a much later time constitute a replay attack on the
          measurement system and are beyond the scope of normal performance
          reporting.</t>
        </section>

        <section title="Application Performance">
          <t>Fortunately, application performance estimation activities are
          not adversely affected by the long estimated limit on waiting time,
          because most applications will use shorter time thresholds. Although
          the designer's tendency might be to set the loss threshold at a
          value equivalent to a particular application's threshold, this
          specific threshold can be applied when post-processing the
          measurements. A shorter waiting time can be enforced by locating
          packets with delays longer than the application's threshold and
          re-designating such packets as lost. Thus, the measurement system
          can use a single loss waiting time and support both application and
          network performance POVs simultaneously.</t>
        </section>
      </section>

      <section title="Errored Packet Designation">
        <t>RFC 2680 designates packets that arrive containing errors as lost
        packets. Many packets that are corrupted by bit errors are discarded
        within the network and do not reach their intended destination.</t>

        <t>This is consistent with applications that would check the payload
        integrity at higher layers and discard the packet. However, some
        applications prefer to deal with errored payloads on their own, and
        even a corrupted payload is better than no packet at all.</t>

        <t>To address this possibility, and to make network characterization
        more complete, distinguishing between packets that
        do not arrive (lost) and errored packets that arrive (conditionally
        lost) is recommended.</t>
      </section>

      <section title="Causes of Lost Packets">
        <t>Although many measurement systems use a waiting time to determine
        whether or not a packet is lost, most of the waiting is in vain. The
        packets are no longer traversing the network and have not reached
        their destination.</t>

        <t>There are many causes of packet loss, including the following:</t>

        <t><list style="numbers">
            <t>Queue drop, or discard</t>

            <t>Corruption of the IP header, or other essential header
            information</t>

            <t>TTL expiration (or use of a TTL value that is too small)</t>

            <t>Link or router failure</t>

            <t>Layers below the Source-to-Destination IP layer can discard
            packets that fail error checking, and link-layer checksums often
            cover the entire packet</t>
          </list>It is reasonable to consider a packet that has not arrived
        after a large amount of time to be lost (due to one of the causes
        above) because packets do not "live forever" in the network or have
        infinite delay.</t>
      </section>

      <section title="Summary for Loss">
        <t>Given that measurement post-processing is possible (even encouraged
        in the definitions of IPPM), measurements of loss can easily
        serve both POVs:</t>

        <t><list style="symbols">
            <t>Use a long waiting time to serve network characterization and
            revise results for specific application delay thresholds as
            needed.</t>

            <t>Distinguish between errored packets and lost packets when
            possible to aid network characterization, and combine the results
            for application performance if appropriate.</t>
          </list></t>
      </section>
    </section>

    <section title="Effect of POV on the Delay Metric">
      <t>This section describes the ways in which the delay metric can be
      tuned to reflect the preferences of the two consumer categories, or
      different POVs.</t>

      <section title="Treatment of Lost Packets">
        <t>The delay metric <xref target="RFC2679"></xref> specifies the
        treatment of packets that do not successfully traverse the network:
        their delay is undefined.</t>

        <t><list>
        <t>&gt;&gt;The *Type-P-One-way-Delay* from Src to Dst at T is
        undefined (informally, infinite)&lt;&lt; means that Src sent the first
        bit of a Type-P packet to Dst at wire-time T and that Dst did not
        receive that packet.</t>
        </list></t>

        <t>It is an accepted but informal practice to assign infinite delay
        to lost packets. We next look at how these two different treatments
        align with the needs of measurement consumers who wish to characterize
        networks or estimate application performance. Also, we look at the way
        that lost packets have been treated in other metrics: delay variation
        and reordering.</t>

        <section title="Application Performance">
          <t>Applications need to perform different functions, dependent on
          whether or not each packet arrives within some finite tolerance. In
          other words, a receiver's packet processing takes only one of two
          alternative directions (a "fork" in the road):</t>

          <t><list style="symbols">
              <t>Packets that arrive within expected tolerance are handled by
              removing headers, restoring smooth delivery timing (as in a
              de-jitter buffer), restoring sending order, checking for errors
              in payloads, and many other operations.</t>

              <t>Packets that do not arrive when expected lead to attempted
              recovery from the apparent loss, such as retransmission
              requests, loss concealment, or forward error correction to
              replace the missing packet.</t>
            </list>So, it is important to maintain a distinction between
          packets that actually arrive and those that do not. Therefore, it
          is preferable to leave the delay of lost packets undefined and to
          characterize the delay distribution as a conditional distribution
          (conditioned on arrival).</t>
        </section>

        <section title="Network Characterization" anchor="delay-net-char">
          <t>In this discussion, we assume that both loss and delay metrics
          will be reported for network characterization (at least).</t>

          <t>Assume that packets that do not arrive are reported as lost, usually
          as a fraction of all sent packets. If these lost packets are
          assigned an undefined delay, then the network's inability to deliver
          them (in a timely way) is relegated only in the loss metric when we
          report statistics on the delay distribution conditioned on the event
          of packet arrival (within the loss waiting time threshold). We can
          say that the delay and loss metrics are orthogonal in that they
          convey non-overlapping information about the network under test.
          This is a valuable property whose absence is discussed below.</t>

          <t>However, if we assign infinite delay to all lost packets,
          then</t>

          <t><list style="symbols">
              <t>The delay metric results are influenced both by packets that
              arrive and those that do not.</t>

              <t>The delay singleton and the loss singleton do not appear to
              be orthogonal (delay is finite when loss=0; delay is infinite
              when loss=1).</t>

              <t>The network is penalized in both the loss and delay metrics,
              effectively double-counting the lost packets.</t>
            </list></t>

          <t>As further evidence of overlap, consider the Cumulative
          Distribution Function (CDF) of delay when the value "positive
          infinity" is assigned to all lost packets. <xref
          target="CDF"></xref> shows a CDF where a small fraction of packets
          are lost.</t>

          <t><figure anchor="CDF"
              title="Cumulative Distribution Function for Delay When Loss = +Infinity">
              <preamble></preamble>

              <artwork align="center"><![CDATA[ 1 | - - - - - - - - - - - - - - - - - -+
   |                                    |
   |          _..----''''''''''''''''''''
   |      ,-''
   |    ,'
   |   /                         Mass at
   |  /                          +infinity
   | /                           = fraction
   ||                            lost
   |/
 0 |_____________________________________

   0               Delay               +o0]]></artwork>

              <postamble></postamble>
            </figure></t>

          <t>We note that a delay CDF that is conditioned on packet arrival
          would not exhibit this apparent overlap with loss.</t>

          <t>Although infinity is a familiar mathematical concept, it is
          somewhat disconcerting to see any time-related metric reported as
          infinity. Questions are bound to arise and tend to detract from the
          goal of informing the consumer with a performance report.</t>
        </section>

        <section title="Delay Variation">
          <t><xref target="RFC3393"></xref> excludes lost packets from
          samples, effectively assigning an undefined delay to packets that do
          not arrive in a reasonable time. Section 4.1 of <xref
          target="RFC3393"></xref> describes this specification and its
          rationale (ipdv = inter-packet delay variation in the quote
          below).</t>

          <t><list>
          <t>The treatment of lost packets as having "infinite" or
          "undefined" delay complicates the derivation of statistics for ipdv.
          Specifically, when packets in the measurement sequence are lost,
          simple statistics such as sample mean cannot be computed. One
          possible approach to handling this problem is to reduce the event
          space by conditioning. That is, we consider conditional statistics;
          namely we estimate the mean ipdv (or other derivative statistic)
          conditioned on the event that selected packet pairs arrive at the
          Destination (within the given timeout). While this itself is not
          without problems (what happens, for example, when every other packet
          is lost), it offers a way to make some (valid) statements about
          ipdv, at the same time avoiding events with undefined outcomes.</t>
          </list></t>

          <t>We note that the argument above applies to all forms of packet
          delay variation that can be constructed using the "selection
          function" concept of <xref target="RFC3393"></xref>. In recent work,
          the two main forms of delay variation metrics have been compared, and
          the results are summarized in <xref target="RFC5481"></xref>.</t>
        </section>

        <section title="Reordering">
          <t><xref target="RFC4737"></xref> defines metrics that are based on
          evaluation of packet arrival order and that include a waiting time
          before declaring that a packet is lost (to exclude the packet
          from further processing).</t>

          <t>If packets are assigned a delay value, then the reordering metric
          would declare any packets with infinite delay to be reordered,
          because their sequence numbers will surely be less than the "Next
          Expected" threshold when (or if) they arrive. But this practice
          would fail to maintain orthogonality between the reordering metric
          and the loss metric. Confusion can be avoided by designating the
          delay of non-arriving packets as undefined and reserving delay
          values only for packets that arrive within a sufficiently long
          waiting time.</t>
        </section>
      </section>

      <section title="Preferred Statistics">
        <t>Today in network characterization, the sample mean is one statistic
        that is almost ubiquitously reported. It is easily computed and
        understood by virtually everyone in this audience category. Also, the
        sample is usually filtered on packet arrival, so that the mean is
        based on a conditional distribution.</t>

        <t>The median is another statistic that summarizes a distribution,
        having somewhat different properties from the sample mean. The median
        is stable in distributions with a few outliers or without them.
        However, the median's stability prevents it from indicating when a
        large fraction of the distribution changes value. &nbsp;50% or more
        values would need to change for the median to capture the change.</t>

        <t>Both the median and sample mean have difficulty with bimodal
        distributions. The median will reside in only one of the modes, and
        the mean may not lie in either mode range. For this and other reasons,
        additional statistics such as the minimum, maximum, and 95th percentile
        have value when summarizing a distribution.</t>

        <t>When both the sample mean and median are available, a comparison
        will sometimes be informative, because these two statistics are equal
        only under unusual circumstances, such as when the delay distribution
        is perfectly symmetrical.</t>

        <t>Also, these statistics are generally useful from the application
        performance POV, so there is a common set that should satisfy
        audiences.</t>

        <t>Plots of the delay distribution may also be useful when
        single-value statistics indicate that new conditions are present. An
        empirically derived probability distribution function will usually
        describe multiple modes more efficiently than any other form of
        result.</t>
      </section>

      <section title="Summary for Delay">
        <t>From the perspectives of</t>

        <t><list style="numbers">
            <t>application/receiver analysis, where subsequent processing
            depends on whether the packet arrives or times out,</t>

            <t>straightforward network characterization without
            double-counting defects, and</t>

            <t>consistency with delay variation and reordering metric
            definitions,</t>
          </list></t>

        <t>the most efficient practice is to distinguish between packets that
        are truly lost and those that are delayed packets with a sufficiently
        long waiting time, and to designate the delay of non-arriving packets
        as undefined.</t>
      </section>
    </section>

    <section title="Reporting Raw Capacity Metrics">
      <t>Raw capacity refers to the metrics defined in <xref
      target="RFC5136"></xref>, which do not include restrictions such as data
      uniqueness or flow-control response to congestion.</t>

      <t>The metrics considered are IP-layer capacity, utilization (or used
      capacity), and available capacity, for individual links and complete
      paths. These three metrics form a triad: knowing one metric constrains
      the other two (within their allowed range), and knowing two determines
      the third. The link metrics have another key aspect in common: they are
      single-measurement-point metrics at the egress of a link. The path
      capacity and available capacity are derived by examining the set of
      single-point link measurements and taking the minimum value.</t>

      <section title="Type-P Parameter">
        <t>The concept of "packets of Type-P" is defined in <xref
        target="RFC2330"></xref>. The Type-P categorization has critical
        relevance in all forms of capacity measurement and reporting. The
        ability to categorize packets based on header fields for assignment to
        different queues and scheduling mechanisms is now commonplace. When
        unused resources are shared across queues, the conditions in all
        packet categories will affect capacity and related measurements. This
        is one source of variability in the results that all audiences would
        prefer to see reported in a useful and easily understood way.</t>

        <t>Communication of Type-P within the One-Way Active Measurement
        Protocol (OWAMP) and the Two-Way Active Measurement Protocol (TWAMP)
        is essentially confined to the Diffserv Code Point (DSCP) <xref
        target="RFC4656"></xref>. DSCP is the most common qualifier for
        Type-P.</t>

        <t>Each audience will have a set of Type-P qualifications and value
        combinations that are of interest. Measurements and reports should
        have the flexibility to report per-type and aggregate performance.</t>
      </section>

      <section title="A priori Factors" anchor="raw-a-priori-factors">
        <t>The audience for network characterization may have detailed
        information about each link that comprises a complete path (due to
        ownership, for example), or some of the links in the path but not
        others, or none of the links.</t>

        <t>There are cases where the measurement audience only has information
        on one of the links (the local access link) and wishes to measure one
        or more of the raw capacity metrics. This scenario is quite common
        and has spawned a substantial number of experimental measurement
        methods (e.g., http://www.caida.org/tools/taxonomy/). Many of these
        methods respect that their users want a result fairly quickly and in
        one trial. Thus, the measurement interval is kept short (a few seconds
        to a minute). For long-term reporting, a sample of short-term results
        needs to be summarized.</t>
      </section>

      <section title="IP-Layer Capacity">
        <t>For links, this metric's theoretical maximum value can be
        determined from the physical-layer bit rate and the bit rate reduction
        due to the layers between the physical layer and IP. When measured,
        this metric takes additional factors into account, such as the ability
        of the sending device to process and forward traffic under various
        conditions. For example, the arrival of routing updates may spawn
        high-priority processes that reduce the sending rate temporarily.
        Thus, the
        measured capacity of a link will be variable, and the maximum capacity
        observed applies to a specific time, time interval, and other relevant
        circumstances.</t>

        <t>For paths composed of a series of links, it is easy to see how the
        sources of variability for the results grow with each link in the
        path. Variability of results will be discussed in more detail below.</t>
      </section>

      <section title="IP-Layer Utilization">
        <t>The ideal metric definition of link utilization <xref
        target="RFC5136"></xref> is based on the actual usage (bits
        successfully received during a time interval) and the maximum capacity
        for the same interval.</t>

        <t>In practice, link utilization can be calculated by counting the
        IP-layer (or other layer) octets received over a time interval and
        dividing by the theoretical maximum number of octets that could have been
        delivered in the same interval. A commonly used time interval is 5
        minutes, and this interval has been sufficient to support network
        operations and design for some time. &nbsp;5 minutes is somewhat long
        compared with the expected download time for web pages but short with
        respect to large file transfers and TV program viewing. It is fair to
        say that considerable variability is concealed by reporting a single
        (average) utilization value for each 5-minute interval. Some
        performance management systems have begun to make 1-minute averages
        available.</t>

        <t>There is also a limit on the smallest useful measurement interval.
        Intervals on the order of the serialization time for a single Maximum
        Transmission Unit (MTU) packet will observe on/off behavior and report
        100% or 0%. The smallest interval needs to be some multiple of MTU
        serialization time for averaging to be effective.</t>
      </section>

      <section title="IP-Layer Available Capacity">
        <t>The available capacity of a link can be calculated using the
        capacity and utilization metrics.</t>

        <t>When available capacity of a link or path is estimated through some
        measurement technique, the following parameters should be
        reported:</t>

        <t><list style="symbols">
            <t>Name and reference to the exact method of measurement</t>

            <t>IP packet length, octets (including IP header)</t>

            <t>Maximum capacity that can be assessed in the measurement
            configuration</t>

            <t>Time duration of the measurement</t>

            <t>All other parameters specific to the measurement method</t>
          </list>Many methods of available capacity measurement have a maximum
        capacity that they can measure, and this maximum may be less than the
        actual available capacity of the link or path. Therefore, it is
        important to know the capacity value beyond which there will be no
        measured improvement.</t>

        <t>The application performance estimation audience
        may have a desired target capacity
        value and simply wish to assess whether there is sufficient available
        capacity. This case simplifies the measurement of link and path capacity
        to some degree, as long as the measurable maximum exceeds the target
        capacity.</t>

      </section>

      <section title="Variability in Utilization and Available Capacity">
        <t>As with most metrics and measurements, assessing the consistency or
        variability in the results gives the user an intuitive feel for the
        degree (or confidence) that any one value is representative of other
        results, or the spread of the underlying distribution of the singleton
        measurements.</t>

        <t>How can utilization be measured and summarized to describe the
        potential variability in a useful way?</t>

        <t>How can the variability in available capacity estimates be
        reported, so that the confidence in the results is also conveyed?</t>

        <t>We suggest some methods below.</t>

        <section title="General Summary of Variability" anchor="gen-summary">
          <t>With a set of singleton utilization or available capacity
          estimates, each representing a time interval needed to ascertain the
          estimate, we seek to describe the variation over the set of
          singletons as though reporting summary statistics of a distribution.
          Three useful summary statistics are</t>

          <t><list style="symbols">
              <t>Minimum,</t>

              <t>Maximum, and</t>

              <t>Range</t>
            </list></t>

          <t>An alternate way to represent the range is as a ratio of maximum to
          minimum value. This enables an easily understandable statistic to
          describe the range observed. For example, when maximum = 3*minimum,
          then the max/min ratio is 3, and users may see variability of this
          order. On the other hand, capacity estimates with a max/min ratio
          near 1 are quite consistent and near the central measure or
          statistic reported.</t>

          <t>For an ongoing series of singleton estimates, a moving average
          of n estimates may provide a single value estimate to more easily
          distinguish substantial changes in performance over time. For
          example, in a window of n singletons observed in time interval t, a
          percentage change of x% is declared to be a substantial change and
          reported as an exception.</t>

          <t>Often, the most informative summary of the results is a two-axis
          plot rather than a table of statistics, where time is plotted on the
          x-axis and the singleton value on the y-axis. The time-series plot
          can illustrate sudden changes in an otherwise stable range, identify
          bi-modality easily, and help quickly assess correlation with other
          time-series. Plots of frequency of the singleton values are likewise
          useful tools to visualize the variation.</t>
        </section>
      </section>
    </section>

    <section title="Reporting Restricted Capacity Metrics">
      <t>Restricted capacity refers to the metrics defined in <xref
      target="RFC3148"></xref>, which include criteria of data uniqueness or
      flow-control response to congestion.</t>

      <t>One primary metric considered is Bulk Transfer Capacity (BTC) for
      complete paths. <xref target="RFC3148"></xref> defines BTC as</t>

      <t><list><t>BTC = data_sent / elapsed_time</t></list></t>

      <t>for a connection with congestion-aware flow control, where data_sent
      is the total number of unique payload bits (no headers).</t>

      <t>We note that this definition *differs* from the raw capacity
      definition in Section 2.3.1 of <xref target="RFC5136"></xref>, where
      IP-layer capacity *includes* all bits in the IP header and payload. This
      means that restricted capacity BTC is already operating at a
      disadvantage when compared to the raw capacity at layers below TCP.
      Further, there are cases where one IP layer is encapsulated in another
      IP layer or other form of tunneling protocol, designating more and more
      of the fundamental transport capacity as header bits that are pure
      overhead to the BTC measurement.</t>

      <t>We also note that raw and restricted capacity metrics are not
      orthogonal in the sense defined in <xref target="delay-net-char"/>
      above. The information
      they convey about the network under test is certainly overlapping, but
      they reveal two different and important aspects of performance.</t>

      <t>When thinking about the triad of raw capacity metrics, BTC is most
      akin to the "IP-Type-P Available Path Capacity", at least in the eyes of
      a network user who seeks to know what transmission performance a path
      might support.</t>

      <section title="Type-P Parameter and Type-C Parameter">
        <t>The concept of "packets of Type-P" is defined in <xref
        target="RFC2330"></xref>. The considerations for restricted capacity
        are identical to the raw capacity section on this topic, with the
        addition that the various fields and options in the TCP header must be
        included in the description.</t>

        <t>The vast array of TCP flow-control options are not well captured by
        Type-P, because they do not exist in the TCP header bits. Therefore,
        we introduce a new notion here: TCP Configuration of "Type-C". The
        elements of Type-C describe all of the settings for TCP options and
        congestion control algorithm variables, including the main form of
        congestion control in use. Readers should consider the parameters and
        variables of <xref target="RFC3148"></xref> and <xref
        target="RFC6349"></xref> when constructing Type-C.</t>
      </section>

      <section title="A Priori Factors">
        <t>The audience for network characterization may have detailed
        information about each link that comprises a complete path (due to
        ownership, for example), or some of the links in the path but not
        others, or none of the links.</t>

        <t>There are cases where the measurement audience only has information
        on one of the links (the local access link) and wishes to measure one
        or more BTC metrics. The discussion in <xref
target="raw-a-priori-factors"/> applies here as well.</t>
      </section>

      <section title="Measurement Interval">
        <t>There are limits on a useful measurement interval for BTC. Three
        factors that influence the interval duration are listed below:<list
            style="numbers">
            <t>Measurements may choose to include or exclude the 3-way
            handshake of TCP connection establishment, which requires at least
            1.5 * RTT (round-trip time) and contains both the delay of
            the path and the host
            processing time for responses. However, user experience includes
            the 3-way handshake for all new TCP connections.</t>

            <t>Measurements may choose to include or exclude Slow-Start,
            preferring instead to focus on a portion of the transfer that
            represents "equilibrium" (which needs to be defined for particular
            circumstances if used). However, user experience includes the
            Slow-Start for all new TCP connections.</t>

            <t>Measurements may choose to use a fixed block of data to
            transfer, where the size of the block has a relationship to the
            file size of the application of interest. This approach yields
            variable size measurement intervals, where a path with faster BTC
            is measured for less time than a path with slower BTC, and this
            has implications when path impairments are time-varying, or
            transient. Users are likely to turn their immediate attention
            elsewhere when a very large file must be transferred; thus, they do
            not directly experience such a long transfer -- they see the
            result (success or failure) and possibly an objective measurement of
            the transfer time (which will likely include the 3-way handshake,
            Slow-Start, and application file management processing time as
            well as the BTC).</t>
          </list></t>

        <t>Individual measurement intervals may be short or long, but there is
        a need to report the results on a long-term basis that captures the
        BTC variability experienced between each interval. Consistent BTC is a
        valuable commodity along with the value attained.</t>
      </section>

      <section title="Bulk Transfer Capacity Reporting">
        <t>When BTC of a link or path is estimated through some measurement
        technique, the following parameters should be reported:</t>

        <t><list style="symbols">
            <t>Name and reference to the exact method of measurement</t>

            <t>Maximum Transmission Unit (MTU)</t>

            <t>Maximum BTC that can be assessed in the measurement
            configuration</t>

            <t>Time and duration of the measurement</t>

            <t>Number of BTC connections used simultaneously</t>

            <t>*All* other parameters specific to the measurement method,
            especially the congestion control algorithm in use</t>
          </list></t>

        <t>See also <xref target="RFC6349"></xref>.</t>

        <t>Many methods of BTC measurement have a maximum
        capacity that they can measure, and this maximum may be less than the
        available capacity of the link or path. Therefore, it is important to
        specify the measured BTC value beyond which there will be no measured
        improvement.</t>

        <t>The application performance estimation audience
        may have a desired target capacity
        value and simply wish to assess whether there is sufficient BTC. This
        case simplifies the measurement of link and path capacity to some degree,
        as long as the measurable maximum exceeds the target capacity.</t>
      </section>

      <section title="Variability in Bulk Transfer Capacity">
        <t>As with most metrics and measurements, assessing the consistency or
        variability in the results gives the user an intuitive feel for the
        degree (or confidence) that any one value is representative of other
        results, or the underlying distribution from which these singleton
        measurements have come.</t>

        <t>With two questions looming --</t>

        <t><list style="numbers">
            <t>What ways can BTC be measured and summarized to describe the
            potential variability in a useful way?</t>

            <t>How can the variability in BTC estimates be reported, so that
            the confidence in the results is also conveyed?</t>
          </list></t>

        <t>-- we suggest the methods listed in <xref target="gen-summary"/>
        above, and the additional
        results presentations given in <xref target="RFC6349"></xref>.</t>
      </section>
    </section>

    <section title="Reporting on Test Streams and Sample Size">
      <t>This section discusses two key aspects of measurement that are
      sometimes omitted from the report: the description of the test stream on
      which the measurements are based, and the sample size.</t>

      <section title="Test Stream Characteristics">
        <t>Network characterization has traditionally used Poisson-distributed
        inter-packet spacing, as this provides an unbiased sample. The average
        inter-packet spacing may be selected to allow observation of specific
        network phenomena. Other test streams are designed to sample some
        property of the network, such as the presence of congestion, link
        bandwidth, or packet reordering.</t>

        <t>If measuring a network in order to make inferences about
        applications or receiver performance, then there are usually
        efficiencies derived from a test stream that has similar
        characteristics to the sender. In some cases, it is essential to
        synthesize the sender stream, as with BTC
        estimates. In other cases, it may be sufficient to sample with a
        "known bias", e.g., a Periodic stream to estimate real-time
        application performance.</t>
      </section>

      <section title="Sample Size">
        <t>Sample size is directly related to the accuracy of the results and
        plays a critical role in the report. Even if only the sample size (in
        terms of number of packets) is given for each value or summary
        statistic, it imparts a notion of the confidence in the result.</t>

        <t>In practice, the sample size will be selected taking both
        statistical and practical factors into account. Among these factors
        are the following:</t>

        <t><list style="numbers">
            <t>The estimated variability of the quantity being measured.</t>

            <t>The desired confidence in the result (although this may be
            dependent on assumption of the underlying distribution of the
            measured quantity).</t>

            <t>The effects of active measurement traffic on user traffic.</t>
          </list>A sample size may sometimes be referred to as "large". This
        is a relative and qualitative term. It is preferable to describe what
        one is attempting to achieve with his sample. For example, stating
        an implication may be helpful: this sample is large enough that a
        single outlying value at ten times the "typical" sample mean (the mean
        without the outlying value) would influence the mean by no more
        than&nbsp;X.</t>

        <t>The Appendix of <xref target="RFC2330"></xref> indicates that a
        sample size of 128 singletons worked well for goodness-of-fit testing,
        while a much larger size (8192 singletons) almost always failed.</t>
      </section>
    </section>

    <section anchor="Security" title="Security Considerations">
      <t>The security considerations that apply to any active measurement of
      live networks are relevant here as well. See the Security Considerations
      section of <xref target="RFC4656"></xref> for mandatory-to-implement
      security features that intend to mitigate attacks.</t>

      <t>Measurement systems conducting long-term measurements are more
      exposed to threats as a by-product of ports open longer to perform their
      task, and more easily detected measurement activity on those ports.
      Further, use of long packet waiting times affords an attacker a better
      opportunity to prepare and launch a replay attack.</t>
    </section>

    <section anchor="Acknowledgements" title="Acknowledgements">
      <t>The authors thank Phil Chimento for his suggestion to employ
      conditional distributions for delay, Steve Konish Jr. for his careful
      review and suggestions, Dave McDysan and Don McLachlan for useful
      comments based on their long experience with measurement and reporting,
      Daniel Genin for his observation of non-orthogonality between raw and
      restricted capacity metrics (and for noticing our previous omission
      of this fact), and Matt Zekauskas for suggestions on organizing the
      memo for easier consumption.</t>

    </section>
  </middle>

  <back>
    <references title="Normative References">

      <?rfc include='reference.RFC.2330'?>

      <?rfc include='reference.RFC.2679'?>

      <?rfc include='reference.RFC.2680'?>

      <?rfc include='reference.RFC.2678'?>

      <?rfc include='reference.RFC.3148'?>

      <?rfc include='reference.RFC.3393'?>

      <?rfc include='reference.RFC.4656'?>

      <?rfc include='reference.RFC.3432'?>

      <?rfc include='reference.RFC.4737'?>

      <?rfc include='reference.RFC.5136'?>
    </references>

    <references title="Informative References">
      <reference anchor="Casner" target="http://www.nanog.org/presentations/archive/index.php">
        <front>
          <title>A Fine-Grained View of High-Performance Networking</title>
          <author initials="S" surname="Casner" fullname="S. Casner">
            <organization></organization>
          </author>
          <author initials="C" surname="Alaettinoglu"
fullname="C. Alaettinoglu">
            <organization></organization>
          </author>
          <author initials="C" surname="Kuan" fullname="C. Kuan">
            <organization></organization>
          </author>
          <date month="May 20-22" year="2001" />
        </front>
       <seriesInfo name="NANOG&nbsp;22" value="Conf."/>
      </reference>

      <reference anchor="Cia03">
        <front>
          <title>Standardized Active Measurements on a Tier 1 IP Backbone</title>
          <author initials="L" surname="Ciavattone" fullname="L. Ciavattone">
            <organization></organization>
          </author>
          <author initials="A" surname="Morton" fullname="A. Morton">
            <organization></organization>
          </author>
          <author initials="G" surname="Ramachandran" fullname="G. Ramachandran">
            <organization></organization>
          </author>
          <date month="June" year="2003" />
        </front>
       <seriesInfo name="IEEE Communications Magazine, Vol. 41 No.&nbsp;6," value="pp. 90-97"/>
      </reference>

      <reference anchor="Y.1540">
        <front>
          <title>Internet protocol data communication service - IP packet
          transfer and availability performance parameters</title>
        <author>
         <organization>International Telecommunication Union</organization>
        </author>
          <date month="March" year="2011" />
        </front>
       <seriesInfo name="ITU-T Recommendation" value="Y.1540"/>
      </reference>

      <reference anchor="Y.1541">
        <front>
          <title>Network performance objectives for IP-based services</title>
          <author>
            <organization>International Telecommunication Union</organization>
          </author>
          <date month="December" year="2011" />
        </front>
       <seriesInfo name="ITU-T Recommendation" value="Y.1541"/>
      </reference>

<reference anchor='RFC5835'>
<front>
<title>Framework for Metric Composition</title>
<author initials='A.' surname='Morton' fullname='A. Morton' role="editor">
<organization /></author>
<author initials='S.' surname='Van den Berghe' fullname='S. Van den Berghe' role="editor">
<organization /></author>
<date year='2010' month='April' />
</front>
<seriesInfo name='RFC' value='5835' />
</reference>

<!-- draft-ietf-ippm-reporting (Expired (IESG: Dead)) -->
<reference anchor='IPPM-RPT'>
<front>
<title>Reporting IP Performance Metrics to Users</title>
<author initials='S' surname='Shalunov' fullname='Stanislav Shalunov'>
    <organization />
</author>
<author initials='M' surname='Swany' fullname='Martin Swany'>
    <organization />
</author>
<date month='March' year='2011' />
</front>
<seriesInfo name='Work in' value='Progress' />
</reference>

      <?rfc include='reference.RFC.5481'?>

      <?rfc include='reference.RFC.6349'?>

    </references>
  </back>
</rfc>
