<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">
<?rfc toc="yes"?>
<?rfc tocompact="yes"?>
<?rfc tocdepth="3"?>
<?rfc tocindent="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes"?>
<?rfc compact="yes"?>
<?rfc subcompact="no"?>
<rfc category="info" consensus="yes" ipr="trust200902" number="8172"
     submissionType="IETF">
  <front>
    <title
    abbrev="Benchmarking VNFs and Related Infrastructure">Considerations for
    Benchmarking Virtual Network Functions and Their Infrastructure</title>

    <author fullname="Al Morton" initials="A." surname="Morton">
      <organization>AT&amp;T Labs</organization>

      <address>
        <postal>
          <street>200 Laurel Avenue South</street>

          <city>Middletown</city>

          <region>NJ</region>

          <code>07748</code>

          <country>United States of America</country>
        </postal>

        <phone>+1 732 420 1571</phone>

        <facsimile>+1 732 368 1192</facsimile>

        <email>acmorton@att.com</email>

        <uri/>
      </address>
    </author>

    <date month="July" year="2017"/>

    <abstract>
      <t>The Benchmarking Methodology Working Group has traditionally
      conducted laboratory characterization of dedicated physical
      implementations of internetworking functions. This memo investigates
      additional considerations when network functions are virtualized and
      performed in general-purpose hardware.</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction">
      <t>The Benchmarking Methodology Working Group (BMWG) has traditionally
      conducted laboratory characterization of dedicated physical
      implementations of internetworking functions (or physical network
      functions (PNFs)). The black-box benchmarks of throughput, latency,
      forwarding rates, and others have served our industry for many years.
      <xref target="RFC1242"/> and <xref target="RFC2544"/> are the
      cornerstones of the work.</t>

      <t>A set of service provider and vendor development goals has emerged:
      reduce costs while increasing flexibility of network devices and
      drastically reduce deployment time. Network Function Virtualization
      (NFV) has the promise to achieve these goals and therefore has garnered
      much attention. It now seems certain that some network functions will be
      virtualized following the success of cloud computing and virtual
      desktops supported by sufficient network path capacity, performance, and
      widespread deployment; many of the same techniques will help achieve
      NFV.</t>

      <t>In the context of Virtual Network Functions (VNFs), the supporting
      Infrastructure requires general-purpose computing systems, storage
      systems, networking systems, virtualization support systems (such as
      hypervisors), and management systems for the virtual and physical
      resources. There will be many potential suppliers of Infrastructure
      systems and significant flexibility in configuring the systems for best
      performance. There are also many potential suppliers of VNFs, adding to
      the combinations possible in this environment. The separation of
      hardware and software suppliers has a profound implication on
      benchmarking activities: much more of the internal configuration of the
      black-box Device Under Test (DUT) must now be specified and reported
      with the results, to foster both repeatability and comparison testing at
      a later time.</t>

      <t>Consider the following user story as further background and
      motivation:<list style="empty">
          <t>I'm designing and building my NFV Infrastructure platform. The
          first steps were easy because I had a small number of categories of
          VNFs to support and the VNF vendor gave hardware recommendations
          that I followed. Now I need to deploy more VNFs from new vendors,
          and there are different hardware recommendations. How well will the
          new VNFs perform on my existing hardware? Which among several new
          VNFs in a given category are most efficient in terms of capacity
          they deliver? And, when I operate multiple categories of VNFs (and
          PNFs) *concurrently* on a hardware platform such that they share
          resources, what are the new performance limits, and what are the
          software design choices I can make to optimize my chosen hardware
          platform? Conversely, what hardware platform upgrades should I
          pursue to increase the capacity of these concurrently operating
          VNFs?</t>
        </list></t>

      <t>See
      &lt;http://www.etsi.org/technologies-clusters/technologies/nfv&gt; for
      more background; the white papers there may be a useful starting place.
      The "NFV Performance &amp; Portability Best Practices" document <xref
      target="NFV.PER001"/> is particularly relevant to BMWG. 
      There are also documents available among the Approved ETSI NFV
      Specifications <xref target="Approved_ETSI_NFV"/>, including documents
      describing Infrastructure performance aspects and service quality
      metrics, and drafts in the ETSI NFV Open Area <xref
      target="Draft_ETSI_NFV"/>, which may also have relevance to
      benchmarking.</t>

      <section title="Requirements Language">
        <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
        "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
        "OPTIONAL" in this document are to be interpreted as described in BCP
        14 <xref target="RFC2119"/> <xref target="RFC8174"/> when, and only
        when, they appear in all capitals, as shown here.</t>
      </section>
    </section>

    <section title="Scope">
      <t>At the time of this writing, BMWG is considering the new topic of
      Virtual Network Functions and related Infrastructure to ensure that
      common issues are recognized from the start; background materials from
      respective standards development organizations and Open Source
      development projects (e.g., IETF, ETSI NFV, and the Open Platform for
      Network Function Virtualization (OPNFV)) are being used.</t>

      <t>This memo investigates additional methodological considerations
      necessary when benchmarking VNFs instantiated and hosted in
      general-purpose hardware, using bare metal hypervisors <xref
      target="BareMetal"/> or other isolation environments such as Linux
      containers. An essential consideration is benchmarking physical and
      Virtual Network Functions in the same way when possible, thereby
      allowing direct comparison. Benchmarking combinations of physical and
      virtual devices and functions in a System Under Test (SUT) is another
      topic of keen interest.</t>

      <t>A clearly related goal is investigating benchmarks for the capacity
      of a general-purpose platform to host a plurality of VNF instances.
      Existing networking technology benchmarks will also be considered for
      adaptation to NFV and closely associated technologies.</t>

      <t>A non-goal is any overlap with traditional computer benchmark
      development and their specific metrics (e.g., SPECmark suites such as
      SPEC CPU).</t>

      <t>A continued non-goal is any form of architecture development related
      to NFV and associated technologies in BMWG, consistent with all
      chartered work since BMWG began in 1989.</t>
    </section>

    <section title="Considerations for Hardware and Testing">
      <t>This section lists the new considerations that must be addressed to
      benchmark VNF(s) and their supporting Infrastructure. The SUT is
      composed of the hardware platform components, the VNFs installed, and
      many other supporting systems. It is critical to document all aspects of
      the SUT to foster repeatability.</t>

      <section title="Hardware Components">
        <t>The following new hardware components will become part of the test
        setup: <list style="numbers">
            <t>High-volume server platforms (general-purpose, possibly with
            virtual technology enhancements)</t>

            <t>Storage systems with large capacity, high speed, and high
            reliability</t>

            <t>Network interface ports specially designed for efficient
            service of many virtual Network Interface Cards (NICs)</t>

            <t>High-capacity Ethernet switches</t>
          </list></t>

        <t>The components above are subjects for development of specialized
        benchmarks that focus on the special demands of network function
        deployment.</t>

        <t>Labs conducting comparisons of different VNFs may be able to use
        the same hardware platform over many studies, until the steady march
        of innovations overtakes their capabilities (as happens with the lab's
        traffic generation and testing devices today).</t>
      </section>

      <section title="Configuration Parameters">
        <t>It will be necessary to configure and document the settings for the
        entire general-purpose platform to ensure repeatability and foster
        future comparisons, including, but clearly not limited to, the
        following:</t>

        <t><list style="symbols">
            <t>number of server blades (shelf occupation)</t>

            <t>CPUs</t>

            <t>caches</t>

            <t>memory</t>

            <t>storage system</t>

            <t>I/O</t>
          </list> as well as configurations that support the devices that host
        the VNF itself: <list style="symbols">
            <t>Hypervisor (or other forms of virtual function hosting)</t>

            <t>Virtual Machine (VM)</t>

            <t>Infrastructure virtual network (which interconnects virtual
            machines with physical network interfaces or with each other
            through virtual switches, for example)</t>
          </list>and finally, the VNF itself, with items such as:<list
            style="symbols">
            <t>specific function being implemented in VNF</t>

            <t>reserved resources for each function (e.g., CPU pinning and
            Non-Uniform Memory Access (NUMA) node assignment)</t>

            <t>number of VNFs (or sub-VNF components, each with its own VM) in
            the service function chain (see Section 1.1 of <xref
            target="RFC7498"/> for a definition of service function chain)</t>

            <t>number of physical interfaces and links transited in the
            service function chain</t>
          </list></t>

        <t>In the physical device benchmarking context, most of the
        corresponding Infrastructure configuration choices were determined by
        the vendor. Although the platform itself is now one of the
        configuration variables, it is important to maintain emphasis on the
        networking benchmarks and capture the platform variables as input
        factors.</t>
      </section>

      <section title="Testing Strategies">
        <t>The concept of characterizing performance at capacity limits may
        change. For example:</t>

        <t>
          <list style="numbers">
            <t>It may be more representative of system capacity to
            characterize the case where the VMs hosting the VNFs are operating
            at 50% utilization and therefore sharing the "real" processing
            power across many VMs.</t>

            <t>Another important test case stems from the need to partition
            (or isolate) network functions. A noisy neighbor (VM hosting a VNF
            in an infinite loop) would ideally be isolated; the performance of
            other VMs would continue according to their specifications, and
            tests would evaluate the degree of isolation.</t>

            <t>System errors will likely occur as transients, implying a
            distribution of performance characteristics with a long tail (like
            latency) and leading to the need for longer-term tests of each set
            of configuration and test parameters.</t>

            <t>The desire for elasticity and flexibility among network
            functions will include tests where there is constant flux in the
            number of VM instances, the resources the VMs require, and the
            setup/teardown of network paths that support VM connectivity.
            Requests for and instantiation of new VMs, along with releases for
            VMs hosting VNFs that are no longer needed, would be a normal
            operational condition. In other words, benchmarking should include
            scenarios with production life-cycle management of VMs and their
            VNFs and network connectivity in progress, including VNF scaling
            up/down operations, as well as static configurations.</t>

            <t>All physical things can fail, and benchmarking efforts can also
            examine recovery aided by the virtual architecture with different
            approaches to resiliency.</t>

            <t>The sheer number of test conditions and configuration
            combinations encourage increased efficiency, including automated
            testing arrangements, combination sub-sampling through an
            understanding of inter-relationships, and machine-readable test
            results.</t>
          </list>
        </t>
      </section>

      <section title="Attention to Shared Resources">
        <t>Since many components of the new NFV Infrastructure are virtual,
        test setup design must have prior knowledge of
        interactions/dependencies within the various resource domains in the
        SUT. For example, a virtual machine performing the role of a
        traditional tester function, such as generating and/or receiving
        traffic, should avoid sharing any SUT resources with the DUT.
        Otherwise, the results will have unexpected dependencies not
        encountered in physical device benchmarking.</t>

        <t>Note that the term "tester" has traditionally referred to devices
        dedicated to testing in BMWG literature. In this new context, "tester"
        additionally refers to functions dedicated to testing, which may be
        either virtual or physical. "Tester" has never referred to the
        individuals performing the tests.</t>

        <t>The possibility to use shared resources in test design while
        producing useful results remains one of the critical challenges to
        overcome. Benchmarking setups may designate isolated resources for the
        DUT and other critical support components (such as the host/kernel) as
        the first baseline step and add other loading processes. The added
        complexity of each setup leads to shared-resource testing scenarios,
        where the characteristics of the competing load (in terms of memory,
        storage, and CPU utilization) will directly affect the benchmarking
        results (and variability of the results), but the results should
        reconcile with the baseline.</t>

        <t>The physical test device remains a solid foundation to compare with
        results using combinations of physical and virtual test functions or
        results using only virtual testers when necessary to assess virtual
        interfaces and other virtual functions.</t>
      </section>
    </section>

    <section title="Benchmarking Considerations">
      <t>This section discusses considerations related to benchmarks
      applicable to VNFs and their associated technologies.</t>

      <section title="Comparison with Physical Network Functions">
        <t>In order to compare the performance of VNFs and system
        implementations with their physical counterparts, identical benchmarks
        must be used. Since BMWG has already developed specifications for many
        network functions, there will be re-use of existing benchmarks through
        references, while allowing for the possibility of benchmark curation
        during development of new methodologies. Consideration should be given
        to quantifying the number of parallel VNFs required to achieve
        comparable scale/capacity with a given physical device or whether some
        limit of scale was reached before the VNFs could achieve the
        comparable level. Again, implementation based on different hypervisors
        or other virtual function hosting remain as critical factors in
        performance assessment.</t>
      </section>

      <section title="Continued Emphasis on Black-Box Benchmarks">
        <t>When the network functions under test are based on open-source
        code, there may be a tendency to rely on internal measurements to some
        extent, especially when the externally observable phenomena only
        support an inference of internal events (such as routing protocol
        convergence observed in the data plane). Examples include CPU/Core
        utilization, network utilization, storage utilization, and memory
        committed/used. These "white-box" metrics provide one view of the
        resource footprint of a VNF. Note that the resource utilization
        metrics do not easily match the 3x4 Matrix, described in <xref
        target="assessment4"/>.</t>

        <t>However, external observations remain essential as the basis for
        benchmarks. Internal observations with fixed specification and
        interpretation may be provided in parallel (as auxiliary metrics), to
        assist the development of operations procedures when the technology is
        deployed, for example. Internal metrics and measurements from
        open-source implementations may be the only direct source of
        performance results in a desired dimension, but corroborating external
        observations are still required to assure the integrity of measurement
        discipline was maintained for all reported results.</t>

        <t>A related aspect of benchmark development is where the scope
        includes multiple approaches to a common function under the same
        benchmark. For example, there are many ways to arrange for activation
        of a network path between interface points, and the activation times
        can be compared if the start-to-stop activation interval has a generic
        and unambiguous definition. Thus, generic benchmark definitions are
        preferred over technology/protocol-specific definitions where
        possible.</t>
      </section>

      <section title="New Benchmarks and Related Metrics">
        <t>There will be new classes of benchmarks needed for network design
        and assistance when developing operational practices (possibly
        automated management and orchestration of deployment scale). Examples
        follow in the paragraphs below, many of which are prompted by the
        goals of increased elasticity and flexibility of the network
        functions, along with reduced deployment times.<list style="symbols">
            <t>Time to deploy VNFs: In cases where the general-purpose
            hardware is already deployed and ready for service, it is valuable
            to know the response time when a management system is tasked with
            "standing up" 100s of virtual machines and the VNFs they will
            host.</t>

            <t>Time to migrate VNFs: In cases where a rack or shelf of
            hardware must be removed from active service, it is valuable to
            know the response time when a management system is tasked with
            "migrating" some number of virtual machines and the VNFs they
            currently host to alternate hardware that will remain in
            service.</t>

            <t>Time to create a virtual network in the general-purpose
            Infrastructure: This is a somewhat simplified version of existing
            benchmarks for convergence time, in that the process is initiated
            by a request from (centralized or distributed) control, rather
            than inferred from network events (link failure). The successful
            response time would remain dependent on data-plane observations to
            confirm that the network is ready to perform.</t>

            <t>Effect of verification measurements on performance: A complete
            VNF, or something as simple as a new policy to implement in a VNF,
            is implemented. The action to verify instantiation of the VNF or
            policy could affect performance during normal operation.</t>
          </list></t>

        <t>Also, it appears to be valuable to measure traditional packet
        transfer performance metrics during the assessment of traditional and
        new benchmarks, including metrics that may be used to support service
        engineering such as the spatial composition metrics found in <xref
        target="RFC6049"/>. Examples include mean one-way delay in Section 4.1
        of <xref target="RFC6049"/>, Packet Delay Variation (PDV) in <xref
        target="RFC5481"/>, and Packet Reordering <xref target="RFC4737"/>
        <xref target="RFC4689"/>.</t>
      </section>

      <section anchor="assessment4" title="Assessment of Benchmark Coverage">
        <t>It can be useful to organize benchmarks according to their
        applicable life-cycle stage and the performance criteria they were
        designed to assess. The table below (derived from <xref
        target="X3.102"/>) provides a way to organize benchmarks such that
        there is a clear indication of coverage for the intersection of
        life-cycle stages and performance criteria.</t>

        <t>
          <figure align="center">
            <artwork><![CDATA[|----------------------------------------------------------|
|               |             |            |               |
|               |   SPEED     |  ACCURACY  |  RELIABILITY  |
|               |             |            |               |
|----------------------------------------------------------|
|               |             |            |               |
|  Activation   |             |            |               |
|               |             |            |               |
|----------------------------------------------------------|
|               |             |            |               |
|  Operation    |             |            |               |
|               |             |            |               |
|----------------------------------------------------------|
|               |             |            |               |
| De-activation |             |            |               |
|               |             |            |               |
|----------------------------------------------------------|
]]></artwork>
          </figure>
        </t>

        <t>For example, the "Time to deploy VNFs" benchmark described above
        would be placed in the intersection of Activation and Speed, making it
        clear that there are other potential performance criteria to
        benchmark, such as the "percentage of unsuccessful VM/VNF stand-ups"
        in a set of 100 attempts. This example emphasizes that the Activation
        and De-activation life-cycle stages are key areas for NFV and related
        Infrastructure and encourages expansion beyond traditional benchmarks
        for normal operation. Thus, reviewing the benchmark coverage using
        this table (sometimes called the 3x3 Matrix) can be a worthwhile
        exercise in BMWG.</t>

        <t>In one of the first applications of the 3x3 Matrix in BMWG <xref
        target="SDN-BENCHMARK"/>, we discovered that metrics on measured size,
        capacity, or scale do not easily match one of the three columns above.
        Following discussion, this was resolved in two ways:</t>

        <t>
          <list style="symbols">
            <t>Add a column, Scale, for use when categorizing and assessing
            the coverage of benchmarks (without measured results). An example
            of this use is found in <xref target="OPNFV-BENCHMARK"/> (and a
            variation may be found in <xref target="SDN-BENCHMARK"/>). This is
            the 3x4 Matrix.</t>

            <t>If using the matrix to report results in an organized way, keep
            size, capacity, and scale metrics separate from the 3x3 Matrix and
            incorporate them in the report with other qualifications of the
            results.</t>
          </list>
        </t>

        <t>Note that the resource utilization (e.g., CPU) metrics do not fit
        in the matrix. They are not benchmarks, and omitting them confirms
        their status as auxiliary metrics. Resource assignments are
        configuration parameters, and these are reported separately.</t>

        <t>This approach encourages use of the 3x3 Matrix to organize reports
        of results, where the capacity at which the various metrics were
        measured could be included in the title of the matrix (and results for
        multiple capacities would result in separate 3x3 Matrices, if there
        were sufficient measurements/results to organize in that way).</t>

        <t>For example, results for each VM and VNF could appear in the 3x3
        Matrix, organized to illustrate resource occupation (CPU Cores) in a
        particular physical computing system, as shown below.<figure
            align="center">
            <artwork><![CDATA[
              VNF#1
          .-----------.
          |__|__|__|__|
Core 1    |__|__|__|__|
          |__|__|__|__|
          |  |  |  |  |
          '-----------'
              VNF#2
          .-----------.
          |__|__|__|__|
Cores 2-5 |__|__|__|__|
          |__|__|__|__|
          |  |  |  |  |
          '-----------'
              VNF#3             VNF#4             VNF#5
          .-----------.    .-----------.     .-----------.
          |__|__|__|__|    |__|__|__|__|     |__|__|__|__|
Core 6    |__|__|__|__|    |__|__|__|__|     |__|__|__|__|
          |__|__|__|__|    |__|__|__|__|     |__|__|__|__|
          |  |  |  |  |    |  |  |  |  |     |  |  |  |  |
          '-----------'    '-----------'     '-----------'
               VNF#6
          .-----------.
          |__|__|__|__|
Core 7    |__|__|__|__|
          |__|__|__|__|
          |  |  |  |  |
          '-----------']]></artwork>
          </figure></t>

        <t>The combination of tables above could be built incrementally,
        beginning with VNF#1 and one Core, then adding VNFs according to their
        supporting Core assignments. X-Y plots of critical benchmarks would
        also provide insight to the effect of increased hardware utilization.
        All VNFs might be of the same type, or to match a production
        environment, there could be VNFs of multiple types and categories. In
        this figure, VNFs #3-#5 are assumed to require small CPU resources,
        while VNF#2 requires four Cores to perform its function.</t>
      </section>

      <section title="Power Consumption">
        <t>Although there is incomplete work to benchmark physical network
        function power consumption in a meaningful way, the desire to measure
        the physical Infrastructure supporting the virtual functions only adds
        to the need. Both maximum power consumption and dynamic power
        consumption (with varying load) would be useful. The Intelligent
        Platform Management Interface (IPMI) standard <xref target="IPMI2.0"/>
        has been implemented by many manufacturers and supports measurement of
        instantaneous energy consumption.</t>

        <t>To assess the instantaneous energy consumption of virtual
        resources, it may be possible to estimate the value using an overall
        metric based on utilization readings, according to <xref
        target="NFVIaas-FRAMEWORK"/>.</t>
      </section>
    </section>

    <section title="Security Considerations">
      <t>Benchmarking activities as described in this memo are limited to
      technology characterization of a DUT/SUT using controlled stimuli in a
      laboratory environment, with dedicated address space and the constraints
      specified in the sections above.</t>

      <t>The benchmarking network topology will be an independent test setup
      and MUST NOT be connected to devices that may forward the test traffic
      into a production network or misroute traffic to the test management
      network.</t>

      <t>Further, benchmarking is performed on a "black-box" basis, relying
      solely on measurements observable external to the DUT/SUT.</t>

      <t>Special capabilities SHOULD NOT exist in the DUT/SUT specifically for
      benchmarking purposes. Any implications for network security arising
      from the DUT/SUT SHOULD be identical in the lab and in production
      networks.</t>
    </section>

    <section anchor="IANA" title="IANA Considerations">
      <t>This document does not require any IANA actions.</t>
    </section>
  </middle>

  <back>
    <references title="Normative References">
      <?rfc include="reference.RFC.2119"?>

      <?rfc include="reference.RFC.8174"?>

      <?rfc include='reference.RFC.2544'?>

      <?rfc include='reference.RFC.4689'?>

      <?rfc include='reference.RFC.4737'?>

      <?rfc include='reference.RFC.7498'?>

      <reference anchor="NFV.PER001">
        <front>
          <title>Network Function Virtualization: Performance &amp;
          Portability Best Practices</title>

          <author fullname="" initials="" surname="">
            <organization>ETSI</organization>
          </author>

          <date month="December" year="2014"/>
        </front>

        <seriesInfo name="ETSI GS NFV-PER 001," value="V1.1.2"/>

        <format type="PDF"/>
      </reference>
    </references>

    <references title="Informative References">
      <?rfc include='reference.RFC.1242'?>

      <?rfc include='reference.RFC.5481'?>

      <?rfc include='reference.RFC.6049'?>

<reference anchor='SDN-BENCHMARK'>
<front>
<title>Terminology for Benchmarking SDN Controller Performance</title>

<author initials='B' surname='Vengainathan' fullname='Bhuvaneswaran Vengainathan'>
    <organization />
</author>

<author initials='A' surname='Basil' fullname='Anton Basil'>
    <organization />
</author>

<author initials='M' surname='Tassinari' fullname='Mark Tassinari'>
    <organization />
</author>

<author initials='V' surname='Manral' fullname='Vishwas Manral'>
    <organization />
</author>

<author initials='S' surname='Banks' fullname='Sarah Banks'>
    <organization />
</author>

<date month='June' year='2017' />

<abstract><t>This document defines terminology for benchmarking an SDN controller's control plane performance. It extends the terminology already defined in RFC 7426 for the purpose of benchmarking SDN controllers. The terms provided in this document help to benchmark SDN controller's performance independent of the controller's supported protocols and/or network services. A mechanism for benchmarking the performance of SDN controllers is defined in the companion methodology document. These two documents provide a standard mechanism to measure and evaluate the performance of various controller implementations.</t></abstract>

</front>

<seriesInfo name='Work in Progress,'
	    value='draft-ietf-bmwg-sdn-controller-benchmark-term-04' />

<format type='TXT'
        target='http://www.ietf.org/internet-drafts/draft-ietf-bmwg-sdn-controller-benchmark-term-04.txt' />
</reference>

      <reference anchor="OPNFV-BENCHMARK">
        <front>
          <title>Benchmarking Virtual Switches in OPNFV</title>

          <author fullname="Maryam Tahhan" initials="M" surname="Tahhan">
            <organization/>
          </author>

          <author fullname="Billy O'Mahony" initials="B" surname="O'Mahony">
            <organization/>
          </author>

          <author fullname="Al Morton" initials="A" surname="Morton">
            <organization/>
          </author>

          <date month="June" year="2017"/>

          <abstract>
            <t>This memo describes the contributions of the Open Platform for
            NFV (OPNFV) project on virtual switch performance "VSPERF",
            particularly in the areas of test set-ups and configuration
            parameters for the system under test. This project has extended
            the current and completed work of the Benchmarking Methodology
            Working Group in IETF, and references existing literature. The
            Benchmarking Methodology Working Group has traditionally conducted
            laboratory characterization of dedicated physical implementations
            of internetworking functions. Therefore, this memo describes the
            additional considerations when virtual switches are implemented in
            general-purpose hardware. The expanded tests and benchmarks are
            also influenced by the OPNFV mission to support virtualization of
            the "telco" infrastructure.</t>
          </abstract>
        </front>

        <seriesInfo name="Work in Progress,"
                    value="draft-ietf-bmwg-vswitch-opnfv-04"/>

        <format target="http://www.ietf.org/internet-drafts/draft-ietf-bmwg-vswitch-opnfv-04.txt"
                type="TXT"/>
      </reference>

      <reference anchor="NFVIaas-FRAMEWORK">
        <front>
          <title>NFVIaaS Architectural Framework for Policy Based Resource
          Placement and Scheduling</title>

          <author fullname="Ram (Ramki) Krishnan" initials="R"
                  surname="Krishnan">
            <organization/>
          </author>

          <author fullname="Norival Figueira" initials="N" surname="Figueira">
            <organization/>
          </author>

          <author fullname="Dilip Krishnaswamy" initials="D"
                  surname="Krishnaswamy">
            <organization/>
          </author>

          <author fullname="Diego Lopez" initials="D" surname="Lopez">
            <organization/>
          </author>

          <author fullname="Steven Wright" initials="S" surname="Wright">
            <organization/>
          </author>

          <author fullname="Tim Hinrichs" initials="T" surname="Hinrichs">
            <organization/>
          </author>

          <author fullname="Ruby Krishnaswamy" initials="R"
                  surname="Krishnaswamy">
            <organization/>
          </author>

          <author fullname="Arun Yerra" initials="A" surname="Yerra">
            <organization/>
          </author>

          <date month="March" year="2016"/>

          <abstract>
            <t>One of the goals of Network Functions Virtualization (NFV) is
            to offer the NFV Infrastructure as a service to other SP customers
            - this is called NFVIaaS. Virtual Network Function (VNF)
            deployment in this paradigm will drive support for unique
            placement policies, given VNF's stringent service level
            specifications (SLS) required by customer SPs. Additionally, NFV
            DCs often have capacity, energy and other constraints - thus,
            optimizing the overall resource usage based on policy is an
            important part of the overall solution. The purpose of this
            document is to depict an architectural framework for policy based
            resource placement and scheduling in the context of NFVIaaS.</t>
          </abstract>
        </front>

        <seriesInfo name="Work in Progress,"
                    value="draft-krishnan-nfvrg-policy-based-rm-nfviaas-06"/>

        <format target="http://www.ietf.org/internet-drafts/draft-krishnan-nfvrg-policy-based-rm-nfviaas-06.txt"
                type="TXT"/>
      </reference>

      <reference anchor="IPMI2.0"
                 target="http://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/ipmi-intelligent-platform-mgt-interface-spec-2nd-gen-v2-0-spec-update.pdf">
        <front>
          <title>Intelligent Platform Management Interface Specification v2.0
          (with latest errata)</title>

          <author>
            <organization>Intel Corporation</organization>
          </author>

          <author>
            <organization>Hewlett-Packard Company</organization>
          </author>

          <author>
            <organization>NEC Corporation</organization>
          </author>

          <author>
            <organization>Dell Inc.</organization>
          </author>

          <date month="April" year="2015"/>
        </front>
      </reference>

      <reference anchor="BareMetal">
        <front>
          <title>Formal requirements for virtualizable third generation
          architectures</title>

          <author fullname="Gerald J. Popek" initials="G" surname="Popek">
            <organization/>
          </author>

          <author fullname="Robert P. Goldberg" initials="R"
                  surname="Goldberg">
            <organization/>
          </author>

          <date month="July" year="1974"/>
        </front>

        <seriesInfo name="Communications of the ACM, Volume 17, Issue 7, Pages      412-421,"
                    value="DOI 10.1145/361011.361073"/>
      </reference>

      <reference anchor="X3.102">
        <front>
          <title>Information Systems - Data Communication Systems and Services
          - User-Oriented Performance Parameters Communications
          Framework</title>

          <author>
            <organization>ANSI</organization>
          </author>

          <date year="1983"/>
        </front>

        <seriesInfo name="ANSI" value="X3.102"/>
      </reference>

      <reference anchor="Approved_ETSI_NFV"
                 target="http://www.etsi.org/standards-search">
        <front>
          <title>ETSI NFV</title>

          <author>
            <organization>ETSI, Network Functions Virtualisation Technical
            Committee</organization>
          </author>

          <date month="" year=""/>
        </front>
      </reference>

      <reference anchor="Draft_ETSI_NFV"
                 target="http://www.etsi.org/technologies-clusters/technologies/nfv">
        <front>
          <title>Network Functions Virtualisation: Specifications</title>

          <author>
            <organization>ETSI</organization>
          </author>

          <date month="" year=""/>
        </front>
      </reference>
    </references>

    <section numbered="no" title="Acknowledgements">
      <t>The author acknowledges an encouraging conversation on this topic
      with Mukhtiar Shaikh and Ramki Krishnan in November 2013. Bhavani Parise
      and Ilya Varlashkin have provided useful suggestions to expand these
      considerations. Bhuvaneswaran Vengainathan has already tried the 3x3
      Matrix with the SDN controller document and contributed to many
      discussions. Scott Bradner quickly pointed out shared resource
      dependencies in an early vSwitch measurement proposal, and the topic was
      included here as a key consideration. Further development was encouraged
      by Barry Constantine's comments following the BMWG session at IETF 92:
      the session itself was an affirmation for this memo. There have been
      many interesting contributions from Maryam Tahhan, Marius Georgescu,
      Jacob Rapp, Saurabh Chattopadhyay, and others.</t>
    </section>
  </back>
</rfc>
