<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd"  [
<!ENTITY % rfc2119 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml'>
<!ENTITY % rfc2409 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.2409.xml'>
<!ENTITY % rfc2629 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.2629.xml'>
<!ENTITY % rfc3654 PUBLIC ''
"http://xml.resource.org/public/rfc/bibxml/reference.RFC.3654.xml">
<!ENTITY % rfc3746 PUBLIC ''
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.3746.xml'>
<!ENTITY % rfc3758 PUBLIC ''
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.3758.xml'>
<!ENTITY % rfc3768 PUBLIC ''
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.3768.xml'>
<!ENTITY % rfc4960 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.4960.xml'>
<!ENTITY % rfc3554 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.3554.xml'>
<!ENTITY % rfc4301 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.4301.xml'>
<!ENTITY % rfc4303 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.4303.xml'>
<!ENTITY % rfc2404 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.2404.xml'>
<!ENTITY % rfc3602 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.3602.xml'>
<!ENTITY % rfc5226 PUBLIC '' 
'http://xml.resource.org/public/rfc/bibxml/reference.RFC.5226.xml'>
]>

<?rfc toc="yes"?>
<?rfc tocompact="yes"?>
<?rfc tocdepth="3"?>
<?rfc tocindent="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes"?>
<?rfc comments="yes"?>
<?rfc inline="yes"?>
<?rfc rfcedstyle="yes"?>
<?rfc subcompact="no"?>
<?rfc strict="no"?>

<rfc number="5811" category="std" ipr="pre5378Trust200902" >

  <front>
	  <title abbrev="ForCES SCTP TML">
		  SCTP-Based Transport Mapping Layer (TML) for the Forwarding and Control Element Separation (ForCES) Protocol
	  </title>

    <author fullname="Jamal Hadi Salim" initials="J." surname="Hadi Salim">
      <organization>Mojatatu Networks</organization>
	<address>
	 <postal>
		<city>Ottawa, Ontario</city>
		<country>Canada</country>
	</postal>
	<email>hadi@mojatatu.com</email>
	</address>

   </author>
 
   <author fullname="Kentaro Ogawa" initials="K." surname="Ogawa">
      <organization>NTT Corporation</organization>
	<address>
	 <postal>
                <street>3-9-11 Midori-cho</street>
		<city>Musashino-shi, Tokyo</city>
                <code>180-8585</code>
		<country>Japan</country>
	</postal>
	<email>ogawa.kentaro@lab.ntt.co.jp</email>
	</address>

   </author>

    <date month="March" year="2010" />
    <area>Routing</area>

<keyword>ForCES</keyword>
<keyword>TML</keyword>

<!--[rfced] Please insert any keywords (beyond those that appear in the title) for use on http://www.rfc-editor.org/rfcsearch.html. -->

<!-- [rfced] Please note that after you have approved the document,
post-xml2rfc changes will be made to update the header and
boilerplate as described in RFC 5741.
-->


<abstract>
    <t>
This document defines the SCTP-based TML (Transport Mapping Layer) for
the ForCES (Forwarding and Control Element Separation) protocol. It
explains the rationale for choosing the SCTP (Stream Control
Transmission Protocol) and also describes how this TML addresses all
the requirements required by and the ForCES protocol.
    </t>
</abstract>

  </front>

  <middle>

<section title="Introduction">
<t>

The ForCES (Forwarding and Control Element Separation) working group 
in the IETF defines the architecture and protocol for separation 
of control elements (CEs) and forwarding elements (FEs) in network elements (NEs)
such as routers.  <xref target="RFC3654"/> and <xref target="RFC3746"/>,	
respectively, define  architectural and protocol 
requirements for the communication between CEs and FEs. The ForCES 
protocol layer specification <xref target="RFC5810"/> describes the 
protocol semantics and workings. The ForCES protocol layer operates
on top of an inter-connect hiding layer known as the TML. The relationship
is illustrated in <xref target="pltml_fig"/>.
<t>
This document defines the SCTP-based TML for the ForCES 
protocol layer. It also addresses all the requirements for the TML 
including security, reliability, and etc., as defined in <xref target="RFC5810"/>. 
</t>

</t>
</section>

<section title="Definitions">
<t>
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in
this document are to be interpreted as described in [RFC2119].

</t>

<t>
		          
The following definitions are taken from <xref target="RFC3654"/> and
<xref target="RFC3746"/>:</t>


<list style="hanging" hangIndent="18">
<t hangText="LFB:">Logical Functional Block. A template that represents a fine-grained, logically separate
aspect of FE processing.
</t>
<t hangText="ForCES Protocol:">The protocol used at the Fp reference point in the ForCES 
Framework in <xref target="RFC3746"/>.</t>  

<t hangText="ForCES PL:">ForCES Protocol Layer. A layer in the ForCES architecture that embodies the ForCES protocol
and the state transfer mechanisms as defined in
<xref target="RFC5810"/>.
</t>
							         
<t hangText="ForCES TML:">ForCES Protocol Transport Mapping Layer. A layer in the
ForCES protocol architecture that specifically addresses the 
protocol message transportation issues, such as how the protocol 
messages are mapped to different transport media (like SCTP, IP, 
TCP, UDP, ATM, Ethernet, etc.), and how to achieve and implement 
reliability, security, etc.
</t>
</list>

</section>

<section title="Protocol Framework Overview" anchor="overv">
<t>
The reader is referred to the Framework document <xref target="RFC3746"/>, 
and in particular Sections 3 and 4, for an architectural overview and 
explanation of where and how the ForCES protocol fits in.  
</t>
<t>
There is some content overlap between the ForCES protocol specification 
<xref target="RFC5810"/> and this section (<xref target="overv"/>) 
in order to provide basic context to the reader of this document. 
</t>
<t>
The ForCES protocol layering constitutes two pieces, the PL and TML.
This is depicted in <xref target="pltml_fig"/>. 
<figure anchor = "pltml_fig" title="Message Exchange between CE and FE
	    to Establish an NE Association">
<preamble> </preamble>
<artwork><![CDATA[


            +----------------------------------------------+
            |                    CE PL                     |
            +----------------------------------------------+
            |                    CE TML                    |
            +----------------------------------------------+
                                   ^
                                   |
                        ForCES PL  |messages 
                                   |
                                   v
            +-----------------------------------------------+
            |                   FE TML                      |
            +-----------------------------------------------+
            |                   FE PL                       |
            +-----------------------------------------------+


]]></artwork>

</figure>

   The PL is in charge of the ForCES protocol. Its semantics and 
   message layout are defined in <xref target="RFC5810"/>.
   The TML is necessary to connect two ForCES endpoints as 
   shown in <xref target="pltml_fig"/>. 
    
   <t>
   Both the PL and TML are standardized by the IETF. While only 
   one PL is defined, different TMLs are expected to be 
   standardized. The TML at each of the nodes (CE and FE) is 
   expected to be of the same definition in order to inter-operate. 
</t>
    
 <t>
   When transmitting from a ForCES endpoint, the PL delivers 
   its messages to the TML.
   The TML then delivers the PL message to the destination
   TML(s).

   <t>
   On reception of a message, the TML delivers the message to its 
   destination PL (as described in the ForCES header). 
   </t>
 </t>
</t>
    
<section title="The PL">
<t>
   The PL is common to all implementations of ForCES and is 
   standardized by the IETF <xref target="RFC5810"/>.
   The PL is responsible for associating an FE or CE to an NE.
   It is also responsible for tearing down such associations. 
</t>
<t>
   An FE may use the PL to asynchronously send packets to the CE.
   The FE may redirect various
   control protocol packets (e.g., OSPF, etc.) to the CE via the PL (from outside the NE). Additionally,
   the FE delivers various events that the CE has subscribed to via the PL
   <xref target="RFC5812"/>.
</t>
<t>
   The CE and FE may interact synchronously via the PL.
   The CE issues status requests to the FE and receives responses 
   via the PL.
   The CE also configures the components of the associated FE's LFBs using 
   the PL <xref target="RFC5812"/>.
</t>
</section>
<section title="The TML">
	<t>
        The TML is responsible for the transport of the PL messages.
	<xref target="RFC5810"/>, Section 5 defines the requirements that
	need to be met by a TML specification. The SCTP TML specified in
	this document meets all the requirements specified in 
	<xref target="RFC5810"/>, Section 5. <xref target="TMLREQ"/> of this document
	describes how the TML requirements are met.
    </t>

<section title="TML and PL Interfaces" anchor="TMLAPIs">
<t>
There are two interfaces to the PL and TML. The specification of
these interfaces is out of scope for this document, but the 
interfaces are introduced to show how they fit into the architecture
and summarize the function provided at the interfaces.


The first interface is between the PL and TML and the other is the
CE Manager (CEM)/FE Manager (FEM) <xref target="RFC3746"/>
interface to both the PL and TML. Both interfaces are shown in 
<xref target="pltml_api"/>. 
</t>

<figure anchor = "pltml_api" title="The TML-PL Interface">
<preamble> </preamble>
<artwork><![CDATA[

                   +----------------------------+ 
                   |  +----------------------+  |
                   |  |                      |  |
  +---------+      |  |          PL          |  |
  |         |      |  +----------------------+  | 
  |FEM/CEM  |<---->|             ^              |
  |         |      |             |              |
  +---------+      |             |TML API       |
                   |             |              |
                   |             V              |
                   |  +----------------------+  |
                   |  |                      |  |
                   |  |          TML         |  |
                   |  |                      |  |
                   |  +----------------------+  |
                   +----------------------------+ 

]]></artwork>

</figure>

<t>

The CEM/FEM <xref target="RFC3746"/> interface is responsible 
for bootstrapping and parameterization of the TML. In its most basic 
form, the CEM/FEM interface takes the form of a simple static config 
file that is read on startup in the pre-association phase.

</t>
<t>
<xref target="serviceint"/> discusses the service
interfaces in more detail.
</t>

</section>
   <section title ="TML Parameterization" anchor ="TMLp">
	   <t>
		   It is expected that it should be possible to use a 
		   configuration reference point, such as the FEM 
		   or the CEM, to configure the TML.
	   </t>
	   <t>
		   Some of the configured parameters may include:
	   </t>
	   <list style = "symbols">
		   <t>PL ID</t>
		   <t>Connection Type and associated data. For example, if a 
			   TML uses IP/SCTP, then parameters such as SCTP 
			   ports and IP addresses need to be configured.</t>
		   <t>Number of transport connections </t>
		   <t>Connection Capability, such as bandwidth, etc.</t>
		   <t>Allowed/Supported Connection Quality of Service (QoS) Policy (or 
			   Congestion Control Policy)</t>
	   </list>
   </section>
</section>

</section>


<section title="SCTP TML Overview">
    <t>
    </t>
    <!--
      -->
        <t>
		SCTP <xref target="RFC4960"/> is an end-to-end transport 
		protocol that is equivalent to TCP, UDP, or DCCP in many 
		aspects. 
		With a few exceptions, SCTP can do most of what UDP, TCP,
		or DCCP can achieve. SCTP as can also do most of what
		a combination of the other transport protocols can achieve
		(e.g., TCP and DCCP or TCP and UDP).
        </t>
	<t>
		Like TCP, it provides ordered, reliable, connection-oriented,
		flow-controlled, congestion-controlled data exchange. 
		Unlike TCP, it does not provide byte streaming and instead
		provides message boundaries.
        </t>
	<t>
		Like UDP, it can provide unreliable, unordered
		data exchange. 
		Unlike UDP, it does not provide multicast support
        </t>
	<t>
		Like DCCP, it can provide unreliable, ordered, congestion
		controlled, connection-oriented data exchange. 
        </t>
	<t>
		SCTP also provides other services that none of the
		three transport protocols mentioned above provide that
		we found attractive. These include:

	   <list style = "symbols">
		   <t>Multi-homing <vspace />
		   </t>
		   <t>Runtime IP address binding <vspace />

		   </t>
		   <t>A range of reliability shades with congestion 
			   control <vspace />

		   </t>
		   <t>Built-in heartbeats <vspace />

		   </t>
		   <t>Multi-streaming <vspace />

		   </t>
		   <t>Message boundaries with reliability <vspace />

		   </t>
		   <t>Improved SYN DOS protection <vspace />


		   </t>
		   <t>Simpler transport events <vspace />


		   </t>
		   <t>Simplified replicasting <vspace />
			   
		   </t>
	   </list>
	</t>
    <section title="Rationale for Using SCTP for TML">
     <t>
	     SCTP has all the features required to provide a robust
	     TML. As a transport that is all-encompassing, it negates 
	     the need for having multiple transport protocols in order
	     to satisfy the TML requirements (<xref target="RFC5810"/>,
	     Section 5).

	     As a result, it allows for simpler coding
	     and therefore reduces a lot of the interoperability concerns.
     </t>
     <t>
	     SCTP is also very mature and widely used,
	     making it a good choice for ubiquitous deployment.

     </t>
     <t>
     </t>
    </section>

<section title="Meeting TML Requirements">

<figure anchor = "sctptml_api" title="The TML-SCTP Interface">
<preamble> </preamble>
<artwork><![CDATA[

               PL
               +----------------------+ 
               |                      |
               +-----------+----------+ 
                           |   TML API
                TML        |
               +-----------+----------+ 
               |           |          |
               |    +------+------+   |
               |    |  TML core   |   |
               |    +-+----+----+-+   |
               |      |    |    |     | 
               |    SCTP socket API   |
               |      |    |    |     | 
               |      |    |    |     | 
               |    +-+----+----+-+   |
               |    |    SCTP     |   |
               |    +------+------+   |
               |           |          |
               |           |          |
               |    +------+------+   |
               |    |      IP     |   |
               |    +-------------+   |
               +----------------------+ 

]]></artwork>

</figure>

<t>
<xref target="sctptml_api"/> details the interfacing between the PL
and SCTP TML and the internals of the SCTP TML. The core of the TML 
interacts on its northbound interface to the PL (utilizing the TML 
API).
On the southbound interface, the TML core interfaces to the SCTP layer
utilizing the standard socket interface <xref target="TSVWG-SCTPSOCKET"/>. 
There are three SCTP socket connections opened between any two PL
endpoints (whether FE or CE).
</t>
<section title="SCTP TML Channels" anchor = "sctptml_channs" >

<figure anchor = "sctptml_chan" title="The TML-SCTP Channels">
<preamble> </preamble>
<artwork><![CDATA[

               +--------------------+
               |                    |
               |     TML   core     |
               |                    |
               +-+-------+--------+-+
                 |       |        |
                 |   Med prio,    |
                 |  Semi-reliable |
                 |    channel     |
                 |       |      Low prio,
                 |       |      Unreliable
                 |       |      channel  
                 |       |        |
                 ^       ^        ^
                 |       |        |
                 Y       Y        Y
       High prio,|       |        |
        reliable |       |        |
         channel |       |        |
                 Y       Y        Y
              +-+--------+--------+-+
              |                     |
              |        SCTP         |
              |                     |
              +---------------------+

]]></artwork>
</figure>
<t>
<xref target="sctptml_chan"/> details further the interfacing 
between the TML core and SCTP layers. There are three channels used
to group and prioritize the work for different types of ForCES traffic.
Each channel constitutes an SCTP socket interface that has different 
properties. 
It should be noted that all SCTP channels are congestion aware (and
for that reason that detail is left out of the description of the three
channels).
SCTP ports 6704, 6705, and 6706 are used for the higher-, medium-, and 
lower-priority channels, respectively. SCTP Payload Protocol ID (PPID)
values of 21, 22, and 23 are used for the higher-, medium-, and
lower-priority channels, respectively.

</t>

<section title="Justifying Choice of Three Sockets" anchor="3socks">
<t>
SCTP allows up to 64 K streams to be sent over a single socket 
interface. The authors initially envisioned using a single socket 
for all three channels (mapping a channel to an SCTP stream). This 
simplifies programming of the TML as well as conserves use of SCTP ports. 
</t>
<t>
Further analysis revealed head-of-line blocking issues with this 
initial approach. Lower-priority packets not needing reliable delivery
could block higher-priority packets (needing reliable delivery) under
congestion situations for an indeterminate period of time (depending
on how many outstanding lower-priority packets are pending).
For this reason, we elected to go with mapping
each of the three channels to a different SCTP socket (instead of
a different stream within a single socket).
</t>

</section>

<section title="Higher-Priority, Reliable Channel"  anchor="HP">
<t>
The higher-priority (HP) channel uses a standard SCTP reliable socket on 
port 6704.  SCTP PPID 21 is used for all messages on the HP channel.
The HP channel is used for CE-solicited messages and their responses:
<list style = "numbers">
<t>
ForCES configuration messages flowing from CE to FE and responses
from the FE to CE.
</t>
<t>
ForCES query messages flowing from CE to FE and responses from
the FE to the CE.
</t>
</list>
</t>
<t>
PL priorities 4-7 MUST be used for all PL messages using this channel.
The following PL messages MUST use the HP channel for transport:
<list style = "symbols">
<t>
	AssociationSetup (default priority: 7)
</t>
<t>
	AssociationSetupResponse (default priority: 7)
</t>
<t>
	AssociationTeardown (default priority: 7)
</t>
<t>
	Config (default priority: 4)
</t>
<t>
	ConfigResponse (default priority: 4)
</t>
<t>
	Query (default priority: 4)
</t>
<t>
	QueryResponse (default priority: 4)
</t>
</list>

<t>
If PL priorities outside of the specified range priority (4-7), PPID,
or PL message types other than the above are received on the HP channel,
then the PL message MUST be dropped.
</t>
<t>
Although an implementation may choose different values from the defined 
range (4-7), it is RECOMMENDED that default priorities be used.
A response to a ForCES message MUST contain the same priority as the request.
For example, a config sent by the CE with priority 5 MUST have a config-response
from the FE with priority 5.
</t>
</t>

</section>

<section title="Medium-Priority, Semi-Reliable Channel" anchor = "MP">
<t>
The medium-priority (MP) channel uses SCTP-PR on port 6705.
SCTP PPID 22 MUST be used for all messages on the MP channel.
Time limits on how long a message is valid are set on each
outgoing message. This channel is used for events from the FE
to the CE that are obsoleted over time. 

Events that are accumulative in nature and are recoverable by the CE
(by issuing a query to the FE) can tolerate lost events and therefore 
should use this channel.
For example, a generated event that carries the value of a counter
that is monotonically incrementing is fit to use this channel.
</t>
<t>
PL priority 3 MUST be used for PL messages on this channel.
The following PL messages MUST use the MP channel for transport:
<list style = "symbols">
<t>
	Event Notification (default priority: 3)
</t>
</list>
<t>
If PL priorities outside of the specified priority, PPID,
or PL message type other than the above are received on the 
MP channel, then the PL message MUST be dropped.
</t>
</t>
</section>

<section title="Lower-Priority, Unreliable Channel" anchor = "LP">
<t>
The lower-priority (LP) channel uses SCTP port 6706.
SCTP PPID 23 is used for all messages on the LP channel.
The LP channel 
also MUST use SCTP-PR with lower timeout values than the MP channel.
The reason an unreliable channel is used for redirect
messages is to allow the control protocol at both the CE and its
peer-endpoint to take charge of how the end-to-end semantics of the
said control protocol's operations.  For example:
<list style = "numbers">
<t>
Some control protocols are reliable in nature, therefore making
this channel reliable introduces an extra layer of reliability
that could be harmful.  So any end-to-end retransmits will
happen remotely.
   </t>
   <t>
Some control protocols may desire having obsolescence of messages
over retransmissions; making this channel reliable contradicts
that desire.
   </t>
</list>
</t>
<t>
Given ForCES PL heartbeats are traffic sensitive, sending
them over the LP channel also makes sense. If the other end is
not processing other channels, it will eventually get heartbeats;
and if it is busy processing other channels, heartbeats will be
obsoleted locally over time (and it does not matter if they did not
make it).
</t>
<t>
PL priorities 1-2 MUST be used for PL messages on this channel.
PL messages that MUST use the MP channel for transport are:
<list style = "symbols">
<t>
	PacketRedirect (default priority: 2)
</t>
<t>
	Heartbeat (default priority: 1)
</t>
</list>

<t>
If PL priorities outside of the specified priority range, PPID,
or PL message types other than the above are received on the 
LP channel, then the PL message MUST be dropped.
</t>
</t>
</section>
<section title="Scheduling of the Three Channels"  anchor = "3csched">
<t>
In processing the sending and recieving of the PL messages, 
the TML core uses strict priority work-conserving scheduling, as 
shown in <xref target="gen-sched"/>.
</t>
<t>
This means that the HP messages are always processed
first until there are no more left. The LP channel is 
processed only if channels that are a higher priority than itself 
have no messages left to process.
This means that under a congestion situation, a higher-priority channel
with sufficient messages that occupy the available bandwidth would
starve lower-priority channel(s). 
</t>
<t>
The design intent of the SCTP TML is to tie processing prioritization,	
as described in <xref target="3socks"/>, and transport congestion
control to provide implicit node congestion control. This
is further detailed in <xref target="sched-det"/>.
</t>
<t>
It should be emphasized that the work scheduling prioritization scheme 
prescribed in this document is receiver-based processing. Fully arrived 
packets on any of the channels are a source of work whose output
may result in transmitted packets. However, we have no control on the order in 
which the SCTP/OS/network chooses to send transmitted packets across 
and make them available to the receiver. This is a limitation that we try 
to ameliorate by our choice of channel properties, ForCES message grouping,
and the tying of CE and FE work scheduling. 
While that helps us ameliorate some of these issues,
it does not fully resolve all.
</t>
<t>
From a ForCES perspective, we can tolerate some reordering. For example,
if an FE transmits a config response (HP) followed by 10000 OSPF redirect
packets (LP) and  the CE gets 5 OSPF redirects (LP) first
before the config response (HP), that is tolerable. What matters is the CE
gets to processing the HP message soon (instead of sitting in long periods
of time processing OSPF packets that would have happened if we use a
single socket with three streams). This is particularly important in order
to deal with node overload well, as discussed in <xref target="oload"/>.
</t>
<t>
<figure anchor = "gen-sched" title="SCTP TML Strict Priority Scheduling">
<preamble> </preamble>
<artwork><![CDATA[

    SCTP channel            +----------+
    Work available          |   DONE   +---<--<--+
        |                   +---+------+         |
        Y                                        ^
        |         +-->--+         +-->---+       |
+-->-->-+         |     |         |      |       |
|       |         |     |         |      |       ^
|       ^         ^     v         ^      v       |
^      / \        |     |         |      |       |
|     /   \       |     ^         |      ^       ^
|    / Is  \      |    / \        |     / \      |
|   / there \     |   /Is \       |    /Is \     |
^  / HP work \    ^  /there\      ^   /there\    ^
|  \    ?    /    | /MP work\     |  /LP work\   |
|   \       /     | \    ?  /     |  \   ?   /   |
|    \     /      |  \     /      |   \     /    ^
|     \   /       ^   \   /       ^    \   /     |
|      \ /        |    \ /        |     \ /      | 
^       Y-->-->-->+     Y-->-->-->+      Y->->->-+ 
|       |    NO         |    NO          |  NO    
|       |               |                |
|       Y               Y                Y
|       | YES           | YES            | YES
^       |               |                |
|       Y               Y                Y         
|  +----+------+    +---|-------+   +----|------+ 
|  |- process  |    |- process  |   |- process  |
|  |  HP work  |    |  MP work  |   | LP work   |
|  +------+----+    +-----+-----+   +-----+-----+ 
|         |               |               |     
^         Y               Y               Y    
|         |               |               |   
|         Y               Y               Y  
+--<--<---+--<--<----<----+-----<---<-----+ 

]]></artwork>
</figure>


</t>

</section>
<section title="SCTP TML Parameterization" anchor="params">
<t>
The following is a list of parameters needed for booting the TML.
It is expected these parameters will be extracted via the FEM/CEM
interface for each PL ID.

<list style = "numbers">
<t>
	The IP address(es) or a resolvable DNS/hostname(s) of the CE/FE.
</t>
<t>
	Whether or not to use IPsec. If IPsec is used, how to
	parameterize the different required ciphers, keys, etc.,
	as described in  <xref target="ipsec"/>
</t>
<t>
	The HP SCTP port, as discussed in <xref target="HP"/>. The
	default HP port value is 6704 (<xref target="IANA"/>).
</t>
<t>
	The MP SCTP port, as discussed in <xref target="MP"/>.
	The default MP port value is 6705 (<xref target="IANA"/>).
</t>
<t>
	The LP SCTP port, as discussed in <xref target="LP"/>.
	The default LP port value is 6706 (<xref target="IANA"/>).
</t>

</list>

</t>
</section>

</section>

<section title="Satisfying TML Requirements" anchor="TMLREQ">
<t>
	<xref target="RFC5810"/>, Section 5 lists requirements that
	a TML needs to meet. This section describes how the SCTP
	TML satisfies those requirements.
</t>
<section title="Satisfying Reliability Requirement">
    <t>
    As mentioned earlier, a shade of reliability
    ranges is possible in SCTP. Therefore, this requirement
    is met.
    </t>
</section>
<section title="Satisfying Congestion Control Requirement">
    <t>
    Congestion control is built into SCTP. Therefore,
    this requirement is met.
    </t>
</section>
<section title="Satisfying Timeliness and Prioritization Requirement">
    <t>
    By using three sockets in conjunction with the
    partial-reliability feature <xref target="RFC3758"/>, both timeliness and
    prioritization requirements are addressed.
    </t>
</section>
<section title="Satisfying Addressing Requirement">
    <t>
    There are no extra headers required for SCTP to 
    fulfill this requirement.
    SCTP can be told to replicast packets to multiple
    destinations. The TML implementation will need to 
    translate PL addresses to a variety of unicast 
    IP addresses in order to emulate multicast and broadcast
    PL addresses.
    </t>
</section>
<section title="Satisfying High-Availability Requirement">
<t>
    Transport link resiliency is one of SCTP's strongest points.
    Failure detection and recovery is built in, as mentioned
    earlier.
   <list style = "symbols">
     <t>
	  The SCTP multi-homing feature is used to provide path 
	  diversity. 
	  Should one of the peer IP addresses become unreachable, 
	  the others are used without needing lower-layer
	  convergence (routing, for example) or even
	  the TML becoming aware.
    </t>
    <t>
	   SCTP heartbeats and data transmission thresholds are used
	   on a per-peer IP address to detect reachability faults.
	   The faults could be a result of an unreachable address or 
	   peer, which may be caused by a variety of reasons, 
	   like interface, network, or endpoint failures. The cause
	   of the fault is noted.
    </t>
    <t>
	   With the ADDIP feature, one can migrate IP addresses
	   to other nodes at runtime. This is not unlike the
	   Virtual Router Redundancy Protocol (VRRP) <xref target="RFC3768"/> use. This feature
	   is used in addition to multi-homing in a planned migration
	   of activity from one FE/CE to another. In such a case, part
	   of the provisioning recipe at the CE for replacing an FE
	   involves migrating activity of one FE to another.
    </t>
   </list>
</t>
</section>
    <section anchor="oload" title="Satisfying Node Overload Prevention Requirement">
	    <t>
	    The architecture of this TML defines three separate 
	    channels, one per socket, to be used within any FE-CE setup. 
	    The work scheduling design for processing the TML channels
	    (<xref target="3csched"/>) is a strict priority.  A fundamental
	    desire of the strict prioritization is to ensure that more
	    important processing work always gets node resources
	    over less important work.
	    </t>
	    <t>
	    When a ForCES node CPU is overwhelmed because the incoming 
	    packet rate is higher than it can keep up with, the channel
	    queues grow and transport congestion subsequently follows.
	    By virtue of using SCTP, the congestion is propagated back 
	    to the source of the incoming packets and eventually
	    alleviated. 
	    </t>
	    <t>
	    The HP channel work gets prioritized at the expense of the 
	    MP, which gets prioritized over LP channels. 
	    The preferential scheduling only kicks in when there is 
	    node overload regardless of whether there is transport
	    congestion. As a result of the preferential work treatment,
	    the ForCES node achieves a robust steady processing capacity.
	    Refer to <xref target="sched-det"/> for details on scheduling.
	    </t>
	    <t>
	    For an example of how the overload prevention works, consider
	    a scenario where an overwhelming amount of redirected packets 
	    (from outside the NE) coming into the NE may overload the 
	    FE while it has outstanding config work from the CE.
	    In such a case, the FE, while it is busy processing config
	    requests from the CE, essentially ignores processing the redirect 
	    packets on the LP channel.
	    If enough redirect packets accumulate, they are dropped
	    either because the LP channel threshold is exceeded or because
	    they are obsoleted. If on the other hand, the FE has successfully
	    processed the higher-priority channels and their related work,
	    then it can proceed and process the LP channel.
	    So as demonstrated in this case, the TML ties transport congestion
	    and node overload implicitly together. 
	    </t>
	    </section>
    <section title="Satisfying Encapsulation Requirement">
	    <t>
	    The SCTP TML sets SCTP PPIDs to identify channels
	    used as described in <xref target="3socks"/>.
	    </t>
	    </section>
     </section>
    </section> <!--TMLREQ-->
</section>

<section anchor="work" title="SCTP TML Channel Work">
<t>
There are two levels of TML channel work within an NE when a ForCES
node (CE or FE) is connected to multiple other ForCES nodes:

<list style = "numbers">
    <t>
	NE-level I/O work where a ForCES node (CE or FE)
	needs to choose which of the peer nodes to process.
    </t>
    <t>
	Node-level I/O work where a ForCES node, handles
	the three SCTP TML channels separately for each single 
	ForCES endpoint.
    </t>
</list>
</t>
<t>
NE-level scheduling definition is left up to the implementation and
is considered out of scope for this document.
<xref target="tml-NE"/> briefly discusses some constraints about which an
implementer needs to worry.
</t>
<t>
This document provides suggestions on SCTP channel work implementation
in <xref target="tml-work"/>.
</t>

<t>
The FE SHOULD do channel connections to the
CE in the order of incrementing priorities, i.e., LP socket first, 
followed by MP, and ending with HP socket connection. The CE, however,
MUST NOT assume that there is ordering of socket connections from any FE.
</t>

</section>


<section anchor="IANA" title="IANA Considerations">
<t>
   Following the policies outlined in "Guidelines for Writing an IANA
   Considerations Section in RFCs" <xref target="RFC5226"/>, 
   the following namespaces are defined in ForCES SCTP TML.
</t>
<list style = "symbols">
<t>
SCTP port 6704 for the HP channel, 6705 for the MP channel, and 6706
for the LP channel. 
</t>
<t>
SCTP Payload Protocol ID (PPID) 21 for the HP channel (ForCES-HP), 22 for the
MP channel (ForCES-MP), and 23 for the LP channel (ForCES-LP).
</t>
</list>
</section>

<section anchor="Security" title="Security Considerations">

<t>
   The SCTP TML provides the following security services to
   the PL:
   <list style = "symbols">
   <t>
	A mechanism to authenticate ForCES CEs and FEs at the transport level
        in order to prevent the participation of unauthorized CEs and 
        unauthorized FEs in the control and data path processing of a ForCES
        NE.
   </t>
   <t>
        A mechanism to ensure message authentication
        of PL data and headers transferred from the CE to FE (and vice versa)
        in order to prevent the injection of incorrect data into PL messages.
   </t>
   <t>
        A mechanism to ensure the confidentiality of
        PL data and headers transferred from the CE to FE  (and vice versa),
        in order to prevent disclosure of PL information transported 
        via the TML.
   </t>
   </list>
</t>
<t>
Security choices provided by the TML are made by the operator 
and take effect during the pre-association phase of the ForCES
protocol. An operator may choose to use all, some or none of the
security services provided by the TML in a CE-FE connection.
</t>
<t>
When operating under a secured environment, or for other
operational concerns (in some cases performance issues)
the operator may turn off all the security functions between CE and FE.
</t>

<t>
IP Security Protocol (IPsec) <xref target="RFC4301"/> is used
to provide needed security mechanisms.
</t>
   <t>
   IPsec is an IP-level security scheme 
   transparent to the higher-layer applications and therefore can provide 
   security for any transport layer protocol. This gives IPsec the
   advantage that it can be used to secure everything between the CE
   and FE without expecting the TML implementation to be aware
   of the details.
   </t>
   <t>
   The IPsec architecture is designed to provide message integrity and 
   message confidentiality outlined in the
   TML security requirements <xref target="RFC5810"/>.
   Mutual authentication and key exchange protocol are provided by
   Internet Key Exchange (IKE) <xref target="RFC2409"/>.
   </t>

   <section anchor="ipsec" title="IPsec Usage">
   <t>
   A ForCES FE or CE MUST support the following:

   <list style = "symbols">
	   <t>
	   Internet Key Exchange (IKE)<xref target="RFC2409"/> with
	   certificates for endpoint authentication.
	   </t>
	   <t>
	   Transport Mode Encapsulating Security Payload (ESP) <xref target="RFC4303"/>.
	   </t>
	   <t> 
	   HMAC-SHA1-96 <xref target="RFC2404"/> for message
	   integrity protection
	   </t> 
          <t> 
	   AES-CBC with 128-bit keys <xref target="RFC3602"/> for message 
	   confidentiality.
	   </t> 
	   <t>
	   Replay protection <xref target="RFC4301"/>.
	   </t> 
   </list>
   </t>
   <t>
   A compliant implementation SHOULD provide operational means for
   configuring the CE and FE to negotiate other cipher suites and 
   even use manual keying.
   </t>
    <section anchor="SAPD" title="SAD and SPD Setup">
    <t>
   To minimize the operational configuration, it is RECOMMENDED that only
   the IANA-issued SCTP protocol number (132) be used as a selector 
   in the Security Policy Database (SPD) for ForCES. In such a case,
   only a single SPD and SAD entry is needed.
    </t>
    <t>
 Setup MAY alternatively extend the above policy so that it
  uses the three SCTP TML port numbers as SPD selectors.  But as noted above,
  this choice will require an increased number of SPD entries.
    </t>
    <t>
   In scenarios where multiple IP addresses are used within a 
   single association, and there is desire to configure different 
   policies on a per-IP address, 
   then following <xref target="RFC3554"/> is RECOMMENDED.
    </t>

    </section>
  </section>
</section>

    <section anchor="Acknowledgements" title="Acknowledgements">
	    <t>
    The authors would like to thank Joel Halpern, Michael Tuxen,
    Randy Stewart, Evangelos Haleplidis, Chuanhuang Li, Lars Eggert,
    Avshalom Houri, Adrian Farrel, Juergen Quittek, Magnus Westerlund,
    and Pasi Eronen for engaging 
    us in discussions that have made this document better.
    </t>
<t>
Ross Callon was an excellent manager who persevered in providing us
guidance and
Joel Halpern was an excellent document shepherd without whom this 
document would have taken longer to publish.
</t>
    </section>
  </middle>

  <back>
<?rfc rfcedstyle="no" ?>
    <references title="Normative References">

    &rfc2404;
    &rfc2119;
    &rfc3554;
    &rfc3602;
    &rfc4301;
    &rfc4303;
    &rfc2409;
    &rfc4960;
    &rfc5226;
    &rfc3758;
<!--    &feproto; -->

<reference anchor='RFC5810'>
<front>
<title>Forwarding and Control Element Separation (ForCES) Protocol Specification</title>


<author initials='A' surname='Doria' fullname='Avri Doria' role="editor">
    <organization />
</author>

<author initials='J' surname='Hadi Salim' fullname='Jamal Hadi Salim' role="editor">
    <organization />
</author>

<author initials='R' surname='HAAS' fullname='Robert HAAS' role="editor">
    <organization />
</author>

<author initials='H' surname='Khosravi' fullname='Hormuzd Khosravi' role="editor">
    <organization />
</author>

<author initials='W' surname='Wang' fullname='Weiming Wang' role="editor">
    <organization />
</author>


<author initials='L' surname='Dong' fullname='Ligang Dong'>
    <organization />
</author>

<author initials='R' surname='Gopal' fullname='Ram Gopal'>
    <organization />
</author>

<author initials='J' surname='Halpern' fullname='Joel Halpern'>
    <organization />
</author>


<date month='March' year='2010' />

<abstract><t>This document specifies the Forwarding and Control Element Separation (ForCES) protocol.  ForCES protocol is used for communications between control elements(CEs) and forwarding elements (FEs) in a ForCES network element (ForCES NE).  This specification is intended to meet the ForCES protocol requirements defined in RFC3654.  Besides the ForCES protocol, this specification also defines the requirements for the Transport Mapping Layer (TML).Authors  The participants in the ForCES Protocol Team, primary co-authors and co-editors, of this protocol specification, are:  Ligang Dong (Zhejiang Gongshang University), Avri Doria (Lulea University of Technology), Ram Gopal (Nokia), Robert Haas (IBM), Jamal Hadi Salim (Znyx), Hormuzd M Khosravi (Intel), and Weiming Wang (Zhejiang Gongshang University).  Special acknowledgement goes to Joel Halpern who has done extensive editing in support of congruence between the model and this protocol specification.  Without his participation and persistence, this specification might never have been completed.</t></abstract>

</front>

<seriesInfo name='RFC' value='5810' />

</reference>




    </references>
    <references title="Informative References">
    &rfc3654;
    &rfc3746;
<!--    &femodel; -->
<reference anchor='RFC5812'>
<front>
<title>Forwarding and Control Element Separation (ForCES) Forwarding Element Model</title>

<author initials='J' surname='Halpern' fullname='Joel Halpern'>
    <organization />
</author>

<author initials='J' surname='Salim' fullname='Jamal Hadi Salim'>
    <organization />
</author>

<date month='March' year='2010' />

<abstract><t>This document defines the forwarding element (FE) model used in the Forwarding and Control Element Separation (ForCES) protocol [2].  The model represents the capabilities, state and configuration of forwarding elements within the context of the ForCES protocol, so that control elements (CEs) can control the FEs accordingly.  More specifically, the model describes the logical functions that are present in an FE, what capabilities these functions support, and how these functions are or can be interconnected.  This FE model is intended to satisfy the model requirements specified in the ForCES requirements document, RFC3654 [6].</t></abstract>

</front>

<seriesInfo name='RFC' value='5812' />

</reference>
<!--    &sctpapi; -->
<reference anchor='TSVWG-SCTPSOCKET'>
<front>
<title>Sockets API Extensions for Stream Control Transmission Protocol (SCTP)</title>

<author initials='R' surname='Stewart' fullname='Randall Stewart'>
    <organization />
</author>

<author initials='K' surname='Poon' fullname='Kacheong Poon'>
    <organization />
</author>

<author initials='M' surname='Tuexen' fullname='Michael Tuexen'>
    <organization />
</author>

<author initials='V' surname='Yasevich' fullname='Vladislav Yasevich'>
    <organization />
</author>

<author initials='P' surname='Lei' fullname='Peter Lei'>
    <organization />
</author>

<date month='January' day='19' year='2010' />

<abstract><t>This document describes a mapping of the Stream Control Transmission Protocol SCTP into a sockets API.  The benefits of this mapping include compatibility for TCP applications, access to new SCTP features and a consolidated error and event notification scheme.</t></abstract>

</front>

<seriesInfo name='Work in' value='Progress' />
</reference>

    &rfc3768;
    </references>
<?rfc rfcedstyle="yes" ?>
<section anchor="tml-work" title="Suggested SCTP TML Channel Work Implementation">
<t>
As mentioned in <xref target="work"/>,
there are two levels of TML channel work within an NE when a ForCES
node (CE or FE) is connected to multiple other ForCES nodes:

<list style = "numbers">
    <t>
	NE-level I/O work where a ForCES node (CE or FE)
	needs to choose which of the peer nodes to process.
    </t>
    <t>
	Node-level I/O work where a ForCES node, handles
	the three SCTP TML channels separately for each single 
	ForCES endpoint.
    </t>
</list>
</t>
<t>
NE-level scheduling definition is left up to the implementation and
is considered out of scope for this document.
<xref target="tml-NE"/> briefly discusses some constraints about which an
implementer needs to worry.
</t>
<t>
This document, and in particular <xref target="tml-init"/>, 
<xref target="sched-det"/>, and <xref target="tml-fin"/> discuss 
details of node-level I/O work.
</t>


<section anchor="tml-init" title="SCTP TML Channel Initialization">
<t>
As discussed in <xref target="work"/>, it is recommended that the 
FE SHOULD do socket connections to the
CE in the order of incrementing priorities, i.e., LP socket first, 
followed by MP, and ending with HP socket connection. The CE, however,
MUST NOT assume that there is ordering of socket connections from any FE.
<xref target="tmlboot"/> has more details on the expected initialization
of SCTP channel work.
</t>
</section>
<section anchor="sched-det" title="Channel Work Scheduling">
<t>
This section provides high-level details of the scheduling view
of the SCTP TML core (<xref target="sctptml_channs"/>). A practical
scheduler implementation takes care of many little details (such
as timers, work quanta, etc.) not described in this document. It is left
to the implementer to take care of those details.
</t>
<t>
The CE(s) and FE(s) are coupled together in the principles of
the scheduling scheme described here to tie together node overload
with transport congestion. The design intent is to provide the
highest possible robust work throughput for the NE under
any network or processing congestion.
</t>

<section anchor="sched-det-FE" title="FE Channel Work Scheduling">

<t>
The FE scheduling, in priority order, needs to I/O process:
<list style = "numbers">
<t>
The HP channel I/O in the following priority order:
    <list style = "numbers">
    <t>
    Transmitting back to the CE any outstanding 
    result of executed work via the HP channel transmit path.
    </t>
    <t>
    Taking new incoming work from the CE that creates 
    ForCES work to be executed by the FE. 
    </t>
    </list>
</t>
<t>
ForCES events that result in transmission of unsolicited
ForCES packets to the CE via the MP channel.
</t>
<t>
Incoming Redirect work in the form of control packets that come
from the CE via LP channel. After redirect processing, these packets
get sent out on external (to the NE) interface.
</t>
<t>
Incoming Redirect work in the form of control packets that come from 
other NEs via external (to the NE) interfaces. After some
processing, such packets are sent to the CE.
</t>

</list>
<t>
It is worth emphasizing, at this point again, that the SCTP TML
processes the channel work in strict priority. For example,
as long as there are messages to send to the CE on the HP channel,
they will be processed first until there
are no more left before processing the next priority work
(which is to read new messages on the HP channel incoming from the
CE).
</t>

</t>

</section>

<section anchor="sched-det-CE" title="CE Channel Work Scheduling">

<t>
The CE scheduling, in priority order, needs to deal with:
<list style = "numbers">
<t>
The HP channel I/O in the following priority order:
    <list style = "numbers">
    <t>
    Process incoming responses to requests of work it made to the FE(s).
    </t>
    <t>
    Transmit any outstanding HP work it needs the FE(s)
    to complete.
    </t>
    </list>
</t>
<t>
Incoming ForCES events from the FE(s) via the MP channel.
</t>
<t>
Outgoing Redirect work in the form of control packets that get sent from 
the CE via LP channel destined to external (to the NE) interface
on FE(s).
</t>
<t>
Incoming Redirect work in the form of control packets that come from 
other NEs via external interfaces (to the NE) on the FE(s).
</t>
</list>

</t>
<t>
It is worth repeating, for emphasis, that the SCTP TML
processes the channel work in strict priority. For example,
if there are messages incoming from an FE on the HP channel,
they will be processed first until there are no more left before
processing the next priority work,
which is to transmit any outstanding HP channel messages going to
the FE.
</t>

</section>

</section>
<section anchor="tml-fin" title="SCTP TML Channel Termination">
<t>
<xref target="tmlshut"/> describes a controlled disassociation
of the FE from the NE. 
</t>
<t>
It is also possible for connectivity to be lost between the FE and 
CE on one or more sockets. In cases where SCTP multi-homing features
are used for path availability, the disconnection of a socket
will only occur if all paths are unreachable; otherwise, SCTP will
ensure reachability. In the situation of a total connectivity loss
of even one SCTP socket, it is recommended that the FE and CE SHOULD
assume a state equivalent to ForCES Association Teardown being issued
and follow the sequence described in <xref target="tmlshut"/>.
</t>
<t>
A CE could also disconnect sockets to an FE to indicate an 
"emergency teardown". The "emergency teardown" may be necessary in 
cases when a CE needs to disconnect an FE but knows that an FE is busy
processing a lot of outstanding commands (some of which the FE hasn't
gotten around to processing, yet).
By virtue of the CE closing the connections, the FE will immediately
be asynchronously notified and will not have to process any 
outstanding commands from the CE.
</t>
</section>
<section anchor="tml-NE" title="SCTP TML NE-Level Channel Scheduling">
<t>
In handling NE-level I/O work, an implementation needs to worry about 
being both fair and robust across peer ForCES nodes. 
</t>
<t>
Fairness is desired
so that each peer node makes progress across the NE. For the sake of
illustration, consider two FEs connected to a CE; whereas one FE
has a few HP messages that need to be processed by the CE, another 
may have infinite HP messages. The scheduling scheme may decide to use
a quota scheduling system to ensure that the second FE does not
hog the CE cycles.
</t>
<t>
Robustness is desired so that the NE does not succumb to a Denial-of-Service (DoS)
attack from hostile entities and always achieves a maximum stable
workload processing level. For the sake of illustration, consider 
again two FEs connected to a CE. Consider FE1 as having a large
number of HP and MP messages and FE2 having a large number of
MP and LP messages. The scheduling scheme needs to ensure that
while FE1 always gets its messages processed, at some point
we allow FE2 messages to be processed. A promotion and preemption-based scheduling could be used by the CE to resolve this issue.
</t>
</section>
</section>

<section anchor="serviceint" title="Suggested Service Interface">

<t>
This section outlines a high-level service interface between
FEM/CEM and TML, the PL and TML, and between local and remote TMLs. 
The intent of this interface discussion is to provide general guidelines.
The implementer is expected to care of details and even
follow a different approach if needed.

<!--[rfced] Could the above sentence be rephrased as:

The implementer is expected to take care of the details and even
follow a different approach if needed.

Or is there another way to rephrase?  -->

</t>
<t>
The theory of operation for the PL-TML service is as follows:
<list style="numbers">
    <t>
    The PL starts up and bootstraps the TML. The end result of
    a successful TML bootstrap is that the CE TML and the FE TML
    connect to each other at the transport level.
    </t>
    <t>
    Transmission and reception of the PL messages commences after
    a successful TML bootstrap.
    The PL uses send and receive PL-TML interfaces to communicate to 
    its peers. The TML is agnostic to the nature of the messages being
    sent or received.
    The first message exchanges that happen are to establish
    ForCES association. Subsequent messages may be either
    unsolicited events from the FE PL, control message redirects
    to/from the CE to/from FE, or configuration from the 
    CE to the FE, and their responses flowing from the FE to the CE.
    </t>
    <t>
    The PL does a shutdown of the TML after terminating ForCES
    association.
    </t>
</list>
</t>
<section anchor="tmlboot" title="TML Bootstrapping">
<t>
<xref target="bootstrap"/> illustrates a flow for the TML
bootstrapped by the PL. 
</t>
<t>
When the PL starts up (possibly after some internal initialization),
it boots up the TML. The TML first interacts with the FEM/CEM and
acquires the necessary TML parameterization (<xref target="params"/>).
Next, the TML uses the information it retrieved from the FEM/CEM
interface to initialize itself.
</t>
<t>
The TML on the FE proceeds to connect the three channels to the CE.
The socket interface is used for each of the channels. The TML
continues to re-try the connections to the CE until all three channels
are connected. It is advisable that the number of connection retry attempts
and the time between each retry is also configurable via the FEM.
On failure to connect one or more channels, and after the configured
number of retry thresholds is exceeded, the TML will return an
appropriate failure indicator to the PL.
On success (as shown in <xref target="bootstrap"/>), a success indication
is presented to the PL.
</t>
<figure anchor = "bootstrap" title="SCTP TML Bootstrapping">
<preamble> </preamble>
<artwork><![CDATA[

FE PL      FE TML           FEM  CEM        CE TML              CE PL
  |            |             |    |            |                    |
  |            |             |    |            |      Bootup        |
  |            |             |    |            |<-------------------|
  |  Bootup    |             |    |            |                    |
  |----------->|             |    |get CEM info|                    |
  |            |get FEM info |    |<-----------|                    |
  |            |------------>|    ~            ~                    |
  |            ~             ~    |----------->|                    |
  |            |<------------|                 |                    |
  |            |                               |-initialize TML     |
  |            |                               |-create the 3 chans.|
  |            |                               | to listen to FEs   |
  |            |                               |                    |
  |            |-initialize TML                |Bootup success      |
  |            |-create the 3 chans. locally   |------------------->|
  |            |-connect 3 chans. remotely     |                    |
  |            |------------------------------>|                    |
  |            ~                               ~ - FE TML connected ~
  |            ~                               ~ - FE TML info init ~
  |            | channels connected            |                    |
  |            |<------------------------------|                    |
  | Bootup     |                               |                    |
  | succeeded  |                               |                    |
  |<-----------|                               |                    |
  |            |                               |                    |

]]></artwork>
</figure>
<t>
On the CE, things are slightly different.
After initializing from the CEM, the TML on the CE side
proceeds to initialize the three channels to listen to remote connections
from the FEs. The success or failure indication is passed on to the CE
PL (in the same manner as was done in the FE).
</t>
<t>
Post bootup, the CE TML waits for connections from the FEs.
Upon a successful connection by an FE, the CE TML
level keeps track of the transport-level details of the FE.
Note, at this stage only transport-level connection has been established;
ForCES-level association follows using send/receive PL-TML interfaces
(refer to <xref target="sndrcv"/> and <xref target="sndrcvflw"/>).
</t>
</section>
<section title="TML Shutdown" anchor="tmlshut">
<t>
<xref target="FEshutdown"/> shows an example of an FE shutting
down the TML. It is assumed at this point that the ForCES
Association Teardown has been issued by the CE. It should
also be noted that different implementations may have different
procedures for cleaning up state, etc.
</t>
<t>
When the FE PL issues a shutdown to its TML for a specific PL ID, the 
TML releases all the channel connections to the CE. This is achieved
by closing the sockets used to communicate to the CE. This results
in the stack sending a SCTP shutdown, which is received on the CE.
</t>
<figure anchor = "FEshutdown" title="FE Shutting Down">
<preamble> </preamble>
<artwork><![CDATA[

FE PL      FE TML                      CE TML              CE PL
  |            |                         |                    |
  |  Shutdown  |                         |                    |
  |----------->|                         |                    |
  |            |-disconnect 3 chans.     |                    |
  |            |-SCTP level shutdown     |                    |
  |            |------------------------>|                    |
  |            |                         |                    |
  |            |                         |TML detects shutdown|
  |            |                         |-FE TML info cleanup|
  |            |                         |-optionally tell PL |
  |            |                         |------------------->|
  |            |                         |                    |
  |            |- clean up any state of  |                    |
  |            |-channels disconnected   |                    |
  |            |<------------------------|                    |
  |            |-SCTP shutdown ACK       |                    |
  |            |                         |                    |
  | Shutdown   |                         |                    |
  | succeeded  |                         |                    |
  |<-----------|                         |                    |
  |            |                         |                    |

]]></artwork>
</figure>

<t>
On the CE side, a TML disconnection would result in possible
cleanup of the FE state. Optionally, depending on the implementation,
there may be need to inform the PL about the TML disconnection. The
CE-stack-level SCTP sends an acknowledgement to the FE TML in response
to the earlier SCTP shutdown.
</t>
</section>

<section title="TML Sending and Receiving" anchor="sndrcv">
<t>
The TML should be agnostic to the content of the PL messages, or 
their operations.
The PL should provide enough information to the TML for it to assign an 
appropriate priority and loss behavior to the message.
<xref target="sndrcvflw"/> shows an
example of a message exchange originated at the FE and sent to 
the CE (such as a ForCES association message), which illustrates
all the necessary service interfaces for sending and receiving.
</t>
<t>
When the FE PL sends a message to the TML, the TML is expected
to pick one of HP/MP/LP channels and send out the ForCES message.
</t>
<figure anchor = "sndrcvflw" title="Send and Recv Flow">
<preamble> </preamble>
<artwork><![CDATA[
FE PL       FE TML           CE TML                CE PL
   |            |              |                      |
   |PL send     |              |                      |
   |----------->|              |                      |
   |            |              |                      |
   |            |              |                      |
   |            |-pick channel |                      |
   |            |-TML  Send    |                      |
   |            |------------->|                      |
   |            |              |                      |
   |            |              |-TML Receive on chan. |
   |            |              |- mux to PL/PL recv   |
   |            |              |--------------------->|
   |            |              |                      ~
   |            |              |                      ~ PL Process
   |            |              |                      ~
   |            |              |  PL send             |
   |            |              |<---------------------|
   |            |              |-pick chan to send on |
   |            |              |-TML send             |
   |            |<-------------|                      |
   |            |-TML Receive  |                      |
   |            |-mux to PL    |                      |
   | PL Recv    |              |                      |
   |<---------- |              |                      |
   |            |              |                      |

]]></artwork>
</figure>
<t>
When the CE TML receives the ForCES message on the channel on which it was sent,
it demultiplexes the message to the CE PL.
</t>
<t>
The CE PL, after some processing (in this example, dealing with the
FE's association), sends the TML the response. As in the case
of FE PL, the CE TML picks the channel to send on before sending.
</t>
<t>
The processing of the ForCES message upon arrival at the FE TML and
delivery to the FE PL is similar to the CE side equivalent as shown
above in <xref target="sndrcv"/>.
</t>
</section>

</section>
  </back>
</rfc>
