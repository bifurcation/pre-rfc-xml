<?xml version="1.0" encoding="US-ASCII"?>

<!DOCTYPE rfc SYSTEM "rfc2629.dtd">

<?rfc rfcedstyle="yes" ?>
<?rfc toc="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes" ?>
<?rfc compact="yes" ?>
<?rfc subcompact="no" ?>

<rfc number="6413" category="info" submissionType="IETF" consensus="yes"
     ipr="pre5378Trust200902">
  <front>
    <title abbrev="IGP Convergence Benchmark Methodology">Benchmarking
    Methodology for Link-State IGP Data-Plane Route Convergence</title>

    <author fullname="Scott Poretsky" initials="S" surname="Poretsky">
      <organization>Allot Communications</organization>

      <address>
        <postal>
          <street>300 TradeCenter</street>

          <city>Woburn</city>

          <region>MA</region>

          <code>01801</code>

          <country>USA</country>
        </postal>

        <phone>+ 1 508 309 2179</phone>

        <email>sporetsky@allot.com</email>
      </address>
    </author>

    <author fullname="Brent Imhoff" initials="B" surname="Imhoff">
      <organization>Juniper Networks</organization>

      <address>
        <postal>
          <street>1194 North Mathilda Ave</street>

          <city>Sunnyvale</city>

          <region>CA</region>

          <code>94089</code>

          <country>USA</country>
        </postal>

        <phone>+ 1 314 378 2571</phone>

        <email>bimhoff@planetspork.com</email>
      </address>
    </author>

    <author fullname="Kris Michielsen" initials="K" surname="Michielsen">
      <organization>Cisco Systems</organization>

      <address>
        <postal>
          <street>6A De Kleetlaan</street>

          <city>Diegem</city>

          <region>BRABANT</region>

          <code>1831</code>

          <country>Belgium</country>
        </postal>

        <email>kmichiel@cisco.com</email>
      </address>
    </author>

    <date month="November" year="2011" />

    <area>Benchmarking Working Group</area>

    <abstract>
      <t>This document describes the methodology for benchmarking Link-State
      Interior Gateway Protocol (IGP) Route Convergence. The methodology is to
      be used for benchmarking IGP convergence time through externally
      observable (black-box) data-plane measurements. The methodology can be
      applied to any link-state IGP, such as IS-IS and OSPF.</t>
    </abstract>

  </front>


  <middle>
    <section title="Introduction">
      <section title="Motivation">
        <t>Convergence time is a critical performance parameter. Service
        Providers use IGP convergence time as a key metric of router design
        and architecture. Fast network convergence can be optimally achieved
        through deployment of fast converging routers. Customers of Service
        Providers use packet loss due to Interior Gateway Protocol (IGP)
        convergence as a key metric of their network service quality. IGP
        route convergence is a Direct Measure of Quality (DMOQ) when
        benchmarking the data plane. The fundamental basis by which network
        users and operators benchmark convergence is packet loss and other
        packet impairments, which are externally observable events having
        direct impact on their application performance. For this reason, it is
        important to develop a standard methodology for benchmarking
        link-state IGP convergence time through externally observable
        (black-box) data-plane measurements. All factors contributing to
        convergence time are accounted for by measuring on the data plane.</t>
      </section>

      <section title="Factors for IGP Route Convergence Time">
        <t>There are four major categories of factors contributing to the
        measured IGP convergence time. As discussed in <xref
        target="Vi02"></xref>, <xref target="Ka02"></xref>, <xref
        target="Fi02"></xref>, <xref target="Al00"></xref>, <xref
        target="Al02"></xref>, and <xref target="Fr05"></xref>, these
        categories are Event Detection, Shortest Path First (SPF) Processing,
        Link State Advertisement (LSA) / Link State Packet (LSP)
        Advertisement, and Forwarding Information Base (FIB) Update. These
        have numerous components that influence the convergence time,
        including but not limited to the list below:</t>

        <t><list style="symbols">
            <t hangText="Event Detection">Event Detection<list style="symbols">
                <t>Physical-Layer Failure/Recovery Indication Time</t>

                <t>Layer 2 Failure/Recovery Indication Time</t>

                <t>IGP Hello Dead Interval</t>
              </list></t>

            <t>SPF Processing<list style="symbols">
                <t>SPF Delay Time</t>

                <t>SPF Hold Time</t>

                <t>SPF Execution Time</t>
              </list></t>

            <t>LSA/LSP Advertisement<list style="symbols">
                <t>LSA/LSP Generation Time</t>

                <t>LSA/LSP Flood Packet Pacing</t>

                <t>LSA/LSP Retransmission Packet Pacing</t>
              </list></t>

            <t>FIB Update<list style="symbols">
                <t>Tree Build Time</t>

                <t>Hardware Update Time</t>
              </list></t>

            <t>Increased Forwarding Delay due to Queueing</t>
          </list></t>

        <t>The contribution of each of the factors listed above will vary
        with each router vendor's architecture and IGP implementation. Routers
        may have a centralized forwarding architecture, in which one
        forwarding table is calculated and referenced for all arriving
        packets, or a distributed forwarding architecture, in which the
        central forwarding table is calculated and distributed to the
        interfaces for local look-up as packets arrive. The distributed
        forwarding tables are typically maintained (loaded and changed) in
        software.</t>

        <t>The variation in router architecture and implementation
        necessitates the design of a convergence test that considers all of
        these components contributing to convergence time and is independent
        of the Device Under Test (DUT) architecture and implementation. The
        benefit of designing a test for these considerations is that it
        enables black-box testing in which knowledge of the routers' internal
        implementation is not required. It is then possible to make valid use
        of the convergence benchmarking metrics when comparing routers from
        different vendors.</t>

        <t>Convergence performance is tightly linked to the number of tasks a
        router has to deal with. 

As the most important tasks are mainly
        related to the control plane and the data plane, the more the DUT is
        stressed as in a live production environment, the closer performance
        measurement results match the ones that would be observed in a live
        production environment.</t>
      </section>

      <section title="Use of Data Plane for IGP Route Convergence Benchmarking">
        <t>Customers of Service Providers use packet loss and other packet
        impairments as metrics to calculate convergence time. Packet loss and
        other packet impairments are externally observable events having
        direct impact on customers' application performance. For this reason,
        it is important to develop a standard router benchmarking methodology
        that is a Direct Measure of Quality (DMOQ) for measuring IGP
        convergence. An additional benefit of using packet loss for
        calculation of IGP Route Convergence time is that it enables black-box
        tests to be designed. Data traffic can be offered to the Device Under
        Test (DUT), an emulated network event can be forced to occur, and
        packet loss and other impaired packets can be externally measured to
        calculate the convergence time. Knowledge of the DUT architecture and
        IGP implementation is not required. There is no need to rely on the
        DUT to produce the test results. There is no need to build intrusive
        test harnesses for the DUT. All factors contributing to convergence
        time are accounted for by measuring on the data plane.</t>

        <t>Other work of the Benchmarking Methodology Working Group (BMWG)
        focuses on characterizing single router control-plane convergence. See
        <xref target="Ma05"></xref>, <xref target="Ma05t"></xref>, and <xref
        target="Ma05c"></xref>.</t>
      </section>

      <section title="Applicability and Scope">
        <t>The methodology described in this document can be applied to IPv4
        and IPv6 traffic and link-state IGPs such as IS-IS <xref
        target="Ca90"></xref><xref target="Ho08"></xref>, OSPF <xref
        target="Mo98"></xref><xref target="Co08"></xref>, and others. IGP
        adjacencies established over any kind of tunnel (such as Traffic
        Engineering tunnels) are outside the scope of this document.
        Convergence time benchmarking in topologies with IGP
        adjacencies that are not point-to-point
        will be covered in a later document. Convergence from
        Bidirectional Forwarding Detection (BFD) is outside the scope of this
        document. Non-Stop Forwarding (NSF), Non-Stop Routing (NSR), Graceful
        Restart (GR), and any other High Availability mechanism are outside the
        scope of this document. Fast reroute mechanisms such as IP
        Fast-Reroute <xref target="Sh10i"></xref> or MPLS Fast-Reroute <xref
        target="Pa05"></xref> are outside the scope of this document.</t>
      </section>
    </section>

    <section title="Existing Definitions">
      <t>The keywords "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
      "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
      document are to be interpreted as described in BCP 14, RFC 2119 <xref
      target="Br97"></xref>. RFC 2119 defines the use of these keywords to
      help make the intent of Standards Track documents as clear as possible.
      While this document uses these keywords, this document is not a
      Standards Track document.</t>

      <t>This document uses much of the terminology defined in <xref
      target="Po11t"></xref>. For any conflicting content, this document
      supersedes <xref target="Po11t"></xref>. This document uses existing
      terminology defined in other documents issued by the Benchmarking
      Methodology Working Group (BMWG). Examples include, but are not limited
      to:</t>

      <texttable anchor="table_terminology" style="none" suppress-title="true">
        <ttcol align="left" width="50%" />

        <ttcol align="left" />

        <c>Throughput</c>

        <c><xref target="Br91" />, Section 3.17</c>

        <c>Offered Load</c>

        <c><xref target="Ma98" />, Section 3.5.2</c>

        <c>Forwarding Rate</c>

        <c><xref target="Ma98" />, Section 3.6.1</c>

        <c>Device Under Test (DUT)</c>

        <c><xref target="Ma98" />, Section 3.1.1</c>

        <c>System Under Test (SUT)</c>

        <c><xref target="Ma98" />, Section 3.1.2</c>

        <c>Out-of-Order Packet</c>

        <c><xref target="Po06" />, Section 3.3.4</c>

        <c>Duplicate Packet</c>

        <c><xref target="Po06" />, Section 3.3.5</c>

        <c>Stream</c>

        <c><xref target="Po06" />, Section 3.3.2</c>

        <c>Forwarding Delay</c>

        <c><xref target="Po06" />, Section 3.2.4</c>

        <c>IP Packet Delay Variation (IPDV)</c>

        <c><xref target="De02" />, Section 1.2</c>

        <c>Loss Period</c>

        <c><xref target="Ko02" />, Section 4</c>
      </texttable>
    </section>

    <section anchor="test_topologies" title="Test Topologies">
      <section title="Test Topology for Local Changes">
        <t>Figure <xref format="counter" target="local_nlb"></xref> shows the
        test topology to measure IGP convergence time due to local Convergence
        Events such as Local Interface failure and recovery (<xref
        target="testcase_local_link_failure"></xref>), Layer 2 session failure
        and recovery (<xref target="testcase_layer2_session_failure"></xref>),
        and IGP adjacency failure and recovery (<xref
        target="testcase_igp_adj_failure"></xref>). This topology is also used
        to measure IGP convergence time due to route withdrawal and
        re-advertisement (<xref target="testcase_route_withdrawal"></xref>)
        and to measure IGP convergence time due to route cost change (<xref
        target="testcase_route_cost_change"></xref>) Convergence Events. IGP
        adjacencies MUST be established between Tester and DUT: one on the
        Ingress Interface, one on the Preferred Egress Interface, and one on
        the Next-Best Egress Interface. For this purpose, the Tester emulates
        three routers (RTa, RTb, and RTc), each establishing one adjacency
        with the DUT.</t>

        <figure align="center" anchor="local_nlb"
                title="IGP convergence test topology for local changes">
          <artwork><![CDATA[
                   -------
                   |     | Preferred        .......
                   |     |------------------. RTb .
.......    Ingress |     | Egress Interface .......
. RTa .------------| DUT |
.......  Interface |     | Next-Best        .......
                   |     |------------------. RTc .
                   |     | Egress Interface .......
                   -------
]]></artwork>
        </figure>

        <t>Figure <xref format="counter" target="local_lb_1toN"></xref> shows
        the test topology to measure IGP convergence time due to local
        Convergence Events with a non-Equal Cost Multipath (ECMP) Preferred
        Egress Interface and ECMP Next-Best Egress
        Interfaces (<xref target="testcase_local_link_failure"></xref>). In
        this topology, the DUT is configured with each Next-Best Egress
        Interface as a member of a single ECMP set. The Preferred Egress
        Interface is not a member of an ECMP set. The Tester emulates N+2
        neighbor routers (N&gt;0): one router for the Ingress Interface (RTa),
        one router for the Preferred Egress Interface (RTb), and N routers for
        the members of the ECMP set (RTc1...RTcN). IGP adjacencies MUST be
        established between Tester and DUT: one on the Ingress Interface, one
        on the Preferred Egress Interface, and one on each member of the ECMP
        set. When the test specifies to observe the Next-Best Egress Interface
        statistics, the combined statistics for all ECMP members should be
        observed.</t>

        <figure align="center" anchor="local_lb_1toN"
                title="IGP convergence test topology for local changes with non-ECMP to ECMP convergence">
          <artwork><![CDATA[
                   -------
                   |     | Preferred        .......
                   |     |------------------. RTb .
                   |     | Egress Interface .......
                   |     |
                   |     | ECMP Set         ........
.......    Ingress |     |------------------. RTc1 .
. RTa .------------| DUT | Interface 1      ........
.......  Interface |     |       .
                   |     |       .
                   |     |       .
                   |     | ECMP Set         ........
                   |     |------------------. RTcN .
                   |     | Interface N      ........
                   -------
]]></artwork>
        </figure>
      </section>

      <section title="Test Topology for Remote Changes">
        <t>Figure <xref format="counter" target="remote_nlb"></xref> shows the
        test topology to measure IGP convergence time due to Remote Interface
        failure and recovery (<xref
        target="testcase_remote_link_failure"></xref>). In this topology, the
        two routers DUT1 and DUT2 are considered the System Under Test (SUT) and
        SHOULD be identically configured devices of the same model. IGP
        adjacencies MUST be established between Tester and SUT, one on the
        Ingress Interface, one on the Preferred Egress Interface, and one on
        the Next-Best Egress Interface. For this purpose, the Tester emulates
        three routers (RTa, RTb, and RTc). In this topology, a packet
forwarding loop, also known as micro-loop (see <xref target="Sh10"/>), may occur
transiently between DUT1 and DUT2 during convergence.</t>

        <figure align="center" anchor="remote_nlb"
                title="IGP convergence test topology for remote changes">
          <artwork><![CDATA[
                   --------
                   |      |  -------- Preferred        .......
                   |      |--| DUT2 |------------------. RTb .
.......    Ingress |      |  -------- Egress Interface .......
. RTa .------------| DUT1 |
.......  Interface |      | Next-Best                  .......
                   |      |----------------------------. RTc .
                   |      | Egress Interface           .......
                   --------
]]></artwork>
        </figure>

        <t>Figure <xref format="counter" target="remote_lb_1toN"></xref> shows
        the test topology to measure IGP convergence time due to remote
        Convergence Events with a non-ECMP Preferred Egress Interface and ECMP
        Next-Best Egress Interfaces (<xref
        target="testcase_remote_link_failure"></xref>). In this topology the
        two routers DUT1 and DUT2 are considered System Under Test (SUT) and
        MUST be identically configured devices of the same model. Router DUT1
        is configured with the Next-Best Egress Interface an ECMP set of
        interfaces. The Preferred Egress Interface of DUT1 is not a member of
        an ECMP set. The Tester emulates N+2 neighbor routers (N&gt;0), one
        for the Ingress Interface (RTa), one for DUT2 (RTb) and one for each
        member of the ECMP set (RTc1...RTcN). IGP adjacencies MUST be
        established between Tester and SUT, one on each interface of
        the SUT. For
        this purpose each of the N+2 routers emulated by the Tester
        establishes one adjacency with the SUT. In this topology, there is a
        possibility of a packet-forwarding loop that may occur transiently
        between DUT1 and DUT2 during convergence (micro-loop, see <xref
        target="Sh10"></xref>). When the test specifies to observe the
        Next-Best Egress Interface statistics, the combined statistics for all
        members of the ECMP set should be observed.</t>

        <figure align="center" anchor="remote_lb_1toN"
                title="IGP convergence test topology for remote changes with non-ECMP to ECMP convergence">
          <artwork><![CDATA[
                   --------
                   |      |  -------- Preferred        .......
                   |      |--| DUT2 |------------------. RTb .
                   |      |  -------- Egress Interface .......
                   |      |
                   |      | ECMP Set                   ........
.......    Ingress |      |----------------------------. RTc1 .
. RTa .------------| DUT1 | Interface 1                ........
.......  Interface |      |       .
                   |      |       .
                   |      |       .
                   |      | ECMP Set                   ........
                   |      |----------------------------. RTcN .
                   |      | Interface N                ........
                   --------
]]></artwork>
        </figure>
      </section>

      <section title="Test Topology for Local ECMP Changes">
        <t>Figure <xref format="counter" target="local_lb"></xref> shows the
        test topology to measure IGP convergence time due to local Convergence
        Events of a member of an Equal Cost Multipath (ECMP) set (<xref
        target="testcase_local_lb_link_failure"></xref>). In this topology,
        the DUT is configured with each egress interface as a member of a
        single ECMP set and the Tester emulates N+1 next-hop routers, one for
        the Ingress Interface (RTa) and one for each member of the ECMP set
        (RTb1...RTbN). IGP adjacencies MUST be established between Tester and
        DUT, one on the Ingress Interface and one on each member of the ECMP
        set. For this purpose, each of the N+1 routers emulated by the Tester
        establishes one adjacency with the DUT. When the test specifies to
        observe the Next-Best Egress Interface statistics, the combined
        statistics for all ECMP members except the one affected by the
        Convergence Event should be observed.</t>

        <figure align="center" anchor="local_lb"
                title="IGP convergence test topology for local ECMP changes">
          <artwork><![CDATA[
                   -------
                   |     | ECMP Set    ........
                   |     |-------------. RTb1 .
                   |     | Interface 1 ........
.......    Ingress |     |       .
. RTa .------------| DUT |       .
.......  Interface |     |       .
                   |     | ECMP Set    ........
                   |     |-------------. RTbN .
                   |     | Interface N ........
                   -------
]]></artwork>
        </figure>
      </section>

      <section title="Test Topology for Remote ECMP Changes">
        <t>Figure <xref format="counter" target="remote_lb"></xref> shows the
        test topology to measure IGP convergence time due to remote
        Convergence Events of a member of an Equal Cost Multipath (ECMP) set
        (<xref target="testcase_remote_lb_link_failure"></xref>). In this
        topology, the two routers DUT1 and DUT2 are considered the System Under
        Test (SUT) and MUST be identically configured devices of the same
        model. Router DUT1 is configured with each egress interface as a
        member of a single ECMP set, and the Tester emulates N+1 neighbor
        routers (N&gt;0), one for the Ingress Interface (RTa) and one for each
        member of the ECMP set (RTb1...RTbN). IGP adjacencies MUST be
        established between Tester and SUT, one on each interface of
        the SUT. For
        this purpose, each of the N+1 routers emulated by the Tester
        establishes one adjacency with the SUT (N-1 emulated routers are
        adjacent to DUT1 egress interfaces, one emulated router is adjacent to
        DUT1 Ingress Interface, and one emulated router is adjacent to DUT2).
        In this topology, there is a possibility of a packet-forwarding loop
        that may occur transiently between DUT1 and DUT2 during convergence
        (micro-loop, see <xref target="Sh10"></xref>). When the test specifies
        to observe the Next-Best Egress Interface statistics, the combined
        statistics for all ECMP members except the one affected by the
        Convergence Event should be observed.</t>

        <figure align="center" anchor="remote_lb"
                title="IGP convergence test topology for remote ECMP changes">
          <artwork><![CDATA[
                   --------
                   |      | ECMP Set    --------   ........
                   |      |-------------| DUT2 |---. RTb1 .
                   |      | Interface 1 --------   ........
                   |      |
                   |      | ECMP Set               ........
.......    Ingress |      |------------------------. RTb2 .
. RTa .------------| DUT1 | Interface 2            ........
.......  Interface |      |       .
                   |      |       .
                   |      |       .
                   |      | ECMP Set               ........
                   |      |------------------------. RTbN .
                   |      | Interface N            ........
                   --------
]]></artwork>
        </figure>
      </section>

      <section title="Test topology for Parallel Link Changes">
        <t>Figure <xref format="counter" target="local_parallel"></xref> shows
        the test topology to measure IGP convergence time due to local
        Convergence Events with members of a Parallel Link (<xref
        target="testcase_parallel_link_failure"></xref>). In this topology,
        the DUT is configured with each egress interface as a member of a
        Parallel Link and the Tester emulates two neighbor routers, one for
        the Ingress Interface (RTa) and one for the Parallel Link members
        (RTb). IGP adjacencies MUST be established on the Ingress Interface
        and on all N members of the Parallel Link between Tester and DUT
        (N&gt;0). For this purpose, the routers emulated by the Tester
        establishes N+1 adjacencies with the DUT. When the test specifies to
        observe the Next-Best Egress Interface statistics, the combined
        statistics for all Parallel Link members except the one affected by
        the Convergence Event should be observed.</t>

        <figure align="center" anchor="local_parallel"
                title="IGP convergence test topology for Parallel Link changes">
          <artwork><![CDATA[
                   -------                .......
                   |     | Parallel Link  .     .
                   |     |----------------.     .
                   |     | Interface 1    .     .
.......    Ingress |     |       .        .     .
. RTa .------------| DUT |       .        . RTb .
.......  Interface |     |       .        .     .
                   |     | Parallel Link  .     .
                   |     |----------------.     .
                   |     | Interface N    .     .
                   -------                .......
]]></artwork>
        </figure>
      </section>
    </section>

    <section anchor="section_conv_and_loc"
             title="Convergence Time and Loss of Connectivity Period">
      <t>Two concepts will be highlighted in this section: convergence time
      and loss of connectivity period.</t>

      <t>The Route Convergence <xref target="Po11t"></xref> time indicates the
      period in time between the Convergence Event Instant <xref
      target="Po11t"></xref> and the instant in time the DUT is ready to
      forward traffic for a specific route on its Next-Best Egress Interface
      and maintains this state for the duration of the Sustained Convergence
      Validation Time <xref target="Po11t"></xref>. To measure Route
      Convergence time, the Convergence Event Instant and the traffic received
      from the Next-Best Egress Interface need to be observed.</t>

      <t>The Route Loss of Connectivity Period <xref target="Po11t"></xref>
      indicates the time during which traffic to a specific route is lost
      following a Convergence Event until Full Convergence <xref
      target="Po11t"></xref> completes. This Route Loss of Connectivity Period
      can consist of one or more Loss Periods <xref target="Ko02"></xref>. For
      the test cases described in this document, it is expected to have a single
      Loss Period. To measure the Route Loss of Connectivity Period, the traffic
      received from the Preferred Egress Interface and the traffic received
      from the Next-Best Egress Interface need to be observed.</t>

      <t>The Route Loss of Connectivity Period is most important since that
      has a direct impact on the network user's application performance.</t>

      <t>In general, the Route Convergence time is larger than or equal to the
      Route Loss of Connectivity Period. Depending on which Convergence Event
      occurs and how this Convergence Event is applied, traffic for a route
      may still be forwarded over the Preferred Egress Interface after the
      Convergence Event Instant, before converging to the Next-Best Egress
      Interface. In that case, the Route Loss of Connectivity Period is shorter
      than the Route Convergence time.</t>

      <t>At least one condition needs to be fulfilled for Route Convergence
      time to be equal to Route Loss of Connectivity Period. The condition is
      that the Convergence Event causes an instantaneous traffic loss for the
      measured route. A fiber cut on the Preferred Egress Interface is an
      example of such a Convergence Event.</t>

      <t>A second condition applies to Route Convergence time measurements
      based on Connectivity Packet Loss <xref target="Po11t"></xref>. This
      second condition is that there is only a single Loss Period during Route
      Convergence. For the test cases described in this document, the second condition is
      expected to apply.</t>

      <section anchor="conv_time_no_instant_loss"
               title="Convergence Events without Instant Traffic Loss">
        <t>To measure convergence time benchmarks for Convergence Events
        caused by a Tester, such as an IGP cost change, the Tester MAY start
        to discard all traffic received from the Preferred Egress Interface at
        the Convergence Event Instant, or MAY separately observe packets
        received from the Preferred Egress Interface prior to the Convergence
        Event Instant. This way, these Convergence Events can be treated the
        same as Convergence Events that cause instantaneous traffic loss.</t>

        <t>To measure convergence time benchmarks without instantaneous
        traffic loss (either real or induced by the Tester) at the Convergence
        Event Instant, such as a reversion of a link failure Convergence
        Event, the Tester SHALL only observe packet statistics on the
        Next-Best Egress Interface. If using the Rate-Derived method to
        benchmark convergence times for such Convergence Events, the Tester
        MUST collect a timestamp at the Convergence Event Instant. If using a
        loss-derived method to benchmark convergence times for such
        Convergence Events, the Tester MUST measure the period in time between
        the Start Traffic Instant and the Convergence Event Instant. To
        measure this period in time, the Tester can collect timestamps at the
        Start Traffic Instant and the Convergence Event Instant.</t>

        <t>The Convergence Event Instant together with the receive rate
        observations on the Next-Best Egress Interface allow the
        derivation of the
        convergence time benchmarks using the Rate-Derived Method <xref
        target="Po11t"></xref>.</t>

        <t>By observing packets on the Next-Best Egress Interface only, the
        observed Impaired Packet count is the number of Impaired Packets
        between Traffic Start Instant and Convergence Recovery Instant. To
        measure convergence times using a loss-derived method, the Impaired
        Packet count between the Convergence Event Instant and the Convergence
        Recovery Instant is needed. The time between Traffic Start Instant and
        Convergence Event Instant must be accounted for. An example may
        clarify this.</t>

        <t>Figure <xref format="counter"
        target="non_instant_loss_example"></xref> illustrates a Convergence
        Event without instantaneous traffic loss for all routes. The top graph
        shows the Forwarding Rate over all routes, the bottom graph shows the
        Forwarding Rate for a single route Rta. &nbsp;Some time after the
        Convergence Event Instant, the Forwarding Rate observed on the Preferred
        Egress Interface starts to decrease. In the example, route Rta is the
        first route to experience packet loss at time Ta. Some time later, the
        Forwarding Rate observed on the Next-Best Egress Interface starts to
        increase. In the example, route Rta is the first route to complete
        convergence at time Ta'.</t>

        <figure align="left" anchor="non_instant_loss_example">
          <artwork align="center"><![CDATA[
     ^        
Fwd  |
Rate |-------------                    ............
     |             \                  .
     |              \                .
     |               \              .
     |                \            .
     |.................-.-.-.-.-.-.----------------
     +----+-------+---------------+----------------->
     ^    ^       ^               ^             time
    T0   CEI      Ta              Ta'

     ^        
Fwd  |
Rate |-------------               .................
Rta  |            |               .
     |            |               .
     |.............-.-.-.-.-.-.-.-.----------------
     +----+-------+---------------+----------------->
     ^    ^       ^               ^             time
    T0   CEI      Ta              Ta'

     Preferred Egress Interface: ---
     Next-Best Egress Interface: ...

     T0  : Start Traffic Instant
     CEI : Convergence Event Instant
     Ta  : the time instant packet loss for route Rta starts
     Ta' : the time instant packet impairment for route Rta ends
]]></artwork>

        </figure>

        <t>If only packets received on the Next-Best Egress Interface are
        observed, the duration of the loss period for route Rta can be
        calculated from the received packets as in Equation 1. Since the
        Convergence Event Instant is the start time for convergence time
        measurement, the period in time between T0 and CEI needs to be
        subtracted from the calculated result to become the convergence time,
        as in Equation 2.</t>

        <figure align="center">
          <artwork><![CDATA[
Next-Best Egress Interface loss period
    = (packets transmitted
        - packets received from Next-Best Egress Interface) / tx rate
    = Ta' - T0
]]></artwork>

          <postamble>Equation 1</postamble>
        </figure>

        <figure align="center">
          <artwork><![CDATA[
convergence time
    = Next-Best Egress Interface loss period - (CEI - T0)
    = Ta' - CEI
]]></artwork>

          <postamble>Equation 2</postamble>
        </figure>
      </section>

      <section title="Loss of Connectivity (LoC)">
        <t>Route Loss of Connectivity Period SHOULD be measured using the
        Route-Specific Loss-Derived Method. Since the start instant and end
        instant of the Route Loss of Connectivity Period can be different for
        each route, these cannot be accurately derived by only observing
        global statistics over all routes. An example may clarify this.</t>

        <t>Following a Convergence Event, route Rta is the first route for
        which packet impairment starts; the Route Loss of Connectivity Period
        for route Rta starts at time Ta. Route Rtb is the last route for which
        packet impairment starts; the Route Loss of Connectivity Period for
        route Rtb starts at time Tb with Tb&gt;Ta.</t>

        <figure anchor="Fig7"
                title="Example Route Loss Of Connectivity Period">
          <artwork align="center"><![CDATA[
     ^
Fwd  |
Rate |--------                       -----------
     |        \                     /
     |         \                   /
     |          \                 /
     |           \               /
     |            ---------------
     +------------------------------------------>
              ^   ^             ^    ^      time
             Ta   Tb           Ta'   Tb'
                               Tb''  Ta''
]]></artwork>
        </figure>

        <t>If the DUT implementation were such that route Rta would be the
        first route for which traffic loss ends at time Ta' (with Ta'&gt;Tb),
        and route Rtb would be the last route for which traffic loss ends at
        time Tb' (with Tb'&gt;Ta'). By only observing global traffic
        statistics over all routes, the minimum Route Loss of Connectivity
        Period would be measured as Ta'-Ta. The maximum calculated Route Loss
        of Connectivity Period would be Tb'-Ta. The real minimum and maximum
        Route Loss of Connectivity Periods are Ta'-Ta and Tb'-Tb. Illustrating
        this with the numbers Ta=0, Tb=1, Ta'=3, and Tb'=5 would give a Loss
        of Connectivity Period between 3 and 5 derived from the global traffic
        statistics, versus the real Loss of Connectivity Period between 3 and
        4.</t>

        <t>If the DUT implementation were such that route Rtb would be the
        first for which packet loss ends at time Tb'' and route Rta would be
        the last for which packet impairment ends at time Ta'', then the
        minimum and maximum Route Loss of Connectivity Periods derived by
        observing only global traffic statistics would be Tb''-Ta and
        Ta''-Ta. The real minimum and maximum Route Loss of Connectivity
        Periods are Tb''-Tb and Ta''-Ta. Illustrating this with the numbers
        Ta=0, Tb=1, Ta''=5, Tb''=3 would give a Loss of Connectivity Period
        between 3 and 5 derived from the global traffic statistics, versus the
        real Loss of Connectivity Period between 2 and 5.</t>

        <t>The two implementation variations in the above example would result
        in the same derived minimum and maximum Route Loss of Connectivity
        Periods when only observing the global packet statistics, while the
        real Route Loss of Connectivity Periods are different.</t>
      </section>
    </section>

    <section title="Test Considerations">
      <section title="IGP Selection">
        <t>The test cases described in Section <xref format="counter"
        target="test_cases"></xref> can be used for link-state IGPs, such as
        IS-IS or OSPF. The IGP convergence time test methodology is
        identical.</t>
      </section>

      <section title="Routing Protocol Configuration">
        <t>The obtained results for IGP convergence time may vary if other
        routing protocols are enabled and routes learned via those protocols
        are installed. IGP convergence times SHOULD be benchmarked without
        routes installed from other protocols. Any enabled IGP routing
        protocol extension (such as extensions for Traffic Engineering) and
        any enabled IGP routing protocol security mechanism must be reported
        with the results.</t>
      </section>

      <section title="IGP Topology">
        <t>The Tester emulates a single IGP topology. The DUT establishes IGP
        adjacencies with one or more of the emulated routers in this single
        IGP topology emulated by the Tester. See test topology details in
        <xref target="test_topologies"></xref>. The emulated topology SHOULD
        only be advertised on the DUT egress interfaces.</t>

        <t>The number of IGP routes and number of nodes in the topology, and
        the type of topology will impact the measured IGP convergence time. To
        obtain results similar to those that would be observed in an
        operational network, it is RECOMMENDED that the number of installed
        routes and nodes closely approximate that of the network (e.g.,
        thousands of routes with tens or hundreds of nodes).</t>

        <t>The number of areas (for OSPF) and levels (for IS-IS) can impact
        the benchmark results.</t>
      </section>

      <section title="Timers">
        <t>There are timers that may impact the measured IGP convergence
        times. The benchmark metrics MAY be measured at any fixed values for
        these timers. To obtain results similar to those that would be
        observed in an operational network, it is RECOMMENDED to configure the
        timers with the values as configured in the operational network.</t>

        <t>Examples of timers that may impact measured IGP convergence time
        include, but are not limited to:</t>

        <t><list style="empty">
            <t>Interface failure indication</t>

            <t>IGP hello timer</t>

            <t>IGP dead-interval or hold-timer</t>

            <t>Link State Advertisement (LSA) or Link State Packet (LSP)
            generation delay</t>

            <t>LSA or LSP flood packet pacing</t>

            <t>Route calculation delay</t>
          </list></t>
      </section>

      <section title="Interface Types">
        <t>All test cases in this methodology document can be executed with
        any interface type. The type of media may dictate which test cases may
        be executed. Each interface type has a unique mechanism for detecting
        link failures, and the speed at which that mechanism operates will
        influence the measurement results. All interfaces MUST be the same
        media and Throughput <xref target="Br91"></xref><xref
        target="Br99"></xref> for each test case. All interfaces SHOULD be
        configured as point-to-point.</t>
      </section>

      <section anchor="test_considerations_offered_load" title="Offered Load">
        <t>The Throughput of the device, as defined in <xref
        target="Br91"></xref> and benchmarked in <xref target="Br99"></xref>
        at a fixed packet size, needs to be determined over the preferred path
        and over the next-best path. The Offered Load SHOULD be the minimum of
        the measured Throughput of the device over the primary path and over
        the backup path. The packet size is selectable and MUST be recorded.
        Packet size is measured in bytes and includes the IP header and
        payload.</t>

        <t>The destination addresses for the Offered Load MUST be distributed
        such that all routes or a statistically representative subset of all
        routes are matched and each of these routes is offered an equal share
        of the Offered Load. It is RECOMMENDED to send traffic matching all
        routes, but a statistically representative subset of all routes can be
        used if required.</t>

        <t>Splitting traffic flows across multiple paths (as with ECMP or
        Parallel Link sets) is in general done by hashing on various fields on
        the IP or contained headers. The hashing is typically based on the IP
        source and destination addresses, the protocol ID, and higher-layer
        flow-dependent fields such as TCP/UDP ports. In practice, within a
        network core, the hashing is based mainly or exclusively on the IP
        source and destination addresses. Knowledge of the hashing algorithm
        used by the DUT is not always possible beforehand and would violate
        the black-box spirit of this document. Therefore, it is RECOMMENDED to
        use a randomly distributed range of source and destination IP
        addresses, protocol IDs, and higher-layer flow-dependent fields for
        the packets of the Offered Load (see also <xref
        target="Ne07"></xref>). The content of the Offered Load MUST remain
        the same during the test. It is RECOMMENDED to repeat a test multiple
        times with different random ranges of the header fields such that
        convergence time benchmarks are measured for different distributions
        of traffic over the available paths.</t>

        <t>In the Remote Interface failure test cases using topologies <xref
        format="counter" target="remote_nlb"></xref>, <xref format="counter"
        target="remote_lb_1toN"></xref>, and <xref format="counter"
        target="remote_lb"></xref>, there is a possibility of a packet-forwarding loop that may occur transiently between DUT1 and DUT2
        during convergence (micro-loop, see <xref target="Sh10"></xref>). The
        Time To Live (TTL) or Hop Limit value of the packets sent by the
        Tester may influence the benchmark measurements since it determines
        which device in the topology may send an ICMP Time Exceeded Message
        for looped packets.</t>

        <t>The duration of the Offered Load MUST be greater than the
        convergence time plus the Sustained Convergence Validation Time.</t>

        <t>Offered load should send a packet to each destination before
        sending another packet to the same destination. It is RECOMMENDED that
        the packets be transmitted in a round-robin fashion with a uniform
        interpacket delay.</t>
      </section>

      <section title="Measurement Accuracy">
        <t>Since Impaired Packet count is observed to measure the Route
        Convergence Time, the time between two successive packets offered to
        each individual route is the highest possible accuracy of any Impaired-Packet-based measurement. The higher the traffic rate offered to each
        route, the higher the possible measurement accuracy.</t>

        <t>Also see Section <xref format="counter"
        target="selection_metrics_methods"></xref> for method-specific
        measurement accuracy.</t>
      </section>

      <section title="Measurement Statistics">
        <t>The benchmark measurements may vary for each trial, due to the
        statistical nature of timer expirations, CPU scheduling, etc.
        Evaluation of the test data must be done with an understanding of
        generally accepted testing practices regarding repeatability, variance,
        and statistical significance of a small number of trials.</t>
      </section>

      <section title="Tester Capabilities">
        <t>It is RECOMMENDED that the Tester used to execute each test case
        have the following capabilities:</t>

        <t><list style="numbers">
            <t>Ability to establish IGP adjacencies and advertise a single IGP
            topology to one or more peers.</t>

            <t>Ability to measure Forwarding Delay, Duplicate Packets, and
            Out-of-Order Packets.</t>

            <t>An internal time clock to control timestamping, time
            measurements, and time calculations.</t>

            <t>Ability to distinguish traffic load received on the Preferred
            and Next-Best Interfaces <xref target="Po11t"></xref>.</t>

            <t>Ability to disable or tune specific Layer 2 and Layer 3
            protocol functions on any interface(s).</t>
          </list></t>

        <t>The Tester MAY be capable of making non-data-plane convergence
        observations and using those observations for measurements. The Tester
        MAY be capable of sending and receiving multiple traffic Streams <xref
        target="Po06"></xref>.</t>

        <t>Also see Section <xref format="counter"
        target="selection_metrics_methods"></xref> for method-specific
        capabilities.</t>
      </section>
    </section>

    <section anchor="selection_metrics_methods"
             title="Selection of Convergence Time Benchmark Metrics and Methods">
      <t>Different convergence time benchmark methods MAY be used to measure
      convergence time benchmark metrics. The Tester capabilities are
      important criteria to select a specific convergence time benchmark
      method. The criteria to select a specific benchmark method include, but
      are not limited to:</t>

      <texttable anchor="table_tester_capabilities" style="none"
                 suppress-title="true">
        <ttcol align="left" width="50%" />

        <ttcol align="left" />

        <c>Tester capabilities:</c>

        <c>Sampling Interval, number of Stream statistics to collect</c>

        <c>Measurement accuracy:</c>

        <c>Sampling Interval, Offered Load, number of routes</c>

        <c>Test specification:</c>

        <c>number of routes</c>

        <c>DUT capabilities:</c>

        <c>Throughput, IP Packet Delay Variation</c>
      </texttable>

      <section title="Loss-Derived Method">
        <section title="Tester Capabilities">
          <t>To enable collecting statistics of Out-of-Order Packets per flow
          (see <xref target="Th00"></xref>, Section 3), the Offered Load SHOULD
          consist of multiple Streams <xref target="Po06"></xref>, and each
          Stream SHOULD consist of a single flow. If sending multiple
          Streams, the measured traffic statistics for all Streams MUST be
          added together.</t>

          <t>In order to verify Full Convergence completion and the Sustained
          Convergence Validation Time, the Tester MUST measure Forwarding Rate
          each Packet Sampling Interval.</t>

          <t>The total number of Impaired Packets between the start of the
          traffic and the end of the Sustained Convergence Validation Time is
          used to calculate the Loss-Derived Convergence Time.</t>
        </section>

        <section title="Benchmark Metrics">
          <t>The Loss-Derived Method can be used to measure the Loss-Derived
          Convergence Time, which is the average convergence time over all
          routes, and to measure the Loss-Derived Loss of Connectivity Period,
          which is the average Route Loss of Connectivity Period over all
          routes.</t>
        </section>

        <section title="Measurement Accuracy">
          <t>The actual value falls within the accuracy interval [-(number of
          destinations/Offered Load), +(number of destinations/Offered Load)]
          around the value as measured using the Loss-Derived Method.</t>
        </section>
      </section>

      <section title="Rate-Derived Method">
        <section title="Tester Capabilities">
          <t>To enable collecting statistics of Out-of-Order Packets per flow
          (see <xref target="Th00"></xref>, Section 3), the Offered Load SHOULD
          consist of multiple Streams <xref target="Po06"></xref>, and each
          Stream SHOULD consist of a single flow. If sending multiple
          Streams, the measured traffic statistics for all Streams MUST be
          added together.</t>

          <t>The Tester measures Forwarding Rate each Sampling Interval. The
          Packet Sampling Interval influences the observation of the different
          convergence time instants. If the Packet Sampling Interval is large
          compared to the time between the convergence time instants, then the
          different time instants may not be easily identifiable from the
          Forwarding Rate observation. The presence of IP Packet Delay
          Variation (IPDV) <xref target="De02"></xref> may cause fluctuations
          of the Forwarding Rate observation and can prevent correct
          observation of the different convergence time instants.</t>

          <t>The Packet Sampling Interval MUST be larger than or equal to the
          time between two consecutive packets to the same destination. For
          maximum accuracy, the value for the Packet Sampling Interval SHOULD
          be as small as possible, but the presence of IPDV may require the use of
          a larger Packet Sampling Interval. The Packet Sampling Interval MUST
          be reported.</t>

          <t>IPDV causes fluctuations in the number of received packets during
          each Packet Sampling Interval. To account for the presence of IPDV
          in determining if a convergence instant has been reached, Forwarding
          Delay SHOULD be observed during each Packet Sampling Interval. The
          minimum and maximum number of packets expected in a Packet Sampling
          Interval in presence of IPDV can be calculated with Equation 3.</t>

          <figure align="center">
            <artwork><![CDATA[
number of packets expected in a Packet Sampling Interval
  in presence of IP Packet Delay Variation
    = expected number of packets without IP Packet Delay Variation
      +/-( (maxDelay - minDelay) * Offered Load)
where minDelay and maxDelay indicate (respectively) the minimum and
  maximum Forwarding Delay of packets received during the Packet
  Sampling Interval
]]></artwork>

            <postamble>Equation 3</postamble>
          </figure>

          <t>To determine if a convergence instant has been reached, the number
          of packets received in a Packet Sampling Interval is compared with
          the range of expected number of packets calculated in Equation
          3.</t>
        </section>

        <section title="Benchmark Metrics">
          <t>The Rate-Derived Method SHOULD be used to measure First Route
          Convergence Time and Full Convergence Time. It SHOULD NOT be used to
          measure Loss of Connectivity Period (see Section <xref
          format="counter" target="section_conv_and_loc"></xref>).</t>
        </section>

        <section title="Measurement Accuracy">
          <t>The measurement accuracy interval of the Rate-Derived Method
          depends on the metric being measured or calculated and the
          characteristics of the related transition. IP Packet Delay Variation
          (IPDV) <xref target="De02"></xref> adds uncertainty to the amount of
          packets received in a Packet Sampling Interval, and this uncertainty
          adds to the measurement error. The effect of IPDV is not accounted
          for in the calculation of the accuracy intervals below. 

IPDV is of importance for the convergence instants where a variation in
Forwarding Rate needs to be observed. This is applicable to the Convergence
Recovery Instant for all topologies, and
for topologies with ECMP it also applies to the Convergence Event Instant and
the First
Route Convergence Instant.
          and for topologies with ECMP also Convergence Event Instant and
          First Route Convergence Instant).</t>

          <t>If the Convergence Event Instant is observed on the data plane
          using the Rate Derived Method, it needs to be instantaneous for all
          routes (see <xref format="default"
          target="conv_time_no_instant_loss"></xref>). The actual value of the
          Convergence Event Instant falls within the accuracy interval
          [&ndash;(Packet Sampling Interval + 1/Offered Load), +0] around the
          value as measured using the Rate-Derived Method.</t>

          <t>If the Convergence Recovery Transition is non-instantaneous for
          all routes, then the actual value of the First Route Convergence
          Instant falls within the accuracy interval [&ndash;(Packet Sampling
          Interval + time between two consecutive packets to the same
          destination), +0] around the value as measured using the
          Rate-Derived Method, and the actual value of the Convergence
          Recovery Instant falls within the accuracy interval [&ndash;(2 *
          Packet Sampling Interval), &ndash;(Packet Sampling Interval - time
          between two consecutive packets to the same destination)] around the
          value as measured using the Rate-Derived Method.</t>

          <t>The term &ldquo;time between two consecutive packets to the same
          destination&rdquo; is added in the above accuracy intervals since
          packets are sent in a particular order to all destinations in a
          stream, and when part of the routes experience packet loss, it is
          unknown where in the transmit cycle packets to these routes are
          sent. This uncertainty adds to the error.</t>

          <t>The accuracy intervals of the derived metrics First Route
          Convergence Time and Rate-Derived Convergence Time are calculated
          from the above convergence instants accuracy intervals. The actual
          value of First Route Convergence Time falls within the accuracy
          interval [&ndash;(Packet Sampling Interval + time between two
          consecutive packets to the same destination), +(Packet Sampling
          Interval + 1/Offered Load)] around the calculated value. The actual
          value of Rate-Derived Convergence Time falls within the accuracy
          interval [&ndash;(2 * Packet Sampling Interval), +(time between two
          consecutive packets to the same destination + 1/Offered Load)]
          around the calculated value.</t>
        </section>
      </section>

      <section title="Route-Specific Loss-Derived Method">
        <section title="Tester Capabilities">
          <t>The Offered Load consists of multiple Streams. The Tester MUST
          measure Impaired Packet count for each Stream separately.</t>

          <t>In order to verify Full Convergence completion and the Sustained
          Convergence Validation Time, the Tester MUST measure Forwarding Rate
each Packet Sampling Interval. This measurement at each Packet
          Sampling Interval MAY be per Stream.</t>

          <t>Only the total number of Impaired Packets measured per Stream at
          the end of the Sustained Convergence Validation Time is used to
          calculate the benchmark metrics with this method.</t>
        </section>

        <section title="Benchmark Metrics">
          <t>The Route-Specific Loss-Derived Method SHOULD be used to measure
          Route-Specific Convergence Times. It is the RECOMMENDED method to
          measure Route Loss of Connectivity Period.</t>

<t>
Under the conditions explained in <xref target="section_conv_and_loc"/>, First Route Convergence
Time and Full Convergence Time, as benchmarked using Rate-Derived
Method, may be equal to the minimum and maximum (respectively) of the
Route-Specific Convergence Times.</t>

        </section>

        <section title="Measurement Accuracy">
          <t>The actual value falls within the accuracy interval [-(number of
          destinations/Offered Load), +(number of destinations/Offered Load)]
          around the value as measured using the Route-Specific Loss-Derived
          Method.</t>
        </section>
      </section>
    </section>

    <section title="Reporting Format">
      <t>For each test case, it is RECOMMENDED that the reporting tables below
      be completed. All time values SHOULD be reported with a sufficiently
      high resolution (fractions of a second sufficient to distinguish
      significant differences between measured values).</t>

      <texttable anchor="table_report_config" style="headers"
                 suppress-title="true">
        <ttcol align="left" width="50%">Parameter</ttcol>

        <ttcol align="left">Units</ttcol>

        <c>Test Case</c>

        <c>test case number</c>

        <c>Test Topology</c>

        <c>Test Topology Figure number</c>

        <c>IGP</c>

        <c>(IS-IS, OSPF, other)</c>

        <c>Interface Type</c>

        <c>(GigE, POS, ATM, other)</c>

        <c>Packet Size offered to DUT</c>

        <c>bytes</c>

        <c>Offered Load</c>

        <c>packets per second</c>

        <c>IGP Routes Advertised to DUT</c>

        <c>number of IGP routes</c>

        <c>Nodes in Emulated Network</c>

        <c>number of nodes</c>

        <c>Number of Parallel or ECMP links</c>

        <c>number of links</c>

        <c>Number of Routes Measured</c>

        <c>number of routes</c>

        <c>Packet Sampling Interval on Tester</c>

        <c>seconds</c>

        <c>Forwarding Delay Threshold</c>

        <c>seconds</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Timer Values configured on DUT:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Interface Failure Indication Delay</c>

        <c>seconds</c>

        <c>&nbsp;IGP Hello Timer</c>

        <c>seconds</c>

        <c>&nbsp;IGP Dead-Interval or Hold-Time</c>

        <c>seconds</c>

        <c>&nbsp;LSA/LSP Generation Delay</c>

        <c>seconds</c>

        <c>&nbsp;LSA/LSP Flood Packet Pacing</c>

        <c>seconds</c>

        <c>&nbsp;LSA/LSP Retransmission Packet Pacing</c>

        <c>seconds</c>

        <c>&nbsp;Route Calculation Delay</c>

        <c>seconds</c>
      </texttable>

      <t>Test Details:<list style="empty">
          <t>Describe the IGP extensions and IGP security mechanisms that are
          configured on the DUT.</t>

          <t>Describe how the various fields on the IP and contained headers
          for the packets for the Offered Load are generated (<xref
          target="test_considerations_offered_load"></xref>).</t>

          <t>If the Offered Load matches a subset of routes, describe how this
          subset is selected.</t>

          <t>Describe how the Convergence Event is applied; does it cause
          instantaneous traffic loss or not?</t>
        </list></t>

      <t>The table below should be completed for the initial Convergence Event
      and the reversion Convergence Event.</t>

      <texttable anchor="table_report_measurements" style="headers"
                 suppress-title="true">
        <ttcol align="left" width="50%">Parameter</ttcol>

        <ttcol align="left">Units</ttcol>

        <c>Convergence Event</c>

        <c>(initial or reversion)</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Traffic Forwarding Metrics:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Total number of packets offered to DUT</c>

        <c>number of packets</c>

        <c>&nbsp;Total number of packets forwarded by DUT</c>

        <c>number of packets</c>

        <c>&nbsp;Connectivity Packet Loss</c>

        <c>number of packets</c>

        <c>&nbsp;Convergence Packet Loss</c>

        <c>number of packets</c>

        <c>&nbsp;Out-of-Order Packets</c>

        <c>number of packets</c>

        <c>&nbsp;Duplicate Packets</c>

        <c>number of packets</c>

        <c>&nbsp;Excessive Forwarding Delay Packets</c>

        <c>number of packets</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Convergence Benchmarks:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Rate-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;First Route Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Full Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Loss-Derived Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;Route-Specific Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Route-Specific Convergence Time[n]</c>

        <c>array of seconds</c>

        <c>&nbsp;&nbsp;Minimum Route-Specific Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Maximum Route-Specific Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Median Route-Specific Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Average Route-Specific Convergence Time</c>

        <c>seconds</c>

        <c>&nbsp;</c>

        <c>&nbsp;</c>

        <c>Loss of Connectivity Benchmarks:</c>

        <c>&nbsp;</c>

        <c>&nbsp;Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Loss-Derived Loss of Connectivity Period</c>

        <c>seconds</c>

        <c>&nbsp;Route-Specific Loss-Derived Method:</c>

        <c>&nbsp;</c>

        <c>&nbsp;&nbsp;Route Loss of Connectivity Period[n]</c>

        <c>array of seconds</c>

        <c>&nbsp;&nbsp;Minimum Route Loss of Connectivity Period</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Maximum Route Loss of Connectivity Period</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Median Route Loss of Connectivity Period</c>

        <c>seconds</c>

        <c>&nbsp;&nbsp;Average Route Loss of Connectivity Period</c>

        <c>seconds</c>
      </texttable>
    </section>

    <section anchor="test_cases" title="Test Cases">
      <t>It is RECOMMENDED that all applicable test cases be performed for
      best characterization of the DUT. The test cases follow a generic
      procedure tailored to the specific DUT configuration and Convergence
      Event <xref target="Po11t"></xref>. This generic procedure is as
      follows:</t>

      <t><list style="numbers">
          <t>Establish DUT and Tester configurations and advertise an IGP
          topology from Tester to DUT.</t>

          <t>Send Offered Load from Tester to DUT on Ingress Interface.</t>

          <t>Verify traffic is routed correctly. Verify if traffic is
          forwarded without Impaired Packets <xref target="Po06"></xref>.</t>

          <t>Introduce Convergence Event <xref target="Po11t"></xref>.</t>

          <t>Measure First Route Convergence Time <xref
          target="Po11t"></xref>.</t>

          <t>Measure Full Convergence Time <xref target="Po11t"></xref>.</t>

          <t>Stop Offered Load.</t>

          <t>Measure Route-Specific Convergence Times, Loss-Derived
          Convergence Time, Route Loss of Connectivity Periods, and
          Loss-Derived Loss of Connectivity Period <xref
          target="Po11t"></xref>. At the same time, measure number of Impaired
          Packets <xref target="Po11t"></xref>.</t>

          <t>Wait sufficient time for queues to drain. The duration of this
          time period MUST be larger than or equal to the Forwarding Delay
          Threshold.</t>

          <t>Restart Offered Load.</t>

          <t>Reverse Convergence Event.</t>

          <t>Measure First Route Convergence Time.</t>

          <t>Measure Full Convergence Time.</t>

          <t>Stop Offered Load.</t>

          <t>Measure Route-Specific Convergence Times, Loss-Derived
          Convergence Time, Route Loss of Connectivity Periods, and
          Loss-Derived Loss of Connectivity Period. At the same time, measure
          number of Impaired Packets <xref target="Po11t"></xref>.</t>
        </list></t>

      <section title="Interface Failure and Recovery">
        <section anchor="testcase_local_link_failure"
                 title="Convergence Due to Local Interface Failure and Recovery">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for Local Interface
          failure and recovery events. The Next-Best Egress Interface can be a
          single interface (Figure <xref format="counter"
          target="local_nlb"></xref>) or an ECMP set (Figure <xref
          format="counter" target="local_lb_1toN"></xref>). The test with ECMP
          topology (Figure <xref format="counter"
          target="local_lb_1toN"></xref>) is OPTIONAL.</t>
</list></t>

          <t>Procedure:

          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figures <xref format="counter"
              target="local_nlb"></xref> or <xref format="counter"
              target="local_lb_1toN"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is forwarded over Preferred Egress
              Interface.</t>

              <t>Remove link on the Preferred Egress Interface of the DUT.
              This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time, measure number of Impaired
              Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on the Preferred Egress Interface of the
              DUT.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period.  At the same time,
              measure number of Impaired Packets.</t>
            </list></t>
        </section>

        <section anchor="testcase_remote_link_failure"
                 title="Convergence Due to Remote Interface Failure and Recovery">
          <t>Objective:

<list>
          <t>To obtain the IGP convergence measurements for Remote Interface
          failure and recovery events. The Next-Best Egress Interface can be a
          single interface (Figure <xref format="counter"
          target="remote_nlb"></xref>) or an ECMP set (Figure <xref
          format="counter" target="remote_lb_1toN"></xref>). The test with
          ECMP topology (Figure <xref format="counter"
          target="remote_lb_1toN"></xref>) is OPTIONAL.</t>
</list></t>

          <t>Procedure:

          <list style="numbers">
              <t>Advertise an IGP topology from Tester to SUT using the
              topology shown in Figures <xref format="counter"
              target="remote_nlb"></xref> or <xref format="counter"
              target="remote_lb_1toN"></xref>.</t>

              <t>Send Offered Load from Tester to SUT on Ingress
              Interface.</t>

              <t>Verify traffic is forwarded over Preferred Egress
              Interface.</t>

              <t>Remove link on the interface of the Tester connected to the
              Preferred Egress Interface of the SUT. This is the Convergence
              Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time, measure number of Impaired
              Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on the interface of the Tester connected to the
              Preferred Egress Interface of the SUT.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>

          <t>Discussion:
<list>

          <t>In this test case, there is a possibility of a packet-forwarding
          loop that may occur transiently between DUT1 and DUT2 during
          convergence (micro-loop, see <xref target="Sh10"></xref>), which may
          increase the measured convergence times and loss of connectivity
          periods.</t>
</list></t>
        </section>

        <section anchor="testcase_local_lb_link_failure"
                 title="Convergence Due to ECMP Member Local Interface Failure and Recovery">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for Local Interface
          link failure and recovery events of an ECMP Member.</t>
</list></t>


          <t>Procedure:

          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the test
              setup shown in Figure <xref format="counter"
              target="local_lb"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is forwarded over the ECMP member interface of
              the DUT that will be failed in the next step.</t>

              <t>Remove link on one of the ECMP member interfaces of the DUT.
              This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time, measure number of Impaired
              Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on the ECMP member interface of the DUT.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>
        </section>

        <section anchor="testcase_remote_lb_link_failure"
                 title="Convergence Due to ECMP Member Remote Interface Failure and Recovery">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for Remote Interface
          link failure and recovery events for an ECMP Member.</t>
</list></t>


          <t>Procedure:


          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the test
              setup shown in Figure <xref format="counter"
              target="remote_lb"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is forwarded over the ECMP member interface of
              the DUT that will be failed in the next step.</t>

              <t>Remove link on the interface of the Tester to R2. This is the
              Convergence Event Trigger.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time, measure number of Impaired
              Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on the interface of the Tester to R2.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>

          <t>Discussion:
<list>

          <t>In this test case, there is a possibility of a packet-forwarding
          loop that may occur temporarily between DUT1 and DUT2 during
          convergence (micro-loop, see <xref target="Sh10"></xref>), which may
          increase the measured convergence times and loss of connectivity
          periods.</t>
</list></t>
        </section>

        <section anchor="testcase_parallel_link_failure"
                 title="Convergence Due to Parallel Link Interface Failure and Recovery">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for local link failure
          and recovery events for a member of a parallel link. The links can
          be used for data load-balancing</t>
</list></t>

          <t>Procedure:


          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the test
              setup shown in Figure <xref format="counter"
              target="local_parallel"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is forwarded over the parallel link member
              that will be failed in the next step.</t>

              <t>Remove link on one of the parallel link member interfaces of
              the DUT. This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times and Loss-Derived
              Convergence Time. At the same time, measure number of Impaired
              Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore link on the Parallel Link member interface of the
              DUT.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>
        </section>
      </section>

      <section title="Other Failures and Recoveries">
        <section anchor="testcase_layer2_session_failure"
                 title="Convergence Due to Layer 2 Session Loss and Recovery">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for a local Layer 2
          loss and recovery.</t>
</list></t>

          <t>Procedure:


          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure <xref format="counter"
              target="local_nlb"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>Remove Layer 2 session from Preferred Egress Interface of the
              DUT. This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore Layer 2 session on Preferred Egress Interface of the
              DUT.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>

          <t>Discussion:
<list>

          <t>When removing the Layer 2 session, the physical layer must stay
          up. Configure IGP timers such that the IGP adjacency does not time
          out before Layer 2 failure is detected.</t>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the Layer 2 session is
          removed. Alternatively, the Tester SHOULD record the time the instant
          Layer 2 session is removed, and traffic loss SHOULD only be measured
          on the Next-Best Egress Interface. For loss-derived benchmarks, the
          time of the Start Traffic Instant SHOULD be recorded as well. See
          Section <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
</list></t>

        </section>

        <section anchor="testcase_igp_adj_failure"
                 title="Convergence Due to Loss and Recovery of IGP Adjacency">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for loss and recovery
          of an IGP Adjacency. The IGP adjacency is removed on the Tester by
          disabling processing of IGP routing protocol packets on the
          Tester.</t>
</list></t>

          <t>Procedure:

          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure <xref format="counter"
              target="local_nlb"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>Remove IGP adjacency from the Preferred Egress Interface
              while the Layer 2 session MUST be maintained. This is the
              Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Restore IGP session on Preferred Egress Interface of the
              DUT.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>

          <t>Discussion:
<list>

          <t>Configure Layer 2 such that Layer 2 does not time out before IGP
          adjacency failure is detected.</t>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the IGP adjacency is
          removed. Alternatively, the Tester SHOULD record the time the instant
          the IGP adjacency is removed and traffic loss SHOULD only be
          measured on the Next-Best Egress Interface. For loss-derived
          benchmarks, the time of the Start Traffic Instant SHOULD be recorded
          as well. See Section <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
</list></t>

        </section>

        <section anchor="testcase_route_withdrawal"
                 title="Convergence Due to Route Withdrawal and Re-Advertisement">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for route withdrawal
          and re-advertisement.</t>
</list></t>

          <t>Procedure:

          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure <xref format="counter"
              target="local_nlb"></xref>. The routes that will be withdrawn
              MUST be a set of leaf routes advertised by at least two nodes in
              the emulated topology. The topology SHOULD be such that before
              the withdrawal the DUT prefers the leaf routes advertised by a
              node "nodeA" via the Preferred Egress Interface, and after the
              withdrawal the DUT prefers the leaf routes advertised by a node
              "nodeB" via the Next-Best Egress Interface.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>The Tester withdraws the set of IGP leaf routes from nodeA.
              This is the Convergence Event. The withdrawal update message
              SHOULD be a single unfragmented packet. If the routes cannot be
              withdrawn by a single packet, the messages SHOULD be sent using
              the same pacing characteristics as the DUT. The Tester MAY
              record the time it sends the withdrawal message(s).</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Re-advertise the set of withdrawn IGP leaf routes from nodeA
              emulated by the Tester. The update message SHOULD be a single
              unfragmented packet. If the routes cannot be advertised by a
              single packet, the messages SHOULD be sent using the same pacing
              characteristics as the DUT. The Tester MAY record the time it
              sends the update message(s).</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>

          <t>Discussion:
<list>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the routes are withdrawn
          by the Tester. Alternatively, the Tester SHOULD record the time the
          instant the routes are withdrawn, and traffic loss SHOULD only be
          measured on the Next-Best Egress Interface. For loss-derived
          benchmarks, the time of the Start Traffic Instant SHOULD be recorded
          as well. See Section <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
</list></t>

        </section>
      </section>

      <section title="Administrative changes">
        <section title="Convergence Due to Local Interface Administrative Changes">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for administratively
          disabling and enabling a Local Interface.</t>
</list></t>


          <t>Procedure:

          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure <xref format="counter"
              target="local_nlb"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>Administratively disable the Preferred Egress Interface of
              the DUT. This is the Convergence Event.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>Administratively enable the Preferred Egress Interface of the
              DUT.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>
        </section>

        <section anchor="testcase_route_cost_change"
                 title="Convergence Due to Cost Change">
          <t>Objective:
<list>

          <t>To obtain the IGP convergence measurements for route cost
          change.</t>
</list></t>

          <t>Procedure:


          <list style="numbers">
              <t>Advertise an IGP topology from Tester to DUT using the
              topology shown in Figure <xref format="counter"
              target="local_nlb"></xref>.</t>

              <t>Send Offered Load from Tester to DUT on Ingress
              Interface.</t>

              <t>Verify traffic is routed over Preferred Egress Interface.</t>

              <t>The Tester, emulating the neighbor node, increases the cost
              for all IGP routes at the Preferred Egress Interface of the DUT so
              that the Next-Best Egress Interface becomes the preferred path. The
              update message advertising the higher cost MUST be a single
              unfragmented packet. This is the Convergence Event. The Tester
              MAY record the time it sends the update message advertising the
              higher cost on the Preferred Egress Interface.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>

              <t>Wait sufficient time for queues to drain.</t>

              <t>Restart Offered Load.</t>

              <t>The Tester, emulating the neighbor node, decreases the cost
              for all IGP routes at the Preferred Egress Interface of the DUT so
              that the Preferred Egress Interface becomes the preferred path. The
              update message advertising the lower cost MUST be a single
              unfragmented packet.</t>

              <t>Measure First Route Convergence Time.</t>

              <t>Measure Full Convergence Time.</t>

              <t>Stop Offered Load.</t>

              <t>Measure Route-Specific Convergence Times, Loss-Derived
              Convergence Time, Route Loss of Connectivity Periods, and
              Loss-Derived Loss of Connectivity Period. At the same time,
              measure number of Impaired Packets.</t>
            </list></t>

          <t>Discussion:
<list>

          <t>To measure convergence time, traffic SHOULD start dropping on the
          Preferred Egress Interface on the instant the cost is changed by the
          Tester. Alternatively, the Tester SHOULD record the time the instant
          the cost is changed, and traffic loss SHOULD only be measured on the
          Next-Best Egress Interface. For loss-derived benchmarks, the time of
          the Start Traffic Instant SHOULD be recorded as well. See Section
          <xref format="counter"
          target="conv_time_no_instant_loss"></xref>.</t>
</list></t>
        </section>
      </section>
    </section>

    <section title="Security Considerations">
      <t>Benchmarking activities as described in this memo are limited to
      technology characterization using controlled stimuli in a laboratory
      environment, with dedicated address space and the constraints specified
      in the sections above.</t>

      <t>The benchmarking network topology will be an independent test setup
      and MUST NOT be connected to devices that may forward the test traffic
      into a production network or misroute traffic to the test management
      network.</t>

      <t>Further, benchmarking is performed on a "black-box" basis, relying
      solely on measurements observable external to the DUT/SUT.</t>

      <t>Special capabilities SHOULD NOT exist in the DUT/SUT specifically for
      benchmarking purposes. Any implications for network security arising
      from the DUT/SUT SHOULD be identical in the lab and in production
      networks.</t>
    </section>

    <section title="Acknowledgements">
      <t>Thanks to Sue Hares, Al Morton, Kevin Dubray, Ron Bonica, David Ward,
      Peter De Vriendt, Anuj Dewagan, Julien Meuric, Adrian Farrel, Stewart
      Bryant, and the Benchmarking Methodology Working Group for their
      contributions to this work.</t>
    </section>
  </middle>

  <back>
    <references title="Normative References">

      <reference anchor="Br91">
        <front>
          <title abbrev="Benchmarking Terminology">Benchmarking terminology
          for network interconnection devices</title>

          <author fullname="Scott Bradner" initials="S." surname="Bradner">
            <organization>Harvard University</organization>

            <address>
              <postal>
                <street>33 Kirkland Street</street>

                <street>William James Hall 1232</street>

                <city>Cambridge</city>

                <region>MA</region>

                <code>02138</code>

                <country>US</country>
              </postal>

              <phone>+1 617 495 3864</phone>

              <email>SOB@HARVARD.HARVARD.EDU</email>
            </address>
          </author>

          <date day="1" month="July" year="1991" />

          <abstract>
            <t>This memo discusses and defines a number of terms that are used
            in describing performance benchmarking tests and the results of
            such tests. The terms defined in this memo will be used in
            additional memos to define specific benchmarking tests and the
            suggested format to be used in reporting the results of each of
            the tests. This memo is a product of the Benchmarking Methodology
            Working Group (BMWG) of the Internet Engineering Task Force
            (IETF).</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="1242" />

        <format octets="22817" target="http://www.rfc-editor.org/rfc/rfc1242.txt"
                type="TXT" />
      </reference>


      <reference anchor="Br97">
        <front>
          <title abbrev="RFC Key Words">Key words for use in RFCs to Indicate
          Requirement Levels</title>

          <author fullname="Scott Bradner" initials="S." surname="Bradner">
            <organization>Harvard University</organization>

            <address>
              <postal>
                <street>1350 Mass. Ave.</street>

                <street>Cambridge</street>

                <street>MA 02138</street>
              </postal>

              <phone>- +1 617 495 3864</phone>

              <email>sob@harvard.edu</email>
            </address>
          </author>

          <date month="March" year="1997" />

          <area>General</area>

          <keyword>keyword</keyword>

          <abstract>
            <t>In many standards track documents several words are used to
            signify the requirements in the specification. These words are
            often capitalized. This document defines these words as they
            should be interpreted in IETF documents. Authors who follow these
            guidelines should incorporate this phrase near the beginning of
            their document: <list>
                <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL",
                "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and
                "OPTIONAL" in this document are to be interpreted as described
                in RFC 2119.</t>
              </list></t>

            <t>Note that the force of these words is modified by the
            requirement level of the document in which they are used.</t>
          </abstract>
        </front>

        <seriesInfo name="BCP" value="14" />

        <seriesInfo name="RFC" value="2119" />

        <format octets="4723" target="http://www.rfc-editor.org/rfc/rfc2119.txt"
                type="TXT" />

        <format octets="17491"
                target="http://xml.resource.org/public/rfc/html/rfc2119.html"
                type="HTML" />

        <format octets="5777"
                target="http://xml.resource.org/public/rfc/xml/rfc2119.xml"
                type="XML" />
      </reference>


      <reference anchor="Br99">
        <front>
          <title abbrev="Benchmarking Methodology">Benchmarking Methodology
          for Network Interconnect Devices</title>

          <author fullname="Scott Bradner" initials="S." surname="Bradner">
            <organization>Harvard University</organization>

            <address>
              <postal>
                <street>1350 Mass. Ave</street>

                <street>Room 813</street>

                <city>Cambridge</city>

                <region>MA</region>

                <code>02138</code>

                <country>US</country>
              </postal>

              <phone>+1 617 495 3864</phone>

              <facsimile>+1 617 496 8500</facsimile>

              <email>sob@harvard.edu</email>
            </address>
          </author>

          <author fullname="Jim McQuaid" initials="J." surname="McQuaid">
            <organization>NetScout Systems</organization>

            <address>
              <postal>
                <street>4 Westford Tech Park Drive</street>

                <city>Westford</city>

                <region>MA</region>

                <code>01886</code>

                <country>US</country>
              </postal>

              <phone>+1 978 614 4116</phone>

              <facsimile>+1 978 614 4004</facsimile>

              <email>mcquaidj@netscout.com</email>
            </address>
          </author>

          <date month="March" year="1999" />

          <abstract>
            <t>This document discusses and defines a number of tests that may
            be used to describe the performance characteristics of a network
            interconnecting device. In addition to defining the tests this
            document also describes specific formats for reporting the results
            of the tests. Appendix A lists the tests and conditions that we
            believe should be included for specific cases and gives additional
            information about testing practices. Appendix B is a reference
            listing of maximum frame rates to be used with specific frame
            sizes on various media and Appendix C gives some examples of frame
            formats to be used in testing.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="2544" />

        <format octets="66688" target="http://www.rfc-editor.org/rfc/rfc2544.txt"
                type="TXT" />
      </reference>


      <reference anchor="Ca90">
        <front>
          <title abbrev="OSI ISIS for IP and Dual Environments">Use of OSI
          IS-IS for routing in TCP/IP and dual environments</title>

          <author fullname="Ross Callon" initials="R." surname="Callon">
            <organization>Digital Equipment Corporation (DEC)</organization>

            <address>
              <postal>
                <street>550 King Street</street>

                <street>LKG 1-2/A19</street>

                <city>Littleton</city>

                <region>MA</region>

                <code>01460-1289</code>

                <country>US</country>
              </postal>

              <phone>+1 508 486 5009</phone>
            </address>
          </author>

          <date day="1" month="December" year="1990" />

          <abstract>
            <t>This RFC specifies an integrated routing protocol, based on the
            OSI Intra-Domain IS-IS Routing Protocol, which may be used as an
            interior gateway protocol (IGP) to support TCP/IP as well as OSI.
            This allows a single routing protocol to be used to support pure
            IP environments, pure OSI environments, and dual environments.
            This specification was developed by the IS-IS working group of the
            Internet Engineering Task Force.</t>

            <t>The OSI IS-IS protocol has reached a mature state, and is ready
            for implementation and operational use. The most recent version of
            the OSI IS-IS protocol is contained in ISO DP 10589. The proposed
            standard for using IS-IS for support of TCP/IP will therefore make
            use of this version (with a minor bug correction, as discussed in
            Annex B). We expect that future versions of this proposed standard
            will upgrade to the final International Standard version of IS-IS
            when available.</t>

            <t>Comments should be sent to "isis@merit.edu".</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="1195" />

        <format octets="187866"
                target="http://www.rfc-editor.org/rfc/rfc1195.txt" type="TXT" />

      </reference>


      <reference anchor="Ma98">
        <front>
          <title abbrev="Benchmarking Terminology">Benchmarking Terminology
          for LAN Switching Devices</title>

          <author fullname="Robert Mandeville" initials="R."
                  surname="Mandeville">
            <organization>European Network Laboratories (ENL)</organization>

            <address>
              <postal>
                <street>2</street>

                <street>rue Helene Boucher</street>

                <street>78286 Guyancourt Cedex</street>

                <country>France</country>
              </postal>

              <phone>+ 33 1 39 44 12 05</phone>

              <facsimile>+ 33 1 39 44 12 06</facsimile>

              <email>bob.mandeville@eunet.fr</email>
            </address>
          </author>

          <date month="February" year="1998" />

          <area>Operations</area>

          <keyword>local area network</keyword>

          <keyword>Benchmarking</keyword>
        </front>

        <seriesInfo name="RFC" value="2285" />

        <format octets="43130" target="http://www.rfc-editor.org/rfc/rfc2285.txt"
                type="TXT" />

        <format octets="58104"
                target="http://xml.resource.org/public/rfc/html/rfc2285.html"
                type="HTML" />

        <format octets="42730"
                target="http://xml.resource.org/public/rfc/xml/rfc2285.xml"
                type="XML" />
      </reference>


      <reference anchor="Mo98">
        <front>
          <title>OSPF Version 2</title>

          <author fullname="John Moy" initials="J." surname="Moy">
            <organization>Ascend Communications, Inc.</organization>

            <address>
              <postal>
                <street>1 Robbins Road</street>

                <city>Westford</city>

                <region>MA</region>

                <code>01886</code>
              </postal>

              <phone>978-952-1367</phone>

              <facsimile>978-392-2075</facsimile>

              <email>jmoy@casc.com</email>
            </address>
          </author>

          <date month="April" year="1998" />

          <area>Routing</area>

          <keyword>open shortest-path first protocol</keyword>

          <keyword>routing</keyword>

          <keyword>OSPF</keyword>

          <abstract>
            <t>This memo documents version 2 of the OSPF protocol. OSPF is a
            link-state routing protocol. It is designed to be run internal to
            a single Autonomous System. Each OSPF router maintains an
            identical database describing the Autonomous System's topology.
            From this database, a routing table is calculated by constructing
            a shortest- path tree.</t>

            <t>OSPF recalculates routes quickly in the face of topological
            changes, utilizing a minimum of routing protocol traffic. OSPF
            provides support for equal-cost multipath. An area routing
            capability is provided, enabling an additional level of routing
            protection and a reduction in routing protocol traffic. In
            addition, all OSPF routing protocol exchanges are
            authenticated.</t>

            <t>The differences between this memo and RFC 2178 are explained in
            Appendix G. All differences are backward-compatible in nature.
            Implementations of this memo and of RFCs 2178, 1583, and 1247 will
            interoperate.</t>

            <t>Please send comments to ospf@gated.cornell.edu.</t>
          </abstract>
        </front>

        <seriesInfo name="STD" value="54" />

        <seriesInfo name="RFC" value="2328" />

        <format octets="447367"
                target="http://www.rfc-editor.org/rfc/rfc2328.txt" type="TXT" />

        <format octets="466915"
                target="http://xml.resource.org/public/rfc/html/rfc2328.html"
                type="HTML" />

        <format octets="446625"
                target="http://xml.resource.org/public/rfc/xml/rfc2328.xml"
                type="XML" />
      </reference>

     <reference anchor="Po06">
        <front>
          <title>Terminology for Benchmarking Network-layer Traffic Control
          Mechanisms</title>

          <author fullname="S. Poretsky" initials="S." surname="Poretsky">
            <organization></organization>
          </author>

          <author fullname="J. Perser" initials="J." surname="Perser">
            <organization></organization>
          </author>

          <author fullname="S. Erramilli" initials="S." surname="Erramilli">
            <organization></organization>
          </author>

          <author fullname="S. Khurana" initials="S." surname="Khurana">
            <organization></organization>
          </author>

          <date month="October" year="2006" />

          <abstract>
            <t>This document describes terminology for the benchmarking of
            devices that implement traffic control using packet classification
            based on defined criteria. The terminology is to be applied to
            measurements made on the data plane to evaluate IP traffic control
            mechanisms. Rules for packet classification can be based on any
            field in the IP header, such as the Differentiated Services Code
            Point (DSCP), or any field in the packet payload, such as port
            number. This memo provides information for the Internet
            community.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="4689" />

        <format octets="62369" target="http://www.rfc-editor.org/rfc/rfc4689.txt"
                type="TXT" />
      </reference>


      <reference anchor="Co08">
        <front>
          <title>OSPF for IPv6</title>

          <author fullname="R. Coltun" initials="R." surname="Coltun">
            <organization></organization>
          </author>

          <author fullname="D. Ferguson" initials="D." surname="Ferguson">
            <organization></organization>
          </author>

          <author fullname="J. Moy" initials="J." surname="Moy">
            <organization></organization>
          </author>

          <author fullname="A. Lindem" initials="A." surname="Lindem">
            <organization></organization>
          </author>

          <date month="July" year="2008" />

          <abstract>
            <t>This document describes the modifications to OSPF to support
            version 6 of the Internet Protocol (IPv6). The fundamental
            mechanisms of OSPF (flooding, Designated Router (DR) election,
            area support, Short Path First (SPF) calculations, etc.) remain
            unchanged. However, some changes have been necessary, either due
            to changes in protocol semantics between IPv4 and IPv6, or simply
            to handle the increased address size of IPv6. These modifications
            will necessitate incrementing the protocol version from version 2
            to version 3. OSPF for IPv6 is also referred to as OSPF version 3
            (OSPFv3).&lt;/t&gt;&lt;t&gt; Changes between OSPF for IPv4, OSPF
            Version 2, and OSPF for IPv6 as described herein include the
            following. Addressing semantics have been removed from OSPF
            packets and the basic Link State Advertisements (LSAs). New LSAs
            have been created to carry IPv6 addresses and prefixes. OSPF now
            runs on a per-link basis rather than on a per-IP-subnet basis.
            Flooding scope for LSAs has been generalized. Authentication has
            been removed from the OSPF protocol and instead relies on IPv6's
            Authentication Header and Encapsulating Security Payload
            (ESP).&lt;/t&gt;&lt;t&gt; Even with larger IPv6 addresses, most
            packets in OSPF for IPv6 are almost as compact as those in OSPF
            for IPv4. Most fields and packet- size limitations present in OSPF
            for IPv4 have been relaxed. In addition, option handling has been
            made more flexible.&lt;/t&gt;&lt;t&gt; All of OSPF for IPv4's
            optional capabilities, including demand circuit support and
            Not-So-Stubby Areas (NSSAs), are also supported in OSPF for IPv6.
            [STANDARDS TRACK]</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="5340" />

        <format octets="225664"
                target="http://www.rfc-editor.org/rfc/rfc5340.txt" type="TXT" />
      </reference>

      <reference anchor="Ho08">
        <front>
          <title>Routing IPv6 with IS-IS</title>

          <author fullname="C. Hopps" initials="C." surname="Hopps">
            <organization></organization>
          </author>

          <date month="October" year="2008" />

          <abstract>
            <t>This document specifies a method for exchanging IPv6 routing
            information using the IS-IS routing protocol. The described method
            utilizes two new TLVs: a reachability TLV and an interface address
            TLV to distribute the necessary IPv6 information throughout a
            routing domain. Using this method, one can route IPv6 along with
            IPv4 and OSI using a single intra-domain routing protocol.
            [STANDARDS TRACK]</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="5308" />

        <format octets="13324" target="http://www.rfc-editor.org/rfc/rfc5308.txt"
                type="TXT" />
      </reference>

      <reference anchor="Ko02">
        <front>
          <title>One-way Loss Pattern Sample Metrics</title>

          <author fullname="R. Koodli" initials="R." surname="Koodli">
            <organization></organization>
          </author>

          <author fullname="R. Ravikanth" initials="R." surname="Ravikanth">
            <organization></organization>
          </author>

          <date month="August" year="2002" />
        </front>

        <seriesInfo name="RFC" value="3357" />

        <format octets="30570" target="http://www.rfc-editor.org/rfc/rfc3357.txt"
                type="TXT" />
      </reference>

<!-- draft-ietf-bmwg-igp-dataplane-conv-term-23 = RFC 6412 -->
      <reference anchor="Po11t">
        <front>
          <title>Terminology for Benchmarking Link-State IGP Data-Plane Route
          Convergence</title>

          <author fullname="Scott Poretsky" initials="S" surname="Poretsky">
            <organization></organization>
          </author>

          <author fullname="Brent Imhoff" initials="B" surname="Imhoff">
            <organization></organization>
          </author>

          <author fullname="Kris Michielsen" initials="K" surname="Michielsen">
            <organization></organization>
          </author>

          <date month="November" year="2011" />

        </front>

        <seriesInfo name="RFC" value="6412"/>

      </reference>

      <reference anchor="De02">
        <front>
          <title>IP Packet Delay Variation Metric for IP Performance Metrics
          (IPPM)</title>

          <author fullname="C. Demichelis" initials="C." surname="Demichelis">
            <organization></organization>
          </author>

          <author fullname="P. Chimento" initials="P." surname="Chimento">
            <organization></organization>
          </author>

          <date month="November" year="2002" />
        </front>

        <seriesInfo name="RFC" value="3393" />

        <format octets="47731"
                target="http://www.rfc-editor.org/rfc/rfc3393.txt" type="TXT" />
      </reference>

      <reference anchor="Sh10">
        <front>
          <title>A Framework for Loop-Free Convergence</title>

          <author fullname="M. Shand" initials="M." surname="Shand">
            <organization></organization>
          </author>

          <author fullname="S. Bryant" initials="S." surname="Bryant">
            <organization></organization>
          </author>

          <date month="January" year="2010" />

          <abstract>
            <t>A micro-loop is a packet forwarding loop that may occur
            transiently among two or more routers in a hop-by-hop packet
            forwarding paradigm.</t>

            <t>This framework provides a summary of the causes and
            consequences of micro-loops and enables the reader to form a
            judgement on whether micro-looping is an issue that needs to be
            addressed in specific networks. It also provides a survey of the
            currently proposed mechanisms that may be used to prevent or to
            suppress the formation of micro-loops when an IP or MPLS network
            undergoes topology change due to failure, repair, or management
            action. When sufficiently fast convergence is not available and
            the topology is susceptible to micro-loops, use of one or more of
            these mechanisms may be desirable. This document is not an
            Internet Standards Track specification; it is published for
            informational purposes.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="5715" />

        <format octets="53223"
                target="http://www.rfc-editor.org/rfc/rfc5715.txt" type="TXT" />
      </reference>

      <reference anchor="Ne07">
        <front>
          <title>Hash and Stuffing: Overlooked Factors in Network Device
          Benchmarking</title>

          <author fullname="D. Newman" initials="D." surname="Newman">
            <organization></organization>
          </author>

          <author fullname="T. Player" initials="T." surname="Player">
            <organization></organization>
          </author>

          <date month="March" year="2007" />

          <abstract>
            <t>Test engineers take pains to declare all factors that affect a
            given measurement, including intended load, packet length, test
            duration, and traffic orientation. However, current benchmarking
            practice overlooks two factors that have a profound impact on test
            results. First, existing methodologies do not require the
            reporting of addresses or other test traffic contents, even though
            these fields can affect test results. Second, "stuff" bits and
            bytes inserted in test traffic by some link-layer technologies add
            significant and variable overhead, which in turn affects test
            results. This document describes the effects of these factors;
            recommends guidelines for test traffic contents; and offers
            formulas for determining the probability of bit- and byte-stuffing
            in test traffic. This memo provides information for the Internet
            community.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="4814" />

        <format octets="59272"
                target="http://www.rfc-editor.org/rfc/rfc4814.txt" type="TXT" />
      </reference>

      <reference anchor="Ma05">
        <front>
          <title>Benchmarking Basic OSPF Single Router Control Plane
          Convergence</title>

          <author fullname="V. Manral" initials="V." surname="Manral">
            <organization></organization>
          </author>

          <author fullname="R. White" initials="R." surname="White">
            <organization></organization>
          </author>

          <author fullname="A. Shaikh" initials="A." surname="Shaikh">
            <organization></organization>
          </author>

          <date month="April" year="2005" />

          <abstract>
            <t>This document provides suggestions for measuring OSPF single
            router control plane convergence. Its initial emphasis is on the
            control plane of a single OSPF router. We do not address
            forwarding plane performance.</t>

            <t>NOTE: In this document, the word "convergence" relates to
            single router control plane convergence only. This memo provides
            information for the Internet community.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="4061" />

        <format octets="32706"
                target="http://www.rfc-editor.org/rfc/rfc4061.txt" type="TXT" />
      </reference>

      <reference anchor="Ma05t">
        <front>
          <title>OSPF Benchmarking Terminology and Concepts</title>

          <author fullname="V. Manral" initials="V." surname="Manral">
            <organization></organization>
          </author>

          <author fullname="R. White" initials="R." surname="White">
            <organization></organization>
          </author>

          <author fullname="A. Shaikh" initials="A." surname="Shaikh">
            <organization></organization>
          </author>

          <date month="April" year="2005" />

          <abstract>
            <t>This document explains the terminology and concepts used in
            OSPF benchmarking. Although some of these terms may be defined
            elsewhere (and we will refer the reader to those definitions in
            some cases) we include discussions concerning these terms, as they
            relate specifically to the tasks involved in benchmarking the OSPF
            protocol. This memo provides information for the Internet
            community.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="4062" />

        <format octets="15784"
                target="http://www.rfc-editor.org/rfc/rfc4062.txt" type="TXT" />
      </reference>

      <reference anchor="Ma05c">
        <front>
          <title>Considerations When Using Basic OSPF Convergence
          Benchmarks</title>

          <author fullname="V. Manral" initials="V." surname="Manral">
            <organization></organization>
          </author>

          <author fullname="R. White" initials="R." surname="White">
            <organization></organization>
          </author>

          <author fullname="A. Shaikh" initials="A." surname="Shaikh">
            <organization></organization>
          </author>

          <date month="April" year="2005" />

          <abstract>
            <t>This document discusses the applicability of various tests for
            measuring single router control plane convergence, specifically in
            regard to the Open Shortest First (OSPF) protocol. There are two
            general sections in this document, the first discusses advantages
            and limitations of specific OSPF convergence tests, and the second
            discusses more general pitfalls to be considered when routing
            protocol convergence is tested. This memo provides information for
            the Internet community.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="4063" />

        <format octets="23401"
                target="http://www.rfc-editor.org/rfc/rfc4063.txt" type="TXT" />
      </reference>

      <reference anchor="Sh10i">
        <front>
          <title>IP Fast Reroute Framework</title>

          <author fullname="M. Shand" initials="M." surname="Shand">
            <organization></organization>
          </author>

          <author fullname="S. Bryant" initials="S." surname="Bryant">
            <organization></organization>
          </author>

          <date month="January" year="2010" />

          <abstract>
            <t>This document provides a framework for the development of IP
            fast- reroute mechanisms that provide protection against link or
            router failure by invoking locally determined repair paths. Unlike
            MPLS fast-reroute, the mechanisms are applicable to a network
            employing conventional IP routing and forwarding. This document is
            not an Internet Standards Track specification; it is published for
            informational purposes.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="5714" />

        <format octets="32854"
                target="http://www.rfc-editor.org/rfc/rfc5714.txt" type="TXT" />
      </reference>

      <reference anchor="Pa05">
        <front>
          <title>Fast Reroute Extensions to RSVP-TE for LSP Tunnels</title>

          <author fullname="P. Pan" initials="P." surname="Pan">
            <organization></organization>
          </author>

          <author fullname="G. Swallow" initials="G." surname="Swallow">
            <organization></organization>
          </author>

          <author fullname="A. Atlas" initials="A." surname="Atlas">
            <organization></organization>
          </author>

          <date month="May" year="2005" />

          <abstract>
            <t>This document defines RSVP-TE extensions to establish backup
            label-switched path (LSP) tunnels for local repair of LSP tunnels.
            These mechanisms enable the re-direction of traffic onto backup
            LSP tunnels in 10s of milliseconds, in the event of a failure.</t>

            <t>Two methods are defined here. The one-to-one backup method
            creates detour LSPs for each protected LSP at each potential point
            of local repair. The facility backup method creates a bypass
            tunnel to protect a potential failure point; by taking advantage
            of MPLS label stacking, this bypass tunnel can protect a set of
            LSPs that have similar backup constraints. Both methods can be
            used to protect links and nodes during network failure. The
            described behavior and extensions to RSVP allow nodes to implement
            either method or both and to interoperate in a mixed network.
            [STANDARDS-TRACK]</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="4090" />

        <format octets="83965"
                target="http://www.rfc-editor.org/rfc/rfc4090.txt" type="TXT" />
      </reference>

      <reference anchor="Th00">
        <front>
          <title>Multipath Issues in Unicast and Multicast Next-Hop
          Selection</title>

          <author fullname="D. Thaler" initials="D." surname="Thaler">
            <organization></organization>
          </author>

          <author fullname="C. Hopps" initials="C." surname="Hopps">
            <organization></organization>
          </author>

          <date month="November" year="2000" />

          <abstract>
            <t>The effect of multipath routing on a forwarder is that the
            forwarder potentially has several next-hops for any given
            destination and must use some method to choose which next-hop
            should be used for a given data packet. This memo summarizes
            current practices, problems, and solutions. This memo provides
            information for the Internet community.</t>
          </abstract>
        </front>

        <seriesInfo name="RFC" value="2991" />

        <format octets="17796"
                target="http://www.rfc-editor.org/rfc/rfc2991.txt" type="TXT" />
      </reference>
    </references>

    <references title="Informative References">
      <reference anchor="Al00">
        <front>
          <title>Towards Millisecond IGP Convergence</title>

          <author fullname="C. Alaettinoglu" initials="C."
                  surname="Alaettinoglu">
            <organization></organization>
          </author>

          <author fullname="V. Jacobson" initials="V." surname="Jacobson">
            <organization></organization>

            <address>
              <postal>
                <street></street>

                <city></city>

                <region></region>

                <code></code>

                <country></country>
              </postal>

              <phone></phone>

              <facsimile></facsimile>

              <email></email>

              <uri></uri>
            </address>
          </author>

          <author fullname="H. Yu" initials="H." surname="Yu">
            <organization></organization>

            <address>
              <postal>
                <street></street>

                <city></city>

                <region></region>

                <code></code>

                <country></country>
              </postal>

              <phone></phone>

              <facsimile></facsimile>

              <email></email>

              <uri></uri>
            </address>
          </author>

          <date month="October" year="2000" />
        </front>

        <seriesInfo name="NANOG" value="20" />
      </reference>

      <reference anchor="Al02">
        <front>
          <title>ISIS Routing on the Qwest Backbone: a Recipe for Subsecond
          ISIS Convergence</title>

          <author fullname="C. Alaettinoglu" initials="C."
                  surname="Alaettinoglu">
            <organization></organization>
          </author>

          <author fullname="S. Casner" initials="S." surname="Casner">
            <organization></organization>

            <address>
              <postal>
                <street></street>

                <city></city>

                <region></region>

                <code></code>

                <country></country>
              </postal>

              <phone></phone>

              <facsimile></facsimile>

              <email></email>

              <uri></uri>
            </address>
          </author>

          <date month="February" year="2002" />
        </front>

        <seriesInfo name="NANOG" value="24" />
      </reference>

      <reference anchor="Fi02">
        <front>
          <title>Tutorial: Deploying Tight-SLA Services on an Internet
          Backbone: ISIS Fast Convergence and Differentiated Services
          Design</title>

          <author fullname="C. Filsfils" initials="C." surname="Filsfils">
            <organization></organization>
          </author>

          <date month="June" year="2002" />
        </front>

        <seriesInfo name="NANOG" value="25" />
      </reference>

      <reference anchor="Ka02">
        <front>
          <title>Why are we scared of SPF? IGP Scaling and Stability</title>

          <author fullname="D. Katz" initials="D." surname="Katz">
            <organization></organization>
          </author>

          <date month="June" year="2002" />
        </front>

        <seriesInfo name="NANOG" value="25" />
      </reference>

      <reference anchor="Vi02">
        <front>
          <title>Convergence and Restoration Techniques for ISP Interior
          Routing</title>

          <author fullname="C. Villamizar" initials="C." surname="Villamizar">
            <organization></organization>
          </author>

          <date month="June" year="2002" />
        </front>

        <seriesInfo name="NANOG" value="25" />
      </reference>

      <reference anchor="Fr05">
        <front>
          <title>Achieving SubSecond IGP Convergence in Large IP
          Networks</title>

          <author fullname="P. Francois" initials="P." surname="Francois">
            <organization></organization>
          </author>

          <author fullname="C. Filsfils" initials="C." surname="Filsfils">
            <organization></organization>

            <address>
              <postal>
                <street></street>

                <city></city>

                <region></region>

                <code></code>

                <country></country>
              </postal>

              <phone></phone>

              <facsimile></facsimile>

              <email></email>

              <uri></uri>
            </address>
          </author>

          <author fullname="J. Evans" initials="J." surname="Evans">
            <organization></organization>

            <address>
              <postal>
                <street></street>

                <city></city>

                <region></region>

                <code></code>

                <country></country>
              </postal>

              <phone></phone>

              <facsimile></facsimile>

              <email></email>

              <uri></uri>
            </address>
          </author>

          <author fullname="O. Bonaventure" initials="O."
                  surname="Bonaventure">
            <organization></organization>

            <address>
              <postal>
                <street></street>

                <city></city>

                <region></region>

                <code></code>

                <country></country>
              </postal>

              <phone></phone>

              <facsimile></facsimile>

              <email></email>

              <uri></uri>
            </address>
          </author>

          <date month="July" year="2005" />
        </front>

        <seriesInfo name="ACM SIGCOMM Computer Communication Review"
                    value="v.35 n.3" />
      </reference>
    </references>
  </back>
</rfc>
