<?xml version="1.0" encoding="US-ASCII"?>

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
  <!ENTITY RFC1034 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1034.xml">
  <!ENTITY RFC1035 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1035.xml">
  <!ENTITY RFC1995 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1995.xml">
  <!ENTITY RFC1996 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1996.xml">
  <!ENTITY RFC2826 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.2826.xml">
  <!ENTITY RFC2845 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.2845.xml">
  <!ENTITY RFC5011 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5011.xml">
  <!ENTITY RFC5936 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5936.xml">
  <!ENTITY RFC6219 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.6219.xml">
  <!ENTITY RFC6891 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.6891.xml">
  <!ENTITY RFC7720 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7720.xml">
  <!ENTITY RFC7872 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7872.xml">
  <!ENTITY RFC5890 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5890.xml">
  <!ENTITY RFC8109 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.8109.xml">
  <!ENTITY RFC8324 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.8324.xml">
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>

<?rfc toc="yes"?>
<?rfc tocappendix="yes"?>
<?rfc tocdepth="3"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes" ?>
<?rfc compact="yes" ?>
<?rfc subcompact="no" ?>

<rfc number="8483" category="info" submissionType="independent" ipr="trust200902">

  <front>
    <title>Yeti DNS Testbed</title>

    <author fullname="Linjian Song" initials="L." surname="Song" role="editor">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2nd Floor, Building 5</street>
          <street>No.58 Jing Hai Wu Lu, BDA</street>
          <city>Beijing</city>
          <region></region>
          <code>100176</code>
          <country>China</country>
        </postal>
        <email>songlinjian@gmail.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Dong Liu" initials="D." surname="Liu">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2nd Floor, Building 5</street>
	  <street>No.58 Jing Hai Wu Lu, BDA</street>
          <city>Beijing</city>
          <region></region>
          <code>100176</code>
          <country>China</country>
        </postal>
        <email>dliu@biigroup.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>


    <author fullname="Paul Vixie" initials="P." surname="Vixie" >
      <organization>TISF</organization>
      <address>
        <postal>
        <street>11400 La Honda Road</street>
        <city>Woodside</city>
        <region>California</region>
        <code>94062</code>
        <country>United States of America</country>
      </postal>
      <email>vixie@tisf.net</email>
      <uri>http://www.redbarn.org/</uri>
      </address>
    </author>

    <author fullname="Akira Kato" initials="A" surname="Kato" >
      <organization abbrev="Keio/WIDE">Keio University/WIDE Project</organization>
      <address>
        <postal>
          <street>Graduate School of Media Design, 4-1-1 Hiyoshi, Kohoku</street>
          <city>Yokohama</city>
          <region></region>
          <code>223-8526</code>
          <country>Japan</country>
        </postal>
        <email>kato@wide.ad.jp</email>
        <uri>http://www.kmd.keio.ac.jp/</uri>
      </address>
    </author>

    <author fullname="Shane Kerr" initials="S." surname="Kerr">
      <address>
        <postal>
          <street>Antoon Coolenlaan 41</street>
          <city>Uithoorn</city>
          <code>1422 GN</code>
          <country>The Netherlands</country>
        </postal>
        <email>shane@time-travellers.org</email>
      </address>
    </author>
    <date month="October" year="2018"/>

<keyword>Root Server, DNSSEC, IPv6</keyword>

    <abstract>
   <t>Yeti DNS is an experimental, non-production root server testbed that
   provides an environment where technical and operational experiments
   can safely be performed without risk to production root server
   infrastructure.  This document aims solely to document the technical
   and operational experience of deploying a system that is similar to
   but different from the Root Server system (on which the Internet's
   Domain Name System is designed and built).</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction" anchor="intro">
      <t>The Domain Name System (DNS), as originally specified in
        <xref target="RFC1034"/> and <xref target="RFC1035"/>, has
        proved to be an enduring and important platform upon which
        almost every end-user of the Internet relies. Despite its
        longevity, extensions to the protocol, new implementations,
        and refinements to DNS operations continue to emerge both
        inside and outside the IETF.</t>

      <t>The Root Server system in particular has seen technical
        innovation and development, for example, in the form of
        wide-scale anycast deployment, the mitigation of unwanted
        traffic on a global scale, the widespread deployment of
        Response Rate Limiting <xref target="RRL"/>, the introduction
        of IPv6 transport, the deployment of DNSSEC, changes in
        DNSSEC key sizes, and preparations to roll the root zone's
        Key Signing Key (KSK) and corresponding trust anchor.
        These projects created tremendous qualitative operational 
        change and required impressive caution and study prior to 
        implementation.  They took place in parallel with the 
        quantitative expansion or delegations for new TLDs (see <eref
        target="https://newgtlds.icann.org/"/>).</t>

      <t>Aspects of the operational structure of the Root Server
        system have been described in such documents as <xref
        target="TNO2009"/>, <xref target="ISC-TN-2003-1"/>, <xref
        target="RSSAC001"/>, and <xref target="RFC7720"/>. Such
        references, considered together, provide sufficient insight
        into the operations of the system as a whole that it is
        straightforward to imagine structural changes to the Root
        Server system's infrastructure and to wonder what the
        operational implications of such changes might be.</t>

      <t>The Yeti DNS Project was conceived in May 2015 with the aim of
   providing a non-production testbed that would be open for use by
   anyone from the technical community to propose or run experiments
   designed to answer these kinds of questions. Coordination for the 
   project was provided by BII, TISF, and the WIDE Project. Thus, Yeti DNS 
   is an independently coordinated project and is not affiliated with the 
   IETF, ICANN, IANA, or any Root Server Operator. The objectives of the 
   Yeti Project were set by the participants in the project based on 
   experiments that they considered would provide valuable information.</t>

  <t>
   Many volunteers collaborated to build a distributed testbed that at
   the time of writing includes 25 Yeti root servers with 16 operators
   and handles experimental traffic from individual volunteers,
   universities, DNS vendors, and distributed measurement networks.</t>

      <t>By design, the Yeti testbed system serves the root zone
        published by the IANA with only those structural modifications
        necessary to ensure that it is able to function usefully
        in the Yeti testbed system instead of the production Root
        Server system.  In particular, no delegation for any top-level
        zone is changed, added, or removed from the IANA-published
        root zone to construct the root zone served by the Yeti
        testbed system, and changes in the root zone are reflected
        in the testbed in near real-time. In this document, for
        clarity, we refer to the zone derived from the IANA-published
        root zone as the Yeti-Root zone.</t>

      <t>The Yeti DNS testbed serves a similar function to the Root
        Server system in the sense that they both serve similar
        zones: the Yeti-Root zone and the IANA-published root zone.
        However, the Yeti DNS testbed only serves clients that are
        explicitly configured to participate in the experiment,
        whereas the Root Server system serves the whole Internet.
        Since the dependent end-users and systems of the Yeti DNS
        testbed are known and their operations well-coordinated
        with those of the Yeti project, it has been possible to
        deploy structural changes in the Yeti DNS testbed with
        effective measurement and analysis, something that is
        difficult or simply impractical in the production Root
        Server system.</t>

        <t>This document describes the motivation for the Yeti project, 
          describes the Yeti testbed infrastructure, and provides the 
          technical and operational experiences of some users of the 
          Yeti testbed. 
   This document neither addresses the relevant policies under which the
   Root Server system is operated nor makes any proposal for changing
   any aspect of its implementation or operation.</t>
    </section>

    <section title="Requirements Notation and Conventions">
     <t>Through the document, any mention of "Root" with
       an uppercase "R" and without other prefix, refers to the
       "IANA Root" systems used in the production Internet. Proper
       mentions of the Yeti infrastructure will be prefixed with
       "Yeti", like "Yeti-Root zone", "Yeti DNS", and so on.</t>
    </section>

    <section title="Areas of Study" anchor="areas_of_study">
     <t>This section provides some examples of the topics that the developers
   of the Yeti DNS testbed considered important to address.  As noted in
   <xref target="intro"/>, the Yeti DNS is an independently coordinated project and
   is not affiliated with the IETF, ICANN, IANA, or any Root Server
   Operator.  Thus, the topics and areas for study were selected by (and
   for) the proponents of the Yeti project to address their own concerns
   and in the hope that the information and tools provided would be of
   wider interest.</t>

   <t>Each example included below is illustrated with indicative questions.</t>

     <section title="Implementation of a Testbed like the Root Server System">
       <t>
         <list style="symbols">
           <t>How can a testbed be constructed and deployed
             on the Internet, allowing useful public participation
             without any risk of disruption of the Root Server system?</t>

           <t>How can representative traffic be introduced into
             such a testbed such that insights into the
             impact of specific differences between the testbed and
             the Root Server system can be observed?</t>
         </list>
       </t>
     </section>

      <section title="Yeti-Root Zone Distribution">
        <t>
          <list style="symbols">
            <t>What are the scaling properties of Yeti-Root zone
              distribution as the number of Yeti-Root servers,
              Yeti-Root server instances, or intermediate distribution
              points increases?</t>
          </list>
        </t>
      </section>

      <section title="Yeti-Root Server Names and Addressing">
        <t>
          <list style="symbols">
            <t>What naming schemes other than those closely analogous
              to the use of ROOT-SERVERS.NET in the production root
              zone are practical, and what are their respective
              advantages and disadvantages?</t>

            <t>What are the risks and benefits of signing the zone that
              contains the names of the Yeti-Root servers?</t>

            <t>What automatic mechanisms might be useful to improve
              the rate at which clients of Yeti-Root servers are
              able to react to a Yeti-Root server renumbering
              event?</t>
          </list>
        </t>
      </section>

      <section title="IPv6-Only Yeti-Root Servers">
        <t>
          <list style="symbols">
            <t>Are there negative operational effects in the use of
              IPv6-only Yeti-Root servers, compared to the use of
              servers that are dual-stack?</t>

            <t>What effect does the IPv6 fragmentation model have on the
              operation of Yeti-Root servers, compared with that of IPv4?</t>
          </list>
        </t>
      </section>

      <section title="DNSSEC in the Yeti-Root Zone">
        <t>
          <list style="symbols">
            <t>Is it practical to sign the Yeti-Root zone using multiple,
              independently operated DNSSEC signers and multiple
              corresponding Zone Signing Keys (ZSKs)?</t>

            <t>To what extent is <xref target="RFC5011"/> ("Automated 
            Updates of DNS Security (DNSSEC) Trust Anchors") supported by
              resolvers?</t>

            <t>Does the KSK Rollover plan designed and in the process
              of being implemented by ICANN work as expected on the
              Yeti testbed?</t>

            <t>What is the operational impact of using much larger RSA
              key sizes in the ZSKs used in a root?</t>

            <t>What are the operational consequences of choosing
              DNSSEC algorithms other than RSA to sign a root?</t>
          </list>
        </t>
      </section>
    </section>

    <section title="Yeti DNS Testbed Infrastructure" anchor="testbed">
      <t>The purpose of the testbed is to allow DNS queries from
        stub resolvers, mediated by recursive resolvers, to be
        delivered to Yeti-Root servers, and for corresponding
        responses generated on the Yeti-Root servers to be returned,
        as illustrated in <xref target="components"/>.</t>

      <figure anchor="components" title="High-Level Testbed Components">
        <artwork><![CDATA[
    ,----------.        ,-----------.        ,------------.
    |   stub   +------> | recursive +------> | Yeti-Root  |
    | resolver | <------+ resolver  | <------+ nameserver |
    `----------'        `-----------'        `------------'
       ^                   ^                    ^
       |  appropriate      |  Yeti-Root hints;  |  Yeti-Root zone
       `- resolver         `- Yeti-Root trust   `- with DNSKEY RRset
          configured          anchor               signed by
                                                   Yeti-Root KSK
]]></artwork>
      </figure>

      <t>To use the Yeti DNS testbed, a recursive resolver must be
        configured to use the Yeti-Root servers. That configuration
        consists of a list of names and addresses for the Yeti-Root
        servers (often referred to as a "hints file") that replaces
        the corresponding hints used for the production Root Server
        system (<xref target="yeti-hints"/>).  If resolvers are 
        configured to validate DNSSEC, then they also need
        to be configured with a DNSSEC trust anchor that corresponds
        to a KSK used in the Yeti DNS Project, in place of the
        normal trust anchor set used for the Root Zone.</t>

      <t>Since the Yeti root(s) are signed with Yeti keys, rather 
        than those used by the IANA Root, corresponding changes 
        are needed in the resolver trust anchors. Corresponding
        changes are required in the Yeti-Root hints file <xref
        target="yeti-hints"/>. Those changes would be properly
        rejected as bogus by any validator using the production Root Server
        system's root zone trust anchor set.</t>

      <t>Stub resolvers become part of the Yeti DNS testbed by
        their use of recursive resolvers that are configured as
        described above.</t>

      <t>The data flow from IANA to stub resolvers through the
        Yeti testbed is illustrated in <xref target="yeti-root"/>
        and is described in more detail in the sections that
        follow.</t>

      <figure anchor="yeti-root" title="Testbed Data Flow">
        <artwork><![CDATA[
                             ,----------------.
                        ,-- / IANA Root Zone / ---.
                        |  `----------------'     |
                        |            |            |
                        |            |            |       Root Zone
,--------------.    ,---V---.    ,---V---.    ,---V---.
| Yeti Traffic |    | BII   |    | WIDE  |    | TISF  |
| Collection   |    |  DM   |    |  DM   |    |  DM   |
`----+----+----'    `---+---'    `---+---'    `---+---'
     |    |       ,-----'    ,-------'            `----.
     |    |       |          |                         |  Yeti-Root
     ^    ^       |          |                         |     Zone
     |    |   ,---V---.  ,---V---.                 ,---V---.
     |    `---+ Yeti  |  | Yeti  |  . . . . . . .  | Yeti  |
     |        | Root  |  | Root  |                 | Root  |
     |        `---+---'  `---+---'                 `---+---'
     |            |          |                         |    DNS
     |            |          |                         |  Response
     |         ,--V----------V-------------------------V--.
     `---------+              Yeti Resolvers              |
               `--------------------+---------------------'
                                    |                       DNS
                                    |                     Response
               ,--------------------V---------------------.
               |            Yeti Stub Resolvers           |
               `------------------------------------------'

The three coordinators of the Yeti DNS testbed: 
   BII : Beijing Internet Institute
   WIDE: Widely Integrated Distributed Environment Project
   TISF: A collaborative engineering and security project by Paul Vixie 
]]></artwork>
      </figure>

      <t>Note that the roots are not bound to Distribution Masters (DMs). DMs
      update their zone on a schedule described in <xref
      target="retrieval"/>.

      Each DM 
        that updates the latest zone can notify all roots, 
        so the zone transfer can happen between any DM and any root. </t>

      <section title="Root Zone Retrieval" anchor="retrieval">
        <t>The Yeti-Root zone is distributed within the
          Yeti DNS testbed through a set of internal master
          servers that are referred to as Distribution
          Masters (DMs). These server elements distribute the
          Yeti-Root zone to all Yeti-Root servers. The means
          by which the Yeti DMs construct the Yeti-Root
          zone for distribution is described below.</t>
          
        <t>Since Yeti DNS DMs do not receive DNS NOTIFY <xref
          target="RFC1996"/> messages from the Root Server system,
          a polling approach is used to determine when new revisions
          of the root zone are available from the production Root
          Server system. Each Yeti DM requests the Root Zone SOA
          record from a Root server that permits unauthenticated
          zone transfers of the root zone, and performs a zone
          transfer from that server if the retrieved value of
          SOA.SERIAL is greater than that of the last retrieved
          zone.</t>

         <t>At the time of writing, unauthenticated zone transfers of the Root
            Zone are available directly from B-Root, C-Root, F-Root, G-Root, 
            K-Root, and L-Root; two servers XFR.CJR.DNS.ICANN.ORG and   XFR.LAX.DNS.ICANN.ORG; and via FTP from sites maintained by the Root 
            Zone Maintainer and the IANA Functions Operator. The Yeti DNS testbed 
            retrieves the Root Zone using zone transfers from F-Root. The schedule 
            on which F-Root is polled by each Yeti DM is as follows:</t>

        <texttable>
          <ttcol>DM Operator</ttcol><ttcol>Time</ttcol>
          <c>BII</c><c>UTC hour + 00 minutes</c>
          <c>WIDE</c><c>UTC hour + 20 minutes</c>
          <c>TISF</c><c>UTC hour + 40 minutes</c>
        </texttable>

        <t>The Yeti DNS testbed uses multiple DMs, each of which acts
          autonomously and equivalently to its siblings. Any single
          DM can act to distribute new revisions of the Yeti-Root
          zone and is also responsible for signing the RRsets that
          are changed as part of the transformation of the Root
          Zone into the Yeti-Root zone described in <xref
          target="transformation"/>. This multiple DM model intends
          to provide a basic structure to implement the idea of 
          shared zone control as proposed in <xref target="ITI2014"/>.</t>
      </section>

      <section title="Transformation of Root Zone to Yeti-Root Zone"
        anchor="transformation">
        <t>Two distinct approaches have been deployed in the Yeti DNS
          testbed, separately, to transform the Root Zone into the
          Yeti-Root zone. At a high level, the approaches are
          equivalent in the sense that they replace a minimal set
          of information in the root zone with corresponding data
          for the Yeti DNS testbed; the mechanisms by which the
          transforms are executed are different, however.  The approaches are
          discussed in Sections <xref target="trans1" format="counter"/> and <xref
          target="trans2" format="counter"/>.</t>

        <t>A third approach has also been proposed, but not yet
          implemented. The motivations and changes implied by that
          approach are described in <xref target="trans3"/>.</t>

        <section title="ZSK and KSK Key Sets Shared between DMs"
          anchor="trans1">
          <t>The approach described here was the first to be
            implemented. It features entirely autonomous operation
            of each DM, but also requires secret key material (the
            private key in each of the Yeti-Root KSK and ZSK key pairs)
            to be distributed and maintained on each DM in a
            coordinated way.</t>

          <t>The Root Zone is transformed as follows to produce the
            Yeti-Root zone. This transformation is carried out
            autonomously on each Yeti DNS Project DM. Each DM carries
            an authentic copy of the current set of Yeti KSK and
            ZSK key pairs, synchronized between all DMs (see <xref
            target="synch"/>).</t>

          <t>
            <list style="numbers">
              <t>SOA.MNAME is set to www.yeti-dns.org.</t>

              <t>SOA.RNAME is set to &lt;dm-operator&gt;.yeti-dns.org,
                where &lt;dm&nbhy;operator&gt; is currently one of "wide",
                "bii", or "tisf".</t>
  
              <t>All DNSKEY, RRSIG, and NSEC records are removed.</t>
  
              <t>The apex Name Server (NS) RRset is removed, with the corresponding
                root server glue (A and AAAA) RRsets.</t>

              <t>A Yeti DNSKEY RRset is added to the apex, comprising
                the public parts of all Yeti KSK and ZSKs.</t>

              <t>A Yeti NS RRset is added to the apex that includes
                all Yeti-Root servers.</t>

              <t>Glue records (AAAA only, since Yeti-Root servers
                are v6-only) for all Yeti-Root servers are added.</t>

              <t>The Yeti-Root zone is signed: the NSEC chain is regenerated; 
                the Yeti KSK is used to sign the DNSKEY RRset; and the shared 
                ZSK is used to sign every other RRset.</t>
            </list>
          </t>

          <t>Note that the SOA.SERIAL value published in the Yeti-Root
            zone is identical to that found in the root zone.</t>
        </section>

        <section title="Unique ZSK per DM; No Shared KSK"
          anchor="trans2">
          <t>The approach described here was the second to be
            implemented and maintained as stable state. Each DM 
            is provisioned with its own, dedicated ZSK key pairs 
            that are not shared with other DMs. A Yeti-Root DNSKEY 
            RRset is constructed and signed
            upstream of all DMs as the union of the set of active
            Yeti-Root KSKs and the set of active ZSKs for every individual
            DM. Each DM now only requires the secret part of its
            own dedicated ZSK key pairs to be available locally,
            and no other secret key material is shared. The high-level
            approach is illustrated in <xref target="multi-ZSK"/>.</t>

          <figure anchor="multi-ZSK" title="Unique ZSK per DM">
            <artwork><![CDATA[
                         ,----------.         ,-----------.
                .--------> BII ZSK  +---------> Yeti-Root |
                | signs  `----------'  signs  `-----------'
                |
  ,-----------. |        ,----------.         ,-----------.
  | Yeti KSK  +-+--------> TISF ZSK +---------> Yeti-Root |
  `-----------' | signs  `----------'  signs  `-----------'
                |
                |        ,----------.         ,-----------.
                `--------> WIDE ZSK +---------> Yeti-Root |
                  signs  `----------'  signs  `-----------'
]]></artwork>
          </figure>

          <t>The process of retrieving the Root Zone from the Root
            Server system and replacing and signing the apex DNSKEY
            RRset no longer takes place on the DMs; instead, it
            takes place on a central Hidden Master. The production
            of signed DNSKEY RRsets is analogous to the use of
            Signed Key Responses (SKRs) produced during ICANN KSK
            key ceremonies <xref target="ICANN2010"/>.</t>

          <t>Each DM now retrieves source data (with a premodified
            and Yeti-signed DNSKEY RRset, but otherwise unchanged)
            from the Yeti DNS Hidden Master instead of from the
            Root Server system.</t>

          <t>Each DM carries out a similar transformation to that
            described in <xref target="trans1"/>, except that DMs
            no longer need to modify or sign the DNSKEY RRset, and
            the DM's unique local ZSK is used to sign every other RRset.</t>
 
        </section>

         <section title="Preserving Root Zone NSEC Chain and ZSK RRSIGs"
          anchor="trans3">
          <t>A change to the transformation described in <xref
            target="trans2"/> has been proposed as a Yeti 
            experiment called <xref target="PINZ">PINZ</xref>, which would preserve the 
            NSEC chain from the Root Zone and all RRSIG RRs
            generated using the Root Zone's ZSKs. The DNSKEY RRset
            would continue to be modified to replace the Root Zone
            KSKs, but Root Zone ZSKs would be kept intact, and the Yeti
            KSK would be used to generate replacement signatures over 
            the apex DNSKEY and NS RRsets. Source data would continue to flow from the
            Root Server system through the Hidden Master to the set
            of DMs, but no DNSSEC operations would be required on
            the DMs, and the source NSEC and most RRSIGs would remain
            intact.</t>

          <t>This approach has been suggested in order to keep minimal 
            changes from the IANA Root zone and provide cryptographically verifiable 
            confidence that no owner name in the root zone had been changed in 
            the process of producing the Yeti-Root zone from the Root Zone, thereby 
            addressing one of the concerns described in <xref target="controversy"/>
            in a way that can be verified automatically.</t>
        </section>
 
      </section>

      <section title="Yeti-Root Zone Distribution" anchor="distr">
        <t>Each Yeti DM is configured with a full list of Yeti-Root
          server addresses to send NOTIFY <xref target="RFC1996"/>
          messages to. This also forms the basis for an address-based
          access-control list for zone transfers. Authentication
          by address could be replaced with more rigorous mechanisms
          (e.g., using Transaction Signatures (TSIGs) <xref
          target="RFC2845"/>). This has not been done at the time
          of writing since the use of address-based controls avoids
          the need for the distribution of shared secrets amongst
          the Yeti-Root server operators.</t>

        <t>Individual Yeti-Root servers are configured with a full
          set of Yeti DM addresses to which SOA and AXFR queries
          may be sent in the conventional manner.</t>
      </section>

      <section title="Synchronization of Service Metadata" anchor="synch">
        <t>Changes in the Yeti DNS testbed infrastructure such as
          the addition or removal of Yeti-Root servers, renumbering
          Yeti-Root servers, or DNSSEC key rollovers require coordinated
          changes to take place on all DMs. The Yeti DNS testbed
          is subject to more frequent changes than are observed in
          the Root Server system and includes substantially more
          Yeti-Root servers than there are IANA Root Servers, and hence
          a manual change process in the Yeti testbed would be more
          likely to suffer from human error. 
An automated and cooperative 
          process was consequently implemented.</t>

        <t>
          The theory of this operation is that each DM operator runs a 
          Git repository locally, containing all service metadata involved 
          in the operation of each DM. When a change is desired and approved 
          among all Yeti coordinators, one DM operator (usually BII) updates 
          the local Git repository. A serial number in the future (in two days) 
          is chosen for when the changes become active. The DM operator then 
          pushes the changes to the Git repositories of the other two DM operators 
          who have a chance to check and edit the changes. When the serial number of the 
          root zone passes the number chosen, the changes are pulled automatically 
          to individual DMs and promoted to production.</t>

          <t>The three Git repositories are synchronized by configuring them as 
          remote servers. For example, at BII we push to all three DMs' repositories as follows:</t>

          <figure>
          <artwork>
          $ git remote -v
          origin yeticonf@yeti-conf.dns-lab.net:dm (fetch)
          origin yeticonf@yeti-conf.dns-lab.net:dm (push)
          origin yeticonf@yeti-dns.tisf.net:dm (push)
          origin yeticonf@yeti-repository.wide.ad.jp:dm (push)
          </artwork>
          </figure>
          
          <t>
          For more detailed information on DM synchronization, please see this
	  document in Yeti's GitHub repository:
          <eref target="https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Yeti-DM-Sync.md"/>. 
          </t>

      </section>

      <section title="Yeti-Root Server Naming Scheme" anchor="naming">
        <t>The current naming scheme for Root Servers was normalized
          to use single-character host names ("A" through "M") under
          the domain ROOT-SERVERS.NET, as described in <xref
          target="RSSAC023"/>. The principal benefit of this naming
          scheme was that DNS label compression could be used to
          produce a priming response that would fit within 512 bytes
          at the time it was introduced, where 512 bytes is the maximum
          DNS message size using UDP transport without EDNS(0) <xref
          target="RFC6891"/>.</t>

        <t>Yeti-Root servers do not use this optimization, but rather
          use free-form nameserver names chosen by their respective
          operators -- in other words, no attempt is made to minimize
          the size of the priming response through the use of label
          compression. 

This approach aims to challenge the need to minimize the priming 
response in a modern DNS ecosystem where EDNS(0) is prevalent.</t>


        <t>Priming responses from Yeti-Root servers (unlike those from Root
   Servers) do not always include server addresses in the additional
   section. In particular, Yeti-Root servers running BIND9 return an
          empty additional section if the configuration parameter
          "minimum-responses" is set, forcing resolvers to complete
          the priming process with a set of conventional recursive
          lookups in order to resolve addresses for each Yeti-Root
          server. The Yeti-Root servers running NSD were observed
          to return a fully populated additional section (depending, of
	  course, on the EDNS buffer size in use).</t>

        <t>Various approaches to normalize the composition of the
          priming response were considered, including:

          <list style="symbols">
            <t>Require use of DNS implementations that exhibit a
              desired behavior in the priming response.</t>

            <t>Modify nameserver software or configuration as used
              by Yeti-Root servers.</t>

            <t>Isolate the names of Yeti-Root servers in one or
              more zones that could be slaved on each Yeti-Root
              server, renaming servers as necessary, giving each a
              source of authoritative data with which the authority
              section of a priming response could be fully populated.
              This is the approach used in the Root Server system
              with the ROOT-SERVERS.NET zone.</t>
          </list>
        </t>

        <t>The potential mitigation of renaming all Yeti-Root servers
          using a scheme that would allow their names to exist directly
          in the root zone was not considered because that
          approach implies the invention of new top-level labels not
          present in the Root Zone.</t>

        <t>Given the relative infrequency of priming queries by
          individual resolvers and the additional complexity or
          other compromises implied by each of those mitigations,
          the decision was made to make no effort to ensure that
          the composition of priming responses was identical across
          servers. Even the empty additional sections generated by
          Yeti-Root servers running BIND9 seem to be sufficient for
          all resolver software tested; resolvers simply perform a
          new recursive lookup for each authoritative server name
          they need to resolve.</t>
      </section>

      <section title="Yeti-Root Servers">
        <t>Various volunteers have donated authoritative servers
          to act as Yeti-Root servers. At the time of writing, there
          are 25 Yeti-Root servers distributed globally, one of
          which is named using a label as specified in IDNA2008 <xref
	  target="RFC5890"/> (it is shown in the following list in punycode).</t>

        <texttable>
          <ttcol>Name</ttcol><ttcol>Operator</ttcol><ttcol>Location</ttcol>

          <c>bii.dns-lab.net</c><c>BII</c><c>CHINA</c>

          <c>yeti-ns.tsif.net</c><c>TSIF</c><c>USA</c>

          <c>yeti-ns.wide.ad.jp</c><c>WIDE Project</c><c>Japan</c>

          <c>yeti-ns.as59715.net</c><c>as59715</c><c>Italy</c>

          <c>dahu1.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

          <c>ns-yeti.bondis.org</c><c>Bond Internet Systems</c><c>Spain</c>

          <c>yeti-ns.ix.ru</c><c>Russia</c><c>MSK-IX</c>

          <c>yeti.bofh.priv.at</c><c>CERT Austria</c><c>Austria</c>

          <c>yeti.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

          <c>yeti-dns01.dnsworkshop.org</c><c>dnsworkshop
            /informnis</c><c>Germany</c>

          <c>dahu2.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

          <c>yeti.aquaray.com</c><c>Aqua Ray SAS</c><c>France</c>

          <c>yeti-ns.switch.ch</c><c>SWITCH</c><c>Switzerland</c>

          <c>yeti-ns.lab.nic.cl</c><c>NIC Chile</c><c>Chile</c>

          <c>yeti-ns1.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns2.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns3.dns-lab.net</c><c>BII</c><c>China</c>

          <c>ca...a23dc.yeti-dns.net</c><c>Yeti-ZA</c><c>South Africa</c>

          <c>3f...374cd.yeti-dns.net</c><c>Yeti-AU</c><c>Australia</c>

          <c>yeti1.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

          <c>xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c</c><c>ERNET
            India</c><c>India</c>

          <c>yeti-dns02.dnsworkshop.org</c><c>dnsworkshop
            /informnis</c><c>USA</c>

          <c>yeti.mind-dns.nl</c><c>Monshouwer Internet
            Diensten</c><c>Netherlands</c>

          <c>yeti-ns.datev.net</c><c>DATEV</c><c>Germany</c>

          <c>yeti.jhcloos.net.</c><c>jhcloos</c><c>USA</c>
        </texttable>

        <t>The current list of Yeti-Root servers is made available
          to a participating resolver first using a substitute hints
          file <xref target="yeti-hints"/> and subsequently by the
          usual resolver priming process <xref target="RFC8109"/>.
          All Yeti-Root servers are IPv6-only, because of the IPv6-only 
   Internet of the foreseeable future, and hence the Yeti-Root 
   hints file contains no IPv4 addresses and the Yeti-Root zone 
   contains no IPv4 glue records. Note that the rationale of an 
          IPv6-only testbed is to test whether an IPv6-only root can 
          survive any problem or impact when IPv4 is turned off, much 
          like the context of the IETF SUNSET4 WG <xref target="SUNSET4"/>.</t>

        <t>At the time of writing, all root servers within the
          Root Server system serve the ROOT-SERVERS.NET zone in
          addition to the root zone, and all but one also serve the
          ARPA zone. Yeti-Root servers serve the Yeti-Root zone
          only.</t>

        <t>Significant software diversity exists across the set of
          Yeti-Root servers, as reported by their volunteer operators 
          at the time of writing:

          <list style="symbols">
            <t>Platform: 18 of 25 Yeti-Root servers are implemented
              on a Virtual Private Server (VPS) rather than bare metal.</t>

            <t>Operating System: 15 Yeti-Root servers run
              on Linux (Ubuntu, Debian, CentOS, Red Hat, and ArchLinux); 4
              run on FreeBSD; 1 on NetBSD; and 1 on Windows Server 2016.</t>

            <t>DNS software: 16 of 25 Yeti-Root servers use BIND9
              (versions varying between 9.9.7 and 9.10.3); 4 use
              NSD (4.10 and 4.15); 2 use Knot (2.0.1 and 2.1.0); 1
              uses Bundy (1.2.0); 1 uses PowerDNS (4.1.3); and 1 uses MS DNS (10.0.14300.1000).</t>
          </list>
        </t>
      </section>

      <section title="Experimental Traffic">
        <t>For the Yeti DNS testbed to be useful as a platform for
          experimentation, it needs to carry statistically
          representative traffic. Several approaches have been taken
          to load the system with traffic, including both real-world
          traffic triggered by end-users and synthetic traffic.</t>

        <t>Resolvers that have been explicitly configured to participate 
          in the testbed, as described in <xref target="testbed"/>, are a 
          source of real-world, end-user traffic. 

Due to an efficient cache mechanism, the mean query rate
   is less than 100 qps in the Yeti testbed, but a variety of sources 
   were observed as active during 2017, as summarized in <xref target="activev6"/>.

        </t>

        <t>Synthetic traffic has been introduced to the system from
          time to time in order to increase traffic loads. Approaches
          include the use of distributed measurement platforms such
          as RIPE ATLAS to send DNS queries to Yeti-Root servers
          and the capture of traffic (sent from non-Yeti resolvers
          to the Root Server system) that was subsequently modified
          and replayed towards Yeti-Root servers.</t>
      </section>

      <section title="Traffic Capture and Analysis">
        <t>Traffic capture of queries and responses is available in the
          testbed in both Yeti resolvers and Yeti-Root servers in
          anticipation of experiments that require packet-level
          visibility into DNS traffic.</t>

        <t>Traffic capture is performed on Yeti-Root servers using
          either 
<list style="symbols"><t>dnscap <eref
          target="https://www.dns-oarc.net/tools/dnscap"/> or</t>
          <t>pcapdump, part of the pcaputils Debian package <eref
          target="https://packages.debian.org/sid/pcaputils"/>,
          with a
	  patch to facilitate triggered file upload (see <eref
	  target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985"/>).</t></list>
          PCAP-format files containing packet captures are uploaded
          using rsync to central storage.</t>
      </section>
    </section>

    <section title="Operational Experience with the Yeti DNS Testbed">
      <t>The following sections provide commentary on the operation
        and impact analyses of the Yeti DNS testbed described in
        <xref target="testbed"/>.  More detailed descriptions of
        observed phenomena are available in the Yeti DNS mailing list archives
	<eref target="http://lists.yeti-dns.org/pipermail/discuss/"/> and on
	the Yeti DNS blog <eref target="https://yeti-dns.org/blog.html"/>.</t>

      <section title="Viability of IPv6-Only Operation">
        <t>All Yeti-Root servers were deployed with IPv6 connectivity,
          and no IPv4 addresses for any Yeti-Root server were made
          available (e.g., in the Yeti hints file or in the DNS itself).
          This implementation decision constrained the Yeti-Root system
          to be v6 only.</t>

        <t>DNS implementations are generally adept at using both
          IPv4 and IPv6 when both are available. Servers that cannot
          be reliably reached over one protocol might be better
          queried over the other, to the benefit of end-users in
          the common case where DNS resolution is on the critical
          path for end-users' perception of performance. However,
          this optimization also means that systemic problems with
          one protocol can be masked by the other. By forcing all
          traffic to be carried over IPv6, the Yeti DNS testbed
          aimed to expose any such problems and make them easier
          to identify and understand. Several examples of IPv6-specific
          phenomena observed during the operation of the testbed
          are described in the sections that follow.</t>

        <t>Although the Yeti-Root servers themselves were only
          reachable using IPv6, real-world end-users often have no
          IPv6 connectivity. The testbed was also able to explore
          the degree to which IPv6-only Yeti-Root servers were able
          to serve single-stack, IPv4-only end-user populations
          through the use of dual-stack Yeti resolvers.</t>

        <section title="IPv6 Fragmentation" anchor="v6frag">
          <t>In the Root Server system, structural changes with the
            potential to increase response sizes (and hence
            fragmentation, fallback to TCP transport, or both) have
            been exercised with great care, since the impact on
            clients has been difficult to predict or measure. The
            Yeti DNS testbed is experimental and has the luxury of
            a known client base, making it far easier to make such
            changes and measure their impact.</t>

          <t>Many of the experimental design choices described in
            this document were expected to trigger larger responses.
            For example, the choice of naming scheme for Yeti-Root
            servers described in <xref target="naming"/> defeats
            label compression. It makes a large priming response
            (up to 1754 octets with 25 NS records and their corresponding glue
	    records);
            the Yeti-Root zone transformation approach described
            in <xref target="trans2"/> greatly enlarges the apex
            DNSKEY RRset especially during the KSK rollover (up to
            1975 octets with 3 ZSKs and 2 KSKs).  Therefore, an increased incidence
            of fragmentation was expected.</t>

          <t>The Yeti DNS testbed provides service on IPv6 only.

            However, middleboxes (such as firewalls and some routers) 
            are not friendly on IPv6 fragments. There are reports of a notable packet drop rate due to
            the mistreatment of middleboxes on IPv6 fragments <xref
            target="FRAGDROP"/> <xref target="RFC7872"/>.  
            One APNIC study <xref target="IPv6-frag-DNS"/> reported
            that 37% of endpoints using IPv6-capable DNS resolvers
            cannot receive a fragmented IPv6 response over UDP.</t>

          <t>To study the impact, RIPE Atlas probes were used.
            For each Yeti-Root server, an Atlas measurement was
            set up using 100 IPv6-enabled probes from five regions,
            sending a DNS query for "./IN/DNSKEY" using UDP transport
            with DO=1. This measurement, when carried out concurrently
            with a Yeti KSK rollover, further exacerbating the
            potential for fragmentation, identified a 7% failure
            rate compared with a non-fragmented control. A failure
            rate of 2% was observed with response sizes of 1414
            octets, which was surprising given the expected prevalence
            of 1500-octet (Ethernet-framed) MTUs.</t>

          <t>The consequences of fragmentation were not limited to
            failures in delivering DNS responses over UDP transport.

            There were two cases where a Yeti-Root server failed when using 
            TCP to transfer the Yeti-Root zone from a DM. DM log files 
            revealed "socket is not connected" errors corresponding
            to zone transfer requests. Further experimentation
            revealed that combinations of NetBSD 6.1, NetBSD 7.0RC1,
            FreeBSD 10.0, Debian 3.2, and VMWare ESXI 5.5 resulted
            in a high TCP Maximum Segment Size (MSS) value of 1440 octets being negotiated
            between client and server despite the presence of the
            IPV6_USE_MIN_MTU socket option, as described in <xref
            target="USE_MIN_MTU"/>. The
            mismatch appears to cause outbound segments of a size
            greater than 1280 octets to be dropped before sending.
            Setting the local TCP MSS to 1220 octets (chosen as
            1280&nbsp;-&nbsp;60, the size of the IPv6 TCP header with no other
            extension headers) was observed to be a pragmatic
            mitigation.</t>
        </section>

        <section title="Serving IPv4-Only End-Users">
          <t>Yeti resolvers have been successfully used by real-world
            end-users for general name resolution within a number of
            participant organizations, including resolution of names
            to IPv4 addresses and resolution by IPv4-only end-user
            devices.</t>

          <t>Some participants, recognizing the operational importance
            of reliability in resolver infrastructure and concerned
            about the stability of their IPv6 connectivity, chose
            to deploy Yeti resolvers in parallel to conventional
            resolvers, making both available to end-users. While
            the viability of this approach provides a useful data
            point, end-users using Yeti resolvers exclusively
            provided a better opportunity to identify and understand
            any failures in the Yeti DNS testbed infrastructure.</t>

          <t>Resolvers deployed in IPv4-only environments were
            able to join the Yeti DNS testbed by way of upstream,
            dual-stack Yeti resolvers. In one case (CERNET2),
            this was done by assigning IPv4 addresses to Yeti-Root servers and
            mapping them in dual-stack IVI translation devices <xref
            target="RFC6219"/>.</t>
        </section>
      </section>

      <section title="Zone Distribution" anchor="zone_dist">
        <t>The Yeti DNS testbed makes use of multiple DMs to
          distribute the Yeti-Root zone, an approach that would
          allow the number of Yeti-Root servers to scale to a higher
          number than could be supported by a single distribution
          source and that provided redundancy. The use of multiple
          DMs introduced some operational challenges, however, which
          are described in the following sections.</t>

        <section title="Zone Transfers">
          <t>Yeti-Root servers were configured to serve the Yeti-Root
            zone as slaves. Each slave had all DMs configured as
            masters, providing redundancy in zone synchronization.</t>

          <t>Each DM in the Yeti testbed served a Yeti-Root zone
            that was functionally equivalent but not congruent to
            that served by every other DM (see <xref target="distr"/>).
            The differences included variations in the SOA.MNAME
            field and, more critically, in the RRSIGs for everything
            other than the apex DNSKEY RRset, since signatures for
            all other RRsets are generated using a private key that
            is only available to the DM serving its particular
            variant of the zone (see Sections <xref target="trans1" format="counter"/> and
            <xref target="trans2" format="counter"/>).</t>

          <t>Incremental Zone Transfer (IXFR), as described in
            <xref target="RFC1995"/>, is a viable mechanism to use
            for zone synchronization between any Yeti-Root server and
            a consistent, single DM. However, if that Yeti-Root server
            ever selected a different DM, IXFR would no longer be a
            safe mechanism; structural changes between the incongruent
            zones on different DMs would not be included in any
            transferred delta, and the result would be a zone that was
            not internally self-consistent.  For this reason, the first
            transfer after a change of DM would require AXFR not
            IXFR.</t>
  
          <t>None of the DNS software in use on Yeti-Root servers
            supports this mixture of IXFR/AXFR according to the master
            server in use. This is unsurprising, given that the
            environment described above in the Yeti-Root system is
            idiosyncratic; conventional zone transfer graphs involve
            zones that are congruent between all nodes.  For this
            reason, all Yeti-Root servers are configured to use AXFR
            at all times, and never IXFR, to ensure that zones being
            served are internally self-consistent.</t>
        </section>

        <section title="Delays in Yeti-Root Zone Distribution">
          <t>Each Yeti DM polled the Root Server system for a new revision
            of the root zone on an interleaved schedule, as described in
            <xref target="retrieval"/>. Consequently, different DMs
            were expected to retrieve each revision of the root
            zone, and make a corresponding revision of the Yeti-Root
            zone available, at different times. The availability
            of a new revision of the Yeti-Root zone on the first
            DM would typically precede that of the last by 40
            minutes.</t>

            <t>Given this distribution mechanism, it might be expected
            that the maximum latency between the publication of a
            new revision of the root zone and the availability of the
            corresponding Yeti-Root zone on any Yeti-Root server would
            be 20 minutes, since in normal operation at least one DM
            should serve that Yeti-Zone within 20 minutes of root zone
            publication. In practice, this was not observed.</t>

          <t>In one case, a Yeti-Root server running Bundy 1.2.0 on
            FreeBSD 10.2-RELEASE was found to lag root zone publication
            by as much as ten hours. Upon investigation, this was found to be
            due to software defects that were subsequently
            corrected.</t>

          <t>More generally, Yeti-Root servers were observed routinely
            to lag root zone publication by more than 20 minutes, and
            relatively often by more than 40 minutes. Whilst in
            some cases this might be assumed to be a result of
            connectivity problems, perhaps suppressing the delivery
            of NOTIFY messages, it was also observed that Yeti-Root
            servers receiving a NOTIFY from one DM would often send
            SOA queries and AXFR requests to a different DM. If
            that DM were not yet serving the new revision of the
            Yeti-Root zone, a delay in updating the Yeti-Root server
            would naturally result.</t>
        </section>

        <section title="Mixed RRSIGs from Different DM ZSKs" anchor="Mixed">

          <t>The second approach for doing the transformation of Root 
            Zone to Yeti-Root zone (<xref target="trans2"/>) 
            introduces a situation where mixed RRSIGs from different 
            DM ZSKs are cached in one resolver.</t>

         <t>It is observed that the Yeti-Root zone served by any 
            particular Yeti-Root server will include signatures 
            generated using the ZSK from the DM that served the Yeti-Root zone to that
            Yeti-Root server. Signatures cached at resolvers might
            be retrieved from any Yeti-Root server, and hence are
            expected to be a mixture of signatures generated by
            different ZSKs. Since all ZSKs can be trusted through
            the signature by the Yeti KSK over the DNSKEY RRset,
            which includes all ZSKs, the mixture of signatures was
            predicted not to be a threat to reliable validation. 
        </t>
        <t> 

            It was first tested in BII's lab environment as a proof 
            of concept. It was observed in the resolver's DNSSEC log that 
            the process of verifying an RDATA set shows "success" with a 
            key (keyid) in the DNSKEY RRset. It was implemented later in 
            three DMs that were carefully coordinated and made public 
            to all Yeti resolver operators and participants in Yeti's 
            mailing list. At least 45 Yeti resolvers (deployed by Yeti 
            operators) were being monitored and had set a reporting trigger 
            if anything was wrong. In addition, the Yeti mailing list is open 
            for error reports from other participants. So far, the Yeti testbed 
            has been operated in this configuration (with multiple ZSKs) 
            for 2 years. 

This configuration has proven workable and reliable, even 
   when rollovers of individual ZSKs are on different schedules. </t>

          <t>Another consequence of this approach is that the apex DNSKEY
            RRset in the Yeti-Root zone is much larger than the
            corresponding DNSKEY RRset in the Root Zone. This requires more 
            space and produces a larger response to the query for the DNSKEY RRset 
            especially during the KSK rollover. </t>

        </section>

      </section>

      <section title="DNSSEC KSK Rollover" anchor="ksk_rollover">
	<t>At the time of writing, the Root Zone KSK is expected
	  to undergo a carefully orchestrated rollover as described
	  in <xref target="ICANN2016"/>. ICANN has commissioned
	  various tests and has published an external test plan
	  <xref target="ICANN2017"/>.</t>

        <t>Three related DNSSEC KSK rollover exercises were carried
	  out on the Yeti DNS testbed, somewhat concurrent with
	  the planning and execution of the rollover in the root zone.
          Brief descriptions of these exercises are included below.</t>

        <section title="Failure-Case KSK Rollover">
	  <t>The first KSK rollover that was executed on the Yeti
	    DNS testbed deliberately ignored the 30-day hold-down
	    timer specified in <xref target="RFC5011"/> before
	    retiring the outgoing KSK.</t>

	  <t>It was confirmed that clients of some (but not all)
	    validating Yeti resolvers experienced resolution failures
	    (received SERVFAIL responses) following this change.
	    Those resolvers required administrator intervention to
	    install a functional trust anchor before resolution was
	    restored.</t>
        </section>

        <section title="KSK Rollover vs. BIND9 Views">
	  <t>The second Yeti KSK rollover was designed with similar
	    phases to the ICANN's KSK rollover, although with
	    modified timings to reduce the time required to complete
	    the process. The "slot" used in this rollover was ten
	    days long, as follows:</t>

          <texttable>
            <ttcol> </ttcol><ttcol>Old Key: 19444</ttcol><ttcol>New Key</ttcol>
            <c>slot 1</c><c>pub+sign</c><c> </c>
            <c>slot 2, 3, 4, 5</c><c>pub+sign</c><c>pub</c>
            <c>slot 6, 7</c><c>pub</c><c>pub+sign</c>
            <c>slot 8</c><c>revoke</c><c>pub+sign</c>
            <c>slot 9</c><c> </c><c>pub+sign</c>
          </texttable>

          <t>During this rollover exercise, a problem was observed on
	    one Yeti resolver that was running BIND 9.10.4-p2 <xref
	    target="KROLL-ISSUE"/>. That resolver was configured
	    with multiple views serving clients in different subnets
	    at the time that the KSK rollover began. DNSSEC validation
	    failures were observed following the completion of the
	    KSK rollover, triggered by the addition of a new view
            that was intended to serve clients from a new subnet.</t>

          <t>BIND 9.10 requires "managed-keys" configuration to be
            specified in every view, a detail that was apparently not
	    obvious to the operator in this case and that was
	    subsequently highlighted by the Internet Systems Consortium (ISC) in their general advice
	    relating to KSK rollover in the root zone to users of
	    BIND 9 <xref target="ISC-BIND"/>.
	    When the "managed-keys" configuration is present in
	    every view that is configured to perform validation,
	    trust anchors for all views are updated during a KSK
	    rollover.</t>
        </section>

        <section title="Large Responses during KSK Rollover">
	  <t>Since a KSK rollover necessarily involves the publication
	    of outgoing and incoming public keys simultaneously,
	    an increase in the size of DNSKEY responses is expected.
	    The third KSK rollover carried out on the Yeti DNS
	    testbed was accompanied by a concerted effort to observe
	    response sizes and their impact on end-users.</t>

	  <t>As described in <xref target="trans2"/>, in the Yeti
	    DNS testbed each DM can maintain control of its own set
	    of ZSKs, which can undergo rollover independently.
	    During a KSK rollover where concurrent ZSK rollovers
	    are executed by each of three DMs, the maximum number
	    of apex DNSKEY RRs present is eight (incoming and
	    outgoing KSK, plus incoming and outgoing of each of
	    three ZSKs).  In practice, however, such concurrency
	    did not occur; only the BII ZSK was rolled during the
	    KSK rollover, and hence only three DNSKEY RRset
	    configurations were observed:

            <list style="symbols">
              <t>3 ZSKs and 2 KSKs, DNSKEY response of 1975 octets;</t>
              <t>3 ZSKs and 1 KSK, DNSKEY response of 1414 octets; and</t>
              <t>2 ZSKs and 1 KSK, DNSKEY response of 1139 octets.</t>
            </list>
          </t>

	  <t>RIPE Atlas probes were used as described in <xref
	    target="v6frag"/> to send DNSKEY queries directly to
	    Yeti-Root servers. The numbers of queries and failures
	    were recorded and categorized according to the response
	    sizes at the time the queries were sent. A summary of
            the results (<xref target="YetiLR"/>) is as follows:</t>

          <texttable>
	    <ttcol>Response Size</ttcol><ttcol>Failures</ttcol><ttcol>Total
	      Queries</ttcol><ttcol>Failure Rate</ttcol>
            <c>1139</c><c>274</c><c>64252</c><c>0.0042</c>
            <c>1414</c><c>3141</c><c>126951</c><c>0.0247</c>
            <c>1975</c><c>2920</c><c>42529</c><c> 0.0687</c>
          </texttable>

	  <t>The general approach illustrated briefly here provides
	    a useful example of how the design of the Yeti DNS
	    testbed, separate from the Root Server system but
	    constructed as a live testbed on the Internet, facilitates
	    the use of general-purpose active measurement facilities
	    (such as RIPE Atlas probes) as well as internal passive
	    measurement (such as packet capture).</t>
	</section>
      </section>

      <section title="Capture of Large DNS Response">
        <t>Packet capture is a common approach in production DNS
	  systems where operators require fine-grained insight into
	  traffic in order to understand production traffic. For
	  authoritative servers, capture of inbound query traffic
	  is often sufficient, since responses can be synthesized
	  with knowledge of the zones being served at the time the
	  query was received. Queries are generally small enough
	  not to be fragmented, and even with TCP transport are
	  generally packed within a single segment.</t>

        <t>The Yeti DNS testbed has different requirements; in
          particular, there is a desire to compare responses obtained
          from the Yeti infrastructure with those received from the
          Root Server system in response to a single query stream
          (e.g., using the "Yeti Many Mirror Verifier" (YmmV) as described in <xref target="Yeti_tools"/>).
          Some Yeti-Root servers were capable of recovering complete DNS
          messages from within nameservers, e.g., using dnstap; however,
          not all servers provided that functionality, and a consistent
          approach was desirable.</t>

	<t>The requirement to perform passive capture of responses from the wire
	  together with experiments that were expected (and in some
	  cases designed) to trigger fragmentation and use of TCP
	  transport led to the development of a new tool, PcapParser,
	  to perform fragment and TCP stream reassembly from raw
	  packet capture data. A brief description of PcapParser
          is included in <xref target="Yeti_tools"/>.</t>
      </section>

      <section title="Automated Maintenance of the Hints File">
        <t>Renumbering events in the Root Server system are relatively
          rare. Although each such event is accompanied by the
          publication of an updated hints file in standard locations,
          the task of updating local copies of that file used by
          DNS resolvers is manual, and the process has an observably long
          tail. For example, in 2015 J-Root was still receiving
          traffic at its old address some thirteen years after
          renumbering <xref target="Wessels2015"/>.</t>

        <t>The observed impact of these old, deployed hints files
          is minimal, likely due to the very low frequency of such
          renumbering events. Even the oldest of hints files would
          still contain some accurate root server addresses from
          which priming responses could be obtained.</t>

        <t>By contrast, due to the experimental nature of the system
          and the fact that it is operated mainly by volunteers,
          Yeti-Root servers are added, removed, and renumbered with
          much greater frequency. A tool to facilitate automatic
          maintenance of hints files was therefore created: <xref
          target="hintUpdate"/>.</t>

        <t>The automated procedure followed by the hintUpdate tool
          is as follows.

          <list style="numbers">
            <t>Use the local resolver to obtain a response to the
              query "./IN/NS".</t>

            <t>Use the local resolver to obtain a set of IPv4 and
              IPv6 addresses for each name server.</t>

            <t>Validate all signatures obtained from the local
              resolvers and confirm that all data is signed.</t>

            <t>Compare the data obtained to that contained within
              the currently active hints file; if there are differences,
              rotate the old one away and replace it with a new
              one.</t>
          </list>
        </t>

        <t>This tool would not function unmodified when used in the
          Root Server system, since the names of individual Root
          Servers (e.g., A.ROOT-SERVERS.NET) are not DNSSEC signed. All
          Yeti-Root server names are DNSSEC signed, however, and hence
          this tool functions as expected in that environment.</t>
      </section>

      <section title="Root Label Compression in Knot DNS Server" anchor="knot_label">
	<t><xref target="RFC1035"/> specifies that domain names can
	  be compressed when encoded in DNS messages, and can be represented
	  as one of

          <list style="numbers">
            <t>a sequence of labels ending in a zero octet;</t>

            <t>a pointer; or</t>

            <t>a sequence of labels ending with a pointer.</t>
          </list>

          The purpose of this flexibility is to reduce the size of
          domain names encoded in DNS messages.</t>

        <t>It was observed that Yeti-Root servers running Knot 2.0
          would compress the zero-length label (the root domain, often
          represented as ".") using a pointer to an earlier example.
          Although legal, this encoding increases the encoded size
          of the root label from one octet to two; it was also found
          to break some client software -- in particular, the Go DNS
          library. Bug reports were filed against both Knot and the
          Go DNS library, and both were resolved in subsequent
          releases.</t>
      </section>


    </section>

    <section title="Conclusions">
      <t>Yeti DNS was designed and implemented as a live DNS root
	system testbed. It serves a root zone ("Yeti-Root" in this
	document) derived from the root zone published
	by the IANA with only those structural modifications necessary
	to ensure its function in the testbed system. The Yeti DNS
	testbed has proven to be a useful platform to address many
	questions that would be challenging to answer using the
	production Root Server system, such as those included in
	<xref target="areas_of_study"/>.</t>

      <t>Indicative findings following from the construction and
        operation of the Yeti DNS testbed include:

        <list style="symbols">
          <t>Operation in a pure IPv6-only environment; confirmation
	    of a significant failure rate in the transmission of
	    large responses (~7%), but no other persistent failures
	    observed. Two cases in which Yeti-Root servers failed
	    to retrieve the Yeti-Root zone due to fragmentation of
	    TCP segments; mitigated by setting a TCP MSS of 1220
	    octets (see <xref target="v6frag"/>).</t>

          <t>Successful operation with three autonomous Yeti-Root zone
	    signers and 25 Yeti-Root servers, and confirmation that
	    IXFR is not an appropriate transfer mechanism of zones
	    that are structurally incongruent across different
	    transfer paths (see <xref target="zone_dist"/>).</t>

	  <t>ZSK size increased to 2048 bits and multiple KSK
	    rollovers executed to exercise support of RFC 5011 in
	    validating resolvers; identification of pitfalls relating
	    to views in BIND9 when configured with "managed-keys"
	    (see <xref target="ksk_rollover"/>).</t>

	  <t>Use of natural (non-normalized) names for Yeti-Root
	    servers exposed some differences between implementations
            in the inclusion of additional-section glue in responses
            to priming queries; however, despite this inefficiency,
            Yeti resolvers were observed to function adequately
            (see <xref target="naming"/>).</t>

          <t>It was observed that Knot 2.0 performed label compression
            on the root (empty) label. This resulted in an increased
            encoding size for references to the root label, since a
	    pointer is encoded as two octets whilst the root label
	    itself only requires one (see <xref target="knot_label"/>).</t>

	  <t>Some tools were developed in response to the operational
	    experience of running and using the Yeti DNS testbed:
            DNS fragment and DNS Additional Truncated Response (ATR)
            for large DNS responses, a
	    BIND9 patch for additional-section glue, YmmV, and
	    IPv6 defrag for capturing and mirroring traffic. In
	    addition, a tool to facilitate automatic maintenance
	    of hints files was created (see <xref
	    target="Yeti_tools"/>).</t>
	</list>
      </t>

      <t> The Yeti DNS testbed was used only by end-users whose local
   infrastructure providers had made the conscious decision to do so, as
   is appropriate for an experimental, non-production system. So far, no 
   serious user complaints have reached Yeti's mailing list during Yeti 
   normal operation. Adding more instances into the Yeti root system 
   may help to enhance the quality of service, but it is generally 
   accepted that Yeti DNS performance is good enough to serve the purpose 
   of DNS Root testbed. </t>

      <t>The experience gained during the operation of the Yeti DNS
        testbed suggested several topics worthy of further study:

        <list style="symbols">
	  <t>Priming truncation and TCP-only Yeti-Root servers:
	    observe and measure the worst-possible case for priming
	    truncation by responding with TC=1 to all priming queries
	    received over UDP transport, forcing clients to retry
	    using TCP. This should also give some insight into the
	    usefulness of TCP-only DNS in general.</t>

	  <t>KSK ECDSA Rollover: one possible way to reduce DNSKEY
	    response sizes is to change to an elliptic curve signing
	    algorithm. While in principle this can be done separately
	    for the KSK and the ZSK, the RIPE NCC has done research
	    recently and discovered that some resolvers require
	    that both KSK and ZSK use the same algorithm. This means
	    that an algorithm roll also involves a KSK roll.
	    Performing an algorithm roll at the root would be an
	    interesting challenge.</t>

	  <t>Sticky Notify for zone transfer: the non-applicability of
            IXFR as a zone transfer mechanism in the Yeti DNS testbed
            could be mitigated by the implementation of a sticky
            preference for master server for each slave. This would
	    be so that
            an initial AXFR response could be followed up with IXFR
	    requests without compromising zone integrity in the
	    case (as with Yeti) that equivalent but incongruent
	    versions of a zone are served by different masters.</t>

	  <t>Key distribution for zone transfer credentials: the use
	    of a shared secret between slave and master requires
            key distribution and management whose scaling properties
            are not ideally suited to systems with large numbers of
            transfer clients. Other approaches for key distribution
            and authentication could be considered.</t>

     <t>
      DNS is a tree-based hierarchical database. Mathematically, it has a 
      root node and dependency between parent and child nodes. So, any 
      failures and instability of parent nodes (Root in Yeti's case) 
      may impact their child nodes if there is a human mistake, a malicious 
      attack, or even an earthquake. It is proposed to define technology 
      and practices to allow any organization, from the smallest company 
      to nations, to be self-sufficient in their DNS.
     </t>
     <t>In Section 3.12 of <xref target="RFC8324"/>, a "Centrally Controlled Root" 
        is viewed as an issue of DNS. In future work, it would be interesting
	to test some technical tools like blockchain <xref target="BC"/> to either remove the technical 
        requirement for a central authority over the root or enhance the 
        security and stability of the existing Root.</t>
        </list>
      </t>

    </section>
    <section title="Security Considerations">

    <t>As introduced in <xref target="synch"/>, service metadata 
      is synchronized among 3 DMs using Git tool. Any security issue around 
      Git may affect Yeti DM operation. For example, a hacker may compromise 
      one DM's Git repository and push unwanted changes to the Yeti DM system;
      this may introduce a bad root server or bad key for a period of time. 
    </t>
    <t>The Yeti resolver needs the bootstrapping files to join the testbed, 
      like the hints file and trust anchor of Yeti. All required information 
      is published on <eref target="yeti-dns.org"/> and <eref target="github.com"/>. If a hacker tampers with those 
      websites by creating a fake page, a new resolver may lose its way and be
      configured 
      with a bad root.</t>

    <t>DNSSEC is an important research goal in the Yeti DNS testbed. To reduce 
      the central function of DNSSEC for Root zone, we sign the Yeti-Root 
      zone using multiple, independently operated DNSSEC signers and multiple 
      corresponding ZSKs (see <xref target="transformation"/>). To verify 
      ICANN's KSK rollover, we rolled the Yeti KSK three times according to 
      RFC 5011, and we do have some observations (see <xref target="ksk_rollover"/>). 

      In addition, larger RSA key sizes were used in the testbed before 
      2048-bit keys were used in the ZSK signing process of the IANA Root zone.</t>
    </section>
    <section title="IANA Considerations">
      <t>This document has no IANA actions.</t>
    </section>

  </middle>

  <back>
    <references title="Normative References">
    &RFC1034; &RFC1035; &RFC1995; &RFC1996; &RFC5011; &RFC5890;

    </references>
    
    <references title="Informative References">
      &RFC2826;
      &RFC2845;
      &RFC6219;
      &RFC6891;
      &RFC7720;
      &RFC7872;
      &RFC8109;
      &RFC8324;
<!-- draft-muks-dns-message-fragments: Expired-->
<reference anchor='FRAGMENTS'>
<front>
<title>DNS message fragments</title>

<author initials='M' surname='Sivaraman' fullname='Mukund Sivaraman'>
    <organization />
</author>

<author initials='S' surname='Kerr' fullname='Shane Kerr'>
    <organization />
</author>

<author initials='D' surname='Song' fullname='Davey Song'>
    <organization />
</author>

<date month='July' day='20' year='2015' />

<abstract><t>This document describes a method to transmit DNS messages over multiple UDP datagrams by fragmenting them at the application layer. The objective is to allow authoriative servers to successfully reply to DNS queries via UDP using multiple smaller datagrams, where larger datagrams may not pass through the network successfully.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-muks-dns-message-fragments-00' />

</reference>

<!-- draft-andrews-tcp-and-ipv6-use-minmtu: Expired -->
<reference anchor='USE_MIN_MTU'>
<front>
<title>TCP Fails To Respect IPV6_USE_MIN_MTU</title>

<author initials='M' surname='Andrews' fullname='Mark Andrews'>
    <organization />
</author>

<date month='October' day='18' year='2015' />

<abstract><t>The IPV6_USE_MIN_MTU socket option directs the IP layer to limit the IPv6 packet size to the minimum required supported MTU from the base IPv6 specification, i.e. 1280 bytes.  Many implementations of TCP running over IPv6 neglect to check the IPV6_USE_MIN_MTU value when performing MSS negotiation and when constructing a TCP segment despite MSS being defined to be the MTU less the IP and TCP header sizes (60 bytes for IPv6).  This leads to oversized IPv6 packets being sent resulting in unintended Path Maximum Transport Unit Discovery (PMTUD) being performed and to fragmented IPv6 packets being sent.</t></abstract>

</front>
<seriesInfo name='Work in Progress,' value='draft-andrews-tcp-and-ipv6-use-minmtu-04' />

</reference>

<!-- draft-taylor-v6ops-fragdrop: Expired -->
<reference anchor='FRAGDROP'>
<front>
<title>Why Operators Filter Fragments and What It Implies</title>

<author initials='J' surname='Jaeggli' fullname='Joel Jaeggli'>
    <organization />
</author>

<author initials='L' surname='Colitti' fullname='Lorenzo Colitti'>
    <organization />
</author>

<author initials='W' surname='Kumari' fullname='Warren Kumari'>
    <organization />
</author>

<author initials='E' surname='Vyncke' fullname='Eric Vyncke'>
    <organization />
</author>

<author initials='M' surname='Kaeo' fullname='Merike Kaeo'>
    <organization />
</author>

<author initials='T' surname='Taylor' fullname='Tom Taylor'>
    <organization />
</author>

<date month='December' day='3' year='2013' />

<abstract><t>This memo was written to make application developers and network operators aware of the significant possibility that IPv6 packets containing fragmentation extension headers may fail to reach their destination.  Some protocol or application assumptions about the ability to use messages larger than a single packet may accordingly not be supportable in all networks or circumstances.  This memo provides observational evidence for the dropping of IPv6 fragments along a significant number of paths, explores the operational impact of fragmentation and the reasons and scenarios where drops occur, and considers the effect of fragment drops on applications where fragmentation is known to occur, particularly including DNS.</t></abstract>

</front>
<seriesInfo name='Work in Progress,' value='draft-taylor-v6ops-fragdrop-02' />

</reference>

<!-- draft-song-atr-large-resp: Active I-D -->
<reference anchor='ATR'>
<front>
<title>ATR: Additional Truncation Response for Large DNS Response</title>

<author initials='L' surname='Song' fullname='Linjian Song'>
    <organization />
</author>

<date month='August' day='2' year='2018' />

<abstract><t>As the increasing use of DNSSEC and IPv6, there are more public evidence and concerns on IPv6 fragmentation issues due to larger DNS payloads over IPv6.  This memo introduces an simple improvement on DNS server by replying an additional truncated response just after the normal fragmented response.  It can be used to relieve users suffering on DNS latency and failures due to large DNS response.  An ATR Experiment was done to show how well it works and some operational issues are discussed in this memo as well.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-song-atr-large-resp-02' />

</reference>

      <reference anchor="ITI2014"
        target="https://www.icann.org/en/system/files/files/iti-report-15may14-en.pdf">
        <front>
          <title>Identifier Technology Innovation Report</title>
          <author><organization>ICANN</organization></author>
          <date day="15" month="May" year="2014"/>
        </front>
      </reference>

      <reference anchor="Wessels2015"
        target="https://indico.dns-oarc.net/event/24/session/10/contribution/10/material/slides/0.pdf">
        <front>
          <title>Thirteen Years of 'Old J-Root'</title>
          <author fullname="Duane Wessels" initials="D." surname="Wessels"/>
          <author initials="J." surname="Castonguay"/>
          <author initials="P." surname="Barber"/>
          <date month="October" year="2015" />
        </front>
<seriesInfo name="DNS-OARC" value="Fall 2015 Workshop"/>
      </reference>


      <reference anchor="ICANN2010"
        target="http://www.root-dnssec.org/wp-content/uploads/2010/05/draft-icann-dnssec-keymgmt-01.txt">
        <front>
          <title>DNSSEC Key Management Implementation for the Root Zone (DRAFT)</title>
          <author initials="J." surname="Schlyter"/>
          <author initials="R." surname="Lamb"/>
          <author initials="R." surname="Balasubramanian"/>
          <date day="11" month="May" year="2010"/>
        </front>
      </reference>

      <reference anchor="ICANN2016"
        target="https://www.iana.org/reports/2016/root-ksk-rollover-design-20160307.pdf">
        <front>
          <title>Root Zone KSK Rollover Plan</title>
          <author><organization>Design Team</organization></author>
          <date month="March" year="2016" />
        </front>
      </reference>

      <reference anchor="ICANN2017"
        target="https://www.icann.org/en/system/files/files/ksk-rollover-external-test-plan-22jul16-en.pdf">
        <front>
          <title>2017 KSK Rollover External Test Plan</title>
          <author><organization>ICANN</organization></author>
          <date day="22" month="July" year="2016"/>
        </front>
      </reference>

     <reference anchor="IPv6-frag-DNS" target="https://blog.apnic.net/2017/08/22/dealing-ipv6-fragmentation-dns">
            <front>
                <title>Dealing with IPv6 fragmentation in the DNS</title>
                <author fullname= "Geoff Huston" initials="G." surname="Huston">
                  <organization>APNIC</organization>
                  <address></address>
                </author>
                <date year="2017" month="August" day="22"/>
            </front>
	    <seriesInfo name="APNIC" value="blog"/>
     </reference>  

     <reference anchor="HOW_ATR_WORKS" target="https://blog.apnic.net/2018/04/16/how-well-does-atr-actually-work/">
            <front>
                <title>How well does ATR actually work?</title>
                <author fullname= "Geoff Huston" initials="G." surname="Huston">
                  <organization>APNIC</organization>
                  <address></address>
                </author>
                <date year="2018" month="April  " day="16"/>
            </front>
	    <seriesInfo name="APNIC" value="blog"/>
     </reference>  

      <reference anchor="hintUpdate"
        target="https://github.com/BII-Lab/Hintfile-Auto-Update">
        <front>
          <title>Hintfile Auto Update</title>
          <author/>
          <date month="October" year="2015"/>
        </front>
	<seriesInfo name="commit" value="de428c0"/>
      </reference>

      <reference anchor="KROLL-ISSUE"
        target="http://yeti-dns.org/yeti/blog/2016/10/26/A-DNSSEC-issue-during-Yeti-KSK-rollover.html">
        <front>
          <title>A DNSSEC issue during Yeti KSK rollover</title>
          <author initials="D." surname="Song"></author>
          <date month="October" year="2016" />
        </front>
	    <seriesInfo name="Yeti DNS" value="blog"/>
      </reference>

<reference anchor="PINZ"
	   target="http://yeti-dns.org/yeti/blog/2018/05/01/Experiment-plan-for-PINZ.html">
        <front>
          <title>Yeti experiment plan for PINZ</title>
          <author initials="D." surname="Song"></author>
          <date month="May" year="2018" />
        </front>
	    <seriesInfo name="Yeti DNS" value="blog"/>
      </reference>

      <reference anchor="TNO2009"
        target="https://www.icann.org/en/system/files/files/root-scaling-model-description-29sep09-en.pdf">
        <front>
          <title>Root Scaling Study: Description of the DNS Root Scaling
            Model</title>
          <author fullname="Bart Gijsen" initials="B." surname="Gijsen"/>
          <author fullname="Almerima Jamakovic" initials="A." surname="Jamakovic"/>
          <author fullname="Frank Roijers" initials="F." surname="Roijers"/>
          <date day="29" month="September" year="2009"/>
        </front>
	    <seriesInfo name="TNO" value="report"/>
      </reference>


      <reference anchor="ISC-BIND"
        target="https://www.isc.org/blogs/2017-root-key-rollover-what-does-it-mean-for-bind-users/">
        <front>
          <title>2017 Root Key Rollover - What Does it Mean for BIND Users?</title>
          <author fullname="Vicky Risk" initials="V." surname="Risk"/>
          <date month="December" year="2016"/>
        </front>
	<seriesInfo name="Internet Systems" value="Consortium"/>
      </reference>


      <reference anchor="ISC-TN-2003-1"
        target="http://ftp.isc.org/isc/pubs/tn/isc-tn-2003-1.txt">
        <front>
          <title>Hierarchical Anycast for Global Service Distribution</title>
          <author fullname="Joe Abley" initials="J." surname="Abley"/>
          <date month="March" year="2003"/>
        </front>
      </reference>

      <reference anchor="RSSAC001"
        target="https://www.icann.org/en/system/files/files/rssac-001-root-service-expectations-04dec15-en.pdf">
        <front>
          <title>Service Expectations of Root Servers</title>
          <author><organization>
	    Root Server System Advisory Committee (RSSAC)
	  </organization></author>
          <date day="4" month="December" year="2015"/>
        </front>
	<seriesInfo name="RSSAC001" value="Version 1"/>
      </reference>

      <reference anchor="RSSAC023"
        target="https://www.icann.org/en/system/files/files/rssac-023-04nov16-en.pdf">
        <front>
          <title>History of the Root Server System</title>
          <author><organization>
	    Root Server System Advisory Committee (RSSAC)
	  </organization></author>
          <date day="4" month="November" year="2016"/>
        </front>
      </reference>

      <reference anchor="RRL"
        target="http://www.redbarn.org/dns/ratelimits">
        <front>
          <title>Response Rate Limiting in the Domain Name System (DNS RRL)</title>
          <author fullname="Paul Vixie" initials="P." surname="Vixie"/>
          <author fullname="Vernon Schryver" initials="V." surname="Schryver"/>
          <date day="10" month="June" year="2012"/>
        </front>
      </reference>

      <reference anchor="YetiLR"
          target="https://yeti-dns.org/yeti/blog/2017/08/02/large-packet-impact-during-yeti-ksk-rollover.html">
        <front>
          <title>Observation on Large response issue during Yeti KSK rollover</title>
          <author fullname="Davey Song"/>
          <date day="2" month="August" year="2017"/>
        </front>
	    <seriesInfo name="Yeti DNS" value="blog"/>
      </reference>

      <reference anchor="BC"
        target="https://en.wikipedia.org/w/index.php?title=Blockchain&amp;oldid=861681529">
        <front>
          <title>Blockchain</title>
          <author><organization>Wikipedia</organization></author>
          <date month="September" year="2018"/>
	</front>
      </reference>

      <reference anchor="SUNSET4"
	    target="https://datatracker.ietf.org/wg/sunset4/about/">
        <front>
          <title>Sunsetting IPv4 (sunset4) Concluded WG</title>
          <author><organization>IETF</organization></author>
          <date/>
	</front>
      </reference>

    </references>

    <section anchor="yeti-hints" title="Yeti-Root Hints File">
      <t>The following hints file (complete and accurate at the
        time of writing) causes a DNS resolver to use the Yeti DNS
        testbed in place of the production Root Server system and
        hence participate in experiments running on the testbed.</t>

      <t>Note that some lines have been wrapped in the text that
        follows in order to fit within the production constraints of
        this document. Wrapped lines are indicated with a blackslash
        character ("\"), following common convention.</t>

      <figure>
        <artwork><![CDATA[
.                     3600000  IN   NS     bii.dns-lab.net
bii.dns-lab.net       3600000  IN   AAAA   240c:f:1:22::6
.                     3600000  IN   NS     yeti-ns.tisf.net
yeti-ns.tisf.net      3600000  IN   AAAA   2001:559:8000::6
.                     3600000  IN   NS     yeti-ns.wide.ad.jp
yeti-ns.wide.ad.jp    3600000  IN   AAAA   2001:200:1d9::35
.                     3600000  IN   NS     yeti-ns.as59715.net
yeti-ns.as59715.net   3600000  IN   AAAA   \
                           2a02:cdc5:9715:0:185:5:203:53
.                     3600000  IN   NS     dahu1.yeti.eu.org
dahu1.yeti.eu.org     3600000  IN   AAAA   \
                           2001:4b98:dc2:45:216:3eff:fe4b:8c5b
.                     3600000  IN   NS     ns-yeti.bondis.org
ns-yeti.bondis.org    3600000  IN   AAAA   2a02:2810:0:405::250
.                     3600000  IN   NS     yeti-ns.ix.ru
yeti-ns.ix.ru         3600000  IN   AAAA   2001:6d0:6d06::53
.                     3600000  IN   NS     yeti.bofh.priv.at
yeti.bofh.priv.at     3600000  IN   AAAA   2a01:4f8:161:6106:1::10
.                     3600000  IN   NS     yeti.ipv6.ernet.in
yeti.ipv6.ernet.in    3600000  IN   AAAA   2001:e30:1c1e:1::333
.                     3600000  IN   NS     yeti-dns01.dnsworkshop.org
yeti-dns01.dnsworkshop.org \
                      3600000  IN   AAAA   2001:1608:10:167:32e::53
.                     3600000  IN   NS     yeti-ns.conit.co
yeti-ns.conit.co      3600000  IN   AAAA   \
                          2604:6600:2000:11::4854:a010
.                     3600000  IN   NS     dahu2.yeti.eu.org
dahu2.yeti.eu.org     3600000  IN   AAAA   2001:67c:217c:6::2
.                     3600000  IN   NS     yeti.aquaray.com
yeti.aquaray.com      3600000  IN   AAAA   2a02:ec0:200::1
.                     3600000  IN   NS     yeti-ns.switch.ch
yeti-ns.switch.ch     3600000  IN   AAAA   2001:620:0:ff::29
.                     3600000  IN   NS     yeti-ns.lab.nic.cl
yeti-ns.lab.nic.cl    3600000  IN   AAAA   2001:1398:1:21::8001
.                     3600000  IN   NS     yeti-ns1.dns-lab.net
yeti-ns1.dns-lab.net  3600000  IN   AAAA   2001:da8:a3:a027::6
.                     3600000  IN   NS     yeti-ns2.dns-lab.net
yeti-ns2.dns-lab.net  3600000  IN   AAAA   2001:da8:268:4200::6
.                     3600000  IN   NS     yeti-ns3.dns-lab.net
yeti-ns3.dns-lab.net  3600000  IN   AAAA   2400:a980:30ff::6
.                     3600000  IN   NS     \
                        ca978112ca1bbdcafac231b39a23dc.yeti-dns.net
ca978112ca1bbdcafac231b39a23dc.yeti-dns.net \
                      3600000  IN   AAAA   2c0f:f530::6
.                     3600000  IN   NS     \
                        3e23e8160039594a33894f6564e1b1.yeti-dns.net
3e23e8160039594a33894f6564e1b1.yeti-dns.net \
                      3600000  IN   AAAA   2803:80:1004:63::1
.                     3600000  IN   NS     \
                        3f79bb7b435b05321651daefd374cd.yeti-dns.net
3f79bb7b435b05321651daefd374cd.yeti-dns.net \
                      3600000  IN   AAAA   2401:c900:1401:3b:c::6
.                     3600000  IN   NS     \
                        xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c
xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c \
                      3600000  IN   AAAA   2001:e30:1c1e:10::333
.                     3600000  IN   NS     yeti1.ipv6.ernet.in
yeti1.ipv6.ernet.in   3600000  IN   AAAA   2001:e30:187d::333
.                     3600000  IN   NS     yeti-dns02.dnsworkshop.org
yeti-dns02.dnsworkshop.org \
                      3600000  IN   AAAA   2001:19f0:0:1133::53
.                     3600000  IN   NS     yeti.mind-dns.nl
yeti.mind-dns.nl      3600000  IN   AAAA   2a02:990:100:b01::53:0
]]></artwork>
      </figure>
    </section>

    <section anchor="yeti-priming" title="Yeti-Root Server Priming Response">
      <t>Here is the reply of a Yeti root name server to a priming
      request. The authoritative server runs NSD.</t>

      <figure>
        <artwork><![CDATA[
...
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 62391
;; flags: qr aa rd; QUERY: 1, ANSWER: 26, AUTHORITY: 0, ADDITIONAL: 7
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags: do; udp: 1460
;; QUESTION SECTION:
;.                      IN NS

;; ANSWER SECTION:
.            86400 IN NS bii.dns-lab.net.
.            86400 IN NS yeti.bofh.priv.at.
.            86400 IN NS yeti.ipv6.ernet.in.
.            86400 IN NS yeti.aquaray.com.
.            86400 IN NS yeti.jhcloos.net.
.            86400 IN NS yeti.mind-dns.nl.
.            86400 IN NS dahu1.yeti.eu.org.
.            86400 IN NS dahu2.yeti.eu.org.
.            86400 IN NS yeti1.ipv6.ernet.in.
.            86400 IN NS ns-yeti.bondis.org.
.            86400 IN NS yeti-ns.ix.ru.
.            86400 IN NS yeti-ns.lab.nic.cl.
.            86400 IN NS yeti-ns.tisf.net.
.            86400 IN NS yeti-ns.wide.ad.jp.
.            86400 IN NS yeti-ns.datev.net.
.            86400 IN NS yeti-ns.switch.ch.
.            86400 IN NS yeti-ns.as59715.net.
.            86400 IN NS yeti-ns1.dns-lab.net.
.            86400 IN NS yeti-ns2.dns-lab.net.
.            86400 IN NS yeti-ns3.dns-lab.net.
.            86400 IN NS xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c.
.            86400 IN NS yeti-dns01.dnsworkshop.org.
.            86400 IN NS yeti-dns02.dnsworkshop.org.
.            86400 IN NS 3f79bb7b435b05321651daefd374cd.yeti-dns.net.
.            86400 IN NS ca978112ca1bbdcafac231b39a23dc.yeti-dns.net.
.            86400 IN RRSIG NS 8 0 86400 (
                         20171121050105 20171114050105 26253 .
                         FUvezvZgKtlLzQx2WKyg+D6dw/pITcbuZhzStZfg+LNa
                         DjLJ9oGIBTU1BuqTujKHdxQn0DcdFh9QE68EPs+93bZr
                         VlplkmObj8f0B7zTQgGWBkI/K4Tn6bZ1I7QJ0Zwnk1mS
                         BmEPkWmvo0kkaTQbcID+tMTodL6wPAgW1AdwQUInfy21
                         p+31GGm3+SU6SJsgeHOzPUQW+dUVWmdj6uvWCnUkzW9p
                         +5en4+85jBfEOf+qiyvaQwUUe98xZ1TOiSwYvk5s/qiv
                         AMjG6nY+xndwJUwhcJAXBVmGgrtbiR8GiGZfGqt748VX
                         4esLNtD8vdypucffem6n0T0eV1c+7j/eIA== )

;; ADDITIONAL SECTION:
bii.dns-lab.net.        86400 IN AAAA 240c:f:1:22::6
yeti.bofh.priv.at.      86400 IN AAAA 2a01:4f8:161:6106:1::10
yeti.ipv6.ernet.in.     86400 IN AAAA 2001:e30:1c1e:1::333
yeti.aquaray.com.       86400 IN AAAA 2a02:ec0:200::1
yeti.jhcloos.net.       86400 IN AAAA 2001:19f0:5401:1c3::53
yeti.mind-dns.nl.       86400 IN AAAA 2a02:990:100:b01::53:0

;; Query time: 163 msec
;; SERVER: 2001:4b98:dc2:45:216:3eff:fe4b:8c5b#53
;; WHEN: Tue Nov 14 16:45:37 +08 2017
;; MSG SIZE  rcvd: 1222
]]></artwork>
      </figure>
    </section>
    
    <section title="Active IPv6 Prefixes in Yeti DNS Testbed"
      anchor="activev6">

  <t>The following table shows the prefixes that were active 
   during 2017.</t>

      <texttable>
        <ttcol>Prefix</ttcol><ttcol>Originator</ttcol><ttcol>Location</ttcol>

        <c>240c::/28</c><c>BII</c><c>CN</c>
        <c>2001:6d0:6d06::/48</c><c>MSK-IX</c><c>RU</c>
        <c>2001:1488::/32</c><c>CZ.NIC</c><c>CZ</c>
        <c>2001:620::/32</c><c>SWITCH</c><c>CH</c>
        <c>2001:470::/32</c><c>Hurricane Electric, Inc.</c><c>US</c>
        <c>2001:0DA8:0202::/48</c><c>BUPT6-CERNET2</c><c>CN</c>
        <c>2001:19f0:6c00::/38</c><c>Choopa, LLC</c><c>US</c>
        <c>2001:da8:205::/48</c><c>BJTU6-CERNET2</c><c>CN</c>
        <c>2001:62a::/31</c><c>Vienna University Computer Center</c><c>AT</c>
        <c>2001:67c:217c::/48</c><c>AFNIC</c><c>FR</c>
        <c>2a02:2478::/32</c><c>Profitbricks GmbH</c><c>DE</c>
        <c>2001:1398:1::/48</c><c>NIC Chile</c><c>CL</c>
        <c>2001:4490:dc4c::/46</c><c>NIB (National Internet Backbone)</c>
          <c>IN</c>
        <c>2001:4b98::/32</c><c>Gandi</c><c>FR</c> 
        <c>2a02:aa8:0:2000::/52</c><c>T-Systems-Eltec</c><c>ES</c>
        <c>2a03:b240::/32</c><c>Netskin GmbH</c><c>CH</c>
        <c>2801:1a0::/42</c><c>Universidad de Ibague</c><c>CO</c>
        <c>2a00:1cc8::/40</c><c>ICT Valle Umbra s.r.l.</c><c>IT</c>
        <c>2a02:cdc0::/29</c><c>ORG-CdSB1-RIPE</c><c>IT</c>
      </texttable>
    </section>

    <section title="Tools Developed for Yeti DNS Testbed" anchor="Yeti_tools">
      <t>Various tools were developed to support the Yeti DNS
        testbed, a selection of which are described briefly below.</t>

      <t>YmmV ("Yeti Many Mirror Verifier") is designed to make it
        easy and safe for a DNS administrator to capture traffic
        sent from a resolver to the Root Server system and to replay
        it towards Yeti-Root servers. Responses from both systems
        are recorded and compared, and differences are logged. See
        <eref target="https://github.com/BII-Lab/ymmv"/>.</t>

      <t>PcapParser is a module used by YmmV which reassembles
        fragmented IPv6 datagrams and TCP segments from a PCAP
        archive and extracts DNS messages contained within them.
        See <eref target="https://github.com/RunxiaWan/PcapParser"/>.</t>

      <t>DNS-layer-fragmentation implements DNS proxies that perform
        application-level fragmentation of DNS messages, based on
        <xref target="FRAGMENTS"/>. The idea
        with these proxies is to explore splitting DNS messages in
        the protocol itself, so they will not by fragmented by the
        IP layer. See <eref
        target="https://github.com/BII-Lab/DNS-layer-Fragmentation"/>.</t>

      <t>DNS_ATR is an implementation of DNS Additional Truncated
         Response (ATR), as described in
         <xref target="ATR"/> and <xref target="HOW_ATR_WORKS"/>.  DNS_ATR acts
         as a proxy between resolver and authoritative servers,
         forwarding queries and responses as a silent and transparent
         listener. Responses that are larger than a nominated
         threshold (1280 octets by default) trigger additional
         truncated responses to be sent immediately following the
         large response. See <eref
         target="https://github.com/songlinjian/DNS_ATR"/>.</t>
    </section>

    <section title="Controversy" anchor="controversy">
      <t>The Yeti DNS Project, its infrastructure and the various
        experiments that have been carried out using that infrastructure,
        have been described by people involved in the project in
        many public meetings at technical venues since its inception.
        The mailing lists using which the operation of the
        infrastructure has been coordinated are open to join, and
        their archives are public. The project as a whole has been
        the subject of robust public discussion.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
        Project is, in effect, operating an alternate root,
        challenging the IAB's comments published in <xref
        target="RFC2826"/>. Other such alternate roots are considered
        to have caused end-user confusion and instability in the
        namespace of the DNS by the introduction of new top-level
        labels or the different use of top-level labels present in
        the Root Server system. The coordinators of the Yeti DNS
        Project do not consider the Yeti DNS Project to be an
        alternate root in this sense, since by design the namespace
        enabled by the Yeti-Root zone is identical to that of the
        Root Zone.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
        Project seeks to influence or subvert administrative policy
        relating to the Root Server system, in particular in the
        use of DNSSEC trust anchors not published by the IANA and
        the use of Yeti-Root servers in regions where governments
        or other organizations have expressed interest in operating
        a Root Server. The coordinators of the Yeti-Root project
        observe that their mandate is entirely technical and has
        no ambition to influence policy directly; they do hope,
        however, that technical findings from the Yeti DNS Project
        might act as a useful resource for the wider technical
        community.</t>
    </section>

    <section title="Acknowledgments" numbered="no">
      <t>Firstly, the authors would like to acknowledge the 
        contributions from the people who were involved in 
        the implementation and operation of the Yeti
        DNS by donating their time and resources. They are: </t>
        <t>
        <list style="empty">
        <t>Tomohiro Ishihara, Antonio Prado, 
        Stephane Bortzmeyer, Mickael Jouanne, Pierre Beyssac, 
        Joao Damas, Pavel Khramtsov, Dmitry Burkov, Dima Burkov, 
        Kovalenko Dmitry, Otmar Lendl, Praveen Misra, Carsten Strotmann, 
        Edwin Gomez, Daniel Stirnimann, Andreas Schulze, Remi Gacogne, 
        Guillaume de Lafond, Yves Bovard, Hugo Salgado, Kees Monshouwer, 
        Li Zhen, Daobiao Gong, Andreas Schulze, James Cloos, and Runxia Wan.
      </t>
        </list>
        </t>

      <t>Thanks to all people who gave important advice and 
        comments to Yeti, either in face-to-face meetings or virtually
        via phone or mailing list. Some of the individuals are as follows: </t>

      <t>
        <list style="empty">
        <t>
        Wu Hequan, Zhou Hongren, Cheng Yunqing, Xia Chongfeng, Tang Xiongyan,
        Li Yuxiao, Feng Ming, Zhang Tongxu, Duan Xiaodong, Wang Yang, Wang
        JiYe, Wang Lei, Zhao Zhifeng, Chen Wei, Wang Wei, Wang Jilong, Du
        Yuejing, Tan XiaoSheng, Chen Shangyi, Huang Chenqing, Ma Yan, Li Xing,
        Cui Yong, Bi Jun, Duan Haixing, Marc Blanchet, Andrew Sullivan,
        Suzanne Wolf, Terry Manderson, Geoff Huston, Jaap Akkerhuis, Kaveh
        Ranjbar, Jun Murai, Paul Wilson, and Kilnam Chonm.
      </t>
        </list>
      </t>

      <t>The authors also acknowledge the assistance of the Independent
        Submissions Editorial Board, and of the following reviewers
        whose opinions helped improve the clarity of this document:</t>
      <t>
        <list style="empty">
          <t>Joe Abley, Paul Mockapetris, and Subramanian Moonesamy.</t>
        </list>
      </t>
    </section>

  </back>
</rfc> 
