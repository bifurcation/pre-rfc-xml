<?xml version='1.0' encoding='us-ascii'?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [

<!ENTITY RFC2131 SYSTEM "reference.RFC.2131.xml">
<!ENTITY RFC3376 SYSTEM "reference.RFC.3376.xml">
<!ENTITY RFC6513 SYSTEM "reference.RFC.6513.xml">
<!ENTITY RFC7364 SYSTEM "reference.RFC.7364.xml">
<!ENTITY RFC7365 SYSTEM "reference.RFC.7365.xml">
<!ENTITY RFC8014 SYSTEM "reference.RFC.8014.xml">
<!ENTITY RFC2710 SYSTEM "reference.RFC.2710.xml">
<!ENTITY RFC3569 SYSTEM "reference.RFC.3569.xml">
<!ENTITY RFC3819 SYSTEM "reference.RFC.3819.xml">
<!ENTITY RFC4762 SYSTEM "reference.RFC.4762.xml">
<!ENTITY RFC6040 SYSTEM "reference.RFC.6040.xml">
<!ENTITY RFC6831 SYSTEM "reference.RFC.6831.xml">
<!ENTITY RFC7117 SYSTEM "reference.RFC.7117.xml">
<!ENTITY RFC7348 SYSTEM "reference.RFC.7348.xml">
<!ENTITY RFC7637 SYSTEM "reference.RFC.7637.xml">
<!ENTITY RFC8279 SYSTEM "reference.RFC.8279.xml">
]>

<rfc number="8293"
     submissionType="IETF"
     consensus="yes"
     category="info">

  <?rfc compact="yes"?>
  <?rfc subcompact="no"?>
  <?rfc sortrefs="no"?>
  <?rfc symrefs="yes"?>
  <?rfc toc="yes"?>
	<front>
	<title abbrev="A Framework for Multicast in NVO3">A Framework for
	Multicast in Network Virtualization over Layer 3</title>
	<author fullname="Anoop Ghanwani" initials="A." surname="Ghanwani">
	<organization>Dell</organization>
	<address><email>anoop@alumni.duke.edu</email>
	</address>
	</author>

	<author fullname="Linda Dunbar" initials="L." surname="Dunbar">
	<organization abbrev="Huawei">Huawei Technologies</organization>
	<address>
	  <postal>
	    <street>5340 Legacy Drive, Suite 1750</street>
	<city>Plano, TX</city>
	<region></region>
	<code>75024</code>
	<country>United States of America</country></postal>
	<phone>(469) 277 5840</phone>
	<email>ldunbar@huawei.com</email>
	</address>
	</author>

	<author fullname="Mike McBride" initials="M." surname="McBride">
	<organization abbrev="Huawei">Huawei Technologies</organization>
	<address><email>mmcbride7@gmail.com</email>
	</address>
	</author>

	<author fullname="Vinay Bannai" initials="V." surname="Bannai">
	<organization>Google</organization>
	<address><email>vbannai@gmail.com</email>
	</address>
	</author>

	<author fullname="Ram Krishnan" initials="R." surname="Krishnan">
	<organization>Dell</organization>
	<address><email>ramkri123@gmail.com</email>
	</address>
	</author>

	<date month="December" year="2017"/>
	<workgroup>NVO3 working group</workgroup>

	<abstract><t>
   This document provides a framework for supporting multicast traffic in
   a network that uses Network Virtualization over Layer 3 (NVO3). Both
   infrastructure multicast and application-specific multicast are
   discussed. It describes the various mechanisms that can be used for
   delivering such traffic as well as the data plane and control plane
   considerations for each of the mechanisms.</t>

	</abstract>
	</front>

	<middle>
	<section title="Introduction" anchor="section-1"><t>



Network Virtualization over Layer 3 (NVO3) <xref target="RFC7365"/>
is a technology that is used to address issues that arise in building large,
multi-tenant data centers (DCs) that make extensive use of server
virtualization <xref target="RFC7364"/>.</t>

	<t>
   This document provides a framework for supporting multicast traffic
   in a network that uses NVO3.
   Both infrastructure multicast and application-specific multicast are
   considered. It describes various mechanisms, and the considerations of each
   of them, that can be used for delivering such traffic in networks that use
   NVO3. </t>

	<t>
The reader is assumed to be familiar with the terminology and concepts as
defined in the NVO3 Framework <xref target="RFC7365"/> and NVO3 Architecture
<xref target="RFC8014"/> documents.</t>

	<section title="Infrastructure Multicast" anchor="section-1.1"><t>
 Infrastructure multicast refers to networking services that require multicast
 or broadcast delivery, such as Address Resolution Protocol (ARP), Neighbor
 Discovery (ND), Dynamic Host Configuration Protocol (DHCP), multicast Domain
 Name Server (mDNS), etc., some of which are described in Sections <xref target="section-5"
   format="counter"/> and <xref target="section-6" format="counter"/> of RFC
 3819 <xref target="RFC3819"/>. It is possible to provide solutions for these
 services that do not involve multicast in the underlay network.  For example,
 in the case of ARP/ND, a Network Virtualization Authority (NVA) can be used
 for distributing the IP address to Media Access Control (MAC) address
 mappings to all of the Network Virtualization Edges (NVEs).  An NVE can then
 trap ARP Request and/or ND Neighbor Solicitation messages from the Tenant
 Systems (TSs) that are attached to it and respond to them, thereby
eliminating the need for the broadcast/multicast of such messages.  In the
case of DHCP, the NVE can be configured to forward these messages using the
DHCP relay function <xref target="RFC2131"/>.</t>
	<t>
   Of course, it is possible to support all of these infrastructure
   multicast protocols natively if the underlay provides multicast
   transport.  However, even in the presence of multicast transport, it
   may be beneficial to use the optimizations mentioned above to reduce
   the amount of such traffic in the network.</t>

	</section>

	<section title="Application-Specific Multicast" anchor="section-1.2"><t>
Application-specific multicast traffic refers to multicast traffic that
originates and is consumed by user applications.  Several such applications
are described elsewhere <xref target="DC-MC"/>.  Application-specific multicast may be either 
Source-Specific Multicast (SSM) or Any-Source Multicast (ASM) <xref target="RFC3569"/> and has the following characteristics:</t>

	<t><list style="numbers">
	<t>
Receiver hosts are expected to subscribe to multicast content using protocols
such as IGMP <xref target="RFC3376"/> (IPv4) or Multicast Listener Discovery (MLD) 
<xref target="RFC2710"/> (IPv6). Multicast sources and
       listeners participate in these protocols using addresses that are in
       the TS address domain.</t>

	<t>
The set of multicast listeners for each multicast group may not be known in
advance.  Therefore, it may not be possible or practical for an NVA to get the
list of participants for each multicast group ahead of time.</t>

	</list>
	</t>

		</section>

	      </section>
	<section title="Terminology and Abbreviations" anchor="section-2"><t>
   In this document, the terms host, Tenant System (TS), and Virtual
   Machine (VM) are used interchangeably to represent an end station
   that originates or consumes data packets.</t>

	<t><list style="hanging" hangIndent="6">
	  <t hangText="ASM:">Any-Source Multicast</t>

	<t hangText="IGMP:">Internet Group Management Protocol</t>

	<t hangText="LISP:">Locator/ID Separation Protocol</t>

	<t hangText="MSN:">Multicast Service Node</t>

	<t hangText="RLOC:">Routing Locator</t>

	<t hangText="NVA:">Network Virtualization Authority</t>

	<t hangText="NVE:">Network Virtualization Edge</t>

	<t hangText="NVGRE:">Network Virtualization using GRE</t>

	<t hangText="PIM:">Protocol-Independent Multicast
	</t>

	<t hangText="SSM:">Source-Specific Multicast</t>

	<t hangText="TS:">Tenant System</t>

	<t hangText="VM:">Virtual Machine</t>

	<t hangText="VN:">Virtual Network</t>

	<t hangText="VTEP:">VXLAN Tunnel End Point</t>

	<t hangText="VXLAN:">Virtual eXtensible LAN</t>
	</list></t>

	</section>

	<section title="Multicast Mechanisms in Networks That Use NVO3" anchor="section-3"><t>
   In NVO3 environments, traffic between NVEs is transported using an
   encapsulation such as VXLAN
   <xref target="RFC7348"/> <xref target="VXLAN-GPE"/>, Network Virtualization using Generic Routing
   Encapsulation (NVGRE) <xref target="RFC7637"/>, Geneve <xref target="Geneve"/>, Generic UDP
   Encapsulation <xref target="GUE"/>, etc.</t>

	<t>
   What makes networks using NVO3 different from other networks is that some
   NVEs, especially NVEs implemented in servers, might not support regular
   multicast protocols such as PIM.  Instead, the only capability they may
   support would be that of encapsulating data packets from VMs with an outer
   unicast header.  Therefore, it is important for networks using NVO3 to have
   mechanisms to support multicast as a network capability for NVEs, to map
   multicast traffic from VMs (users/applications) to an equivalent multicast
   capability inside the NVE, or to figure out the outer destination address
   if NVE does not support native multicast (e.g., PIM) or IGMP.</t>

	<t>
   With NVO3, there are many possible ways
   that multicast may be handled in such networks.  We discuss some of
   the attributes of the following four methods:</t>

	<t><list style="numbers">
	<t>No multicast support</t>

	<t>Replication at the source NVE</t>

	<t>Replication at a multicast service node</t>

	<t>IP multicast in the underlay</t>

	</list>
	</t>

	<t>
   These methods are briefly mentioned in the NVO3 Framework <xref target="RFC7365"/>
   and NVO3 Architecture <xref target="RFC8014"/> documents. This document provides
   more details about the basic mechanisms underlying each of these
   methods and discusses the issues and trade-offs of each.</t>

	<t>
   We note that other methods are also possible, such as <xref target="EDGE-REP"/>,
   but we focus on the above four because they are the most common.</t>

	<t>
It is worth noting that when selecting a multicast mechanism, it is useful to
consider the impact of these on any multicast congestion control mechanisms
that applications may be using to obtain the desired system dynamics.  In
addition, the same rules for Explicit Congestion Notification (ECN) would
apply to multicast traffic being encapsulated, as for unicast traffic <xref
target="RFC6040"/>.</t>

	<section title="No Multicast Support" anchor="section-3.1"><t>
   In this scenario, there is no support whatsoever for multicast
   traffic when using the overlay. This method can only work if the
   following conditions are met:</t>

	<t><list style="numbers">
	<t>All of the application traffic in the network is unicast
        traffic, and the only multicast/broadcast traffic is from ARP/ND
        protocols.</t>

	<t>An NVA is used by all of the NVEs to determine the mapping of a given TS's MAC
and IP address to the NVE that it is attached to. In other words, there is no
data-plane learning. Address resolution requests via ARP/ND that are issued by
the TSs must be resolved by the NVE that they are attached to.
</t>

	</list>
	</t>

	<t>
   With this approach, it is not possible to support application-
   specific multicast.  

However, certain multicast/broadcast applications can be supported without
multicast; for example, DHCP, which can be supported by use of DHCP
relay function <xref target="RFC2131"/>.
</t>

	<t>
The main drawback of this approach, even for unicast traffic, is that
   it is not possible to initiate communication with a TS for which a
   mapping to an NVE does not already exist at the NVA. This
   is a problem in the case where the NVE is implemented in a physical
   switch and the TS is a physical end station that has not registered
   with the NVA.</t>

	</section>

	<section title="Replication at the Source NVE" anchor="section-3.2"><t>
   With this method, the overlay attempts to provide a multicast
   service without requiring any specific support from the underlay,
   other than that of a unicast service.  A multicast or broadcast
   transmission is achieved by replicating the packet at the source
   NVE and making copies, one for each destination NVE that the
   multicast packet must be sent to.</t>

	<t>
 For this mechanism to work, the source NVE must know, a priori, the IP
 addresses of all destination NVEs that need to receive the packet.  For the
 purpose of ARP/ND, this would involve knowing the IP addresses of all the
 NVEs that have TSs in the VN of the TS that generated the request. </t>

<t>For the
 support of application-specific multicast traffic, a method similar to that
 of receiver-sites registration for a particular multicast group, described in
 <xref target="LISP-Signal-Free"/>, can be used. The registrations from
 different receiver sites can be merged at the NVA, which can construct a
 multicast replication list inclusive of all NVEs to which receivers for a
 particular multicast group are attached.  The replication list for each
 specific multicast group is maintained by the NVA.  Note that using
 receiver-sites registration does not necessarily mean the source NVE must do
 replication.  If the NVA indicates that multicast packets are encapsulated to
 multicast service nodes, then there would be no replication at the NVE.</t>

	<t>
The receiver-sites registration is achieved by egress NVEs performing
   IGMP/MLD snooping to maintain the state for which attached TSs
   have subscribed to a given IP multicast group.  When the members of a
   multicast group are outside the NVO3 domain, it is necessary for NVO3
   gateways to keep track of the remote members of each multicast group.
   The NVEs and NVO3 gateways then communicate with the multicast groups
   that are of interest to the NVA.  If the membership is not
   communicated to the NVA, and if it is necessary to prevent TSs
   attached to an NVE that have not subscribed to a multicast group from
   receiving the multicast traffic, the NVE would need to maintain
   multicast group membership information.</t>

	<t>
   In the absence of IGMP/MLD snooping, the traffic would be delivered
   to all TSs that are part of the VN.</t>

	<t>
  In multihoming environments, i.e., in those where a TS is attached to
   more than one NVE, the NVA would be expected to provide information
   to all of the NVEs under its control about all of the NVEs to which
   such a TS is attached.  The ingress NVE can then choose any one of those
    NVEs as the egress NVE for the data frames destined towards the
    multi-homed TS.
</t>

	<t>
   This method requires multiple copies of the same packet to all NVEs
   that participate in the VN.  If, for example, a tenant subnet is
   spread across 50 NVEs, the packet would have to be replicated 50
   times at the source NVE.  Obviously, this approach creates more
   traffic to the network that can cause congestion when the network
   load is high. This also creates an issue with the forwarding
   performance of the NVE.</t>

	<t>
   Note that this method is similar to what was used in Virtual Private
   LAN Service (VPLS) <xref target="RFC4762"/> prior to support of Multiprotocol
   Label Switching (MPLS) multicast <xref target="RFC7117"/>. While there are some
   similarities between MPLS Virtual Private Network (VPN) and NVO3,
   there are some key differences:</t>

	<t><list style="symbols">
     <t>The attachment from Customer Edge (CE) to Provider Edge (PE) in VPNs is
     somewhat static, whereas in a DC that allows VMs to migrate
     anywhere, the TS attachment to NVE is much more dynamic.</t>

	<t>The number of PEs to which a single
	VPN customer is attached in an MPLS VPN environment is normally far
	less than the number of NVEs to which a VN's VMs are attached in a DC.
	</t>

	</list>
	</t>
	<t>
   When a VPN customer has multiple multicast groups, "Multicast VPN"
   <xref target="RFC6513"/> combines all those multicast groups within each VPN
   client to one single multicast group in the MPLS (or VPN) core.
   The result is that messages from any of the multicast groups
   belonging to one VPN customer will reach all the PE nodes of the
   client. In other words, any messages belonging to any multicast
   groups under customer X will reach all PEs of the customer X. When
   the customer X is attached to only a handful of PEs, the use of
   this approach does not result in an excessive waste of bandwidth in
   the provider's network.</t>

	<t>

In a DC environment, a typical hypervisor-based virtual switch may only
support on the order of 10's of VMs (as of this writing). A subnet
   with N VMs may be, in the worst case, spread across N virtual switches (vSwitches).
   Using an "MPLS VPN multicast" approach in such a scenario would
   require the creation of a multicast group in the core in order for the VN
   to reach all N NVEs. If only a small percentage of this client's VMs
   participate in application-specific multicast, a great number of
   NVEs will receive multicast traffic that is not forwarded to any
   of their attached VMs, resulting in a considerable waste of
   bandwidth.</t>

	<t>
   Therefore, the multicast VPN solution may not scale in a DC
   environment with the dynamic attachment of VNs to NVEs and a
   greater number of NVEs for each VN.</t>

	</section>

	<section title="Replication at a Multicast Service Node" anchor="section-3.3"><t>
   With this method, all multicast packets would be sent using a
   unicast tunnel encapsulation from the ingress NVE to a Multicast
   Service Node (MSN).  The MSN, in turn, would create multiple copies
   of the packet and would deliver a copy, using a unicast tunnel
   encapsulation, to each of the NVEs that are part of the multicast
   group for which the packet is intended.</t>

	<t>
   This mechanism is similar to that used by the Asynchronous Transfer
   Mode (ATM) Forum's LAN Emulation (LANE) specification <xref target="LANE"/>. The
   MSN is similar to the Rendezvous Point (RP) in Protocol Independent Multicast - Sparse Mode (PIM-SM), but different
   in that the user data traffic is carried by the NVO3 tunnels.</t>

	<t>
The following are possible ways for the MSN to get the membership
information for each multicast group:
</t>

	<t><list style="symbols"><t> 
The MSN can obtain this membership information from the IGMP/MLD report
messages sent by TSs in response to IGMP/MLD query messages from the MSN. The
IGMP/MLD query messages are sent from the MSN to the NVEs, which then forward
the query messages to TSs attached to them. An IGMP/MLD query message sent out
by the MSN to an NVE is encapsulated with the MSN address in the outer IP
source address field and the address of the NVE in the outer IP destination
address field. An encapsulated IGMP/MLD query message also has a virtual
network (VN) identifier (corresponding to the VN that the TSs belong to) in
the outer header and a multicast address in the inner IP destination address
field. Upon receiving the encapsulated IGMP/MLD query message, the NVE
establishes a mapping for "MSN address" to "multicast address", decapsulates
the received encapsulated IGMP/MLD message, and multicasts the decapsulated
query message to the TSs that belong to the VN attached to that NVE. An
IGMP/MLD report message sent by a TS includes the multicast address and the
address of the TS. With the proper "MSN address" to "multicast address"
mapping, the NVEs can encapsulate all multicast data frames containing the
"multicast address" with the address of the MSN in the outer IP destination
address field.</t>

     <t>
     The MSN can obtain the membership information from the NVEs that
     have the capability to establish multicast groups by snooping
     native IGMP/MLD messages (note that the communication must be specific
     to the multicast addresses) or by having the NVA obtain the
     information from the NVEs and in turn have MSN communicate with
     the NVA. This approach requires additional protocol between MSN
     and NVEs.</t>

	</list>
	</t>

	<t>
   Unlike the method described in <xref target="section-3.2"/>, there is no performance
   impact at the ingress NVE, nor are there any issues with multiple
   copies of the same packet from the source NVE to the MSN.  However, there remain issues with multiple copies of
   the same packet on links that are common to the paths from the MSN
   to each of the egress NVEs.  Additional issues that are introduced
   with this method include the availability of the MSN, methods to
   scale the services offered by the MSN, and the suboptimality of the
   delivery paths.</t>

	<t>
   Finally, the IP address of the source NVE must be preserved in
   packet copies created at the multicast service node if data-plane
   learning is in use.  This could create problems if IP source address
   Reverse Path Forwarding (RPF) checks are in use.</t>

	</section>

	<section title="IP Multicast in the Underlay" anchor="section-3.4"><t>
	In this method, the underlay supports IP multicast and the ingress NVE
	encapsulates the packet with the appropriate IP multicast address in
	the tunnel encapsulation header for delivery to the desired set of
	NVEs.  The protocol in the underlay could be any variant of PIM, or a
	protocol-dependent multicast, such as <xref
	target="ISIS-Multicast"/>.</t>

	<t>
If an NVE connects to its attached TSs via a Layer 2 network, there are
multiple ways for NVEs to support the application-specific multicast:</t>

	<t><list style="symbols"><t> The NVE only supports the basic IGMP/MLD
	snooping function, while the "TS routers" handle the
	application-specific multicast.  This scheme doesn't utilize the
	underlay IP multicast protocols.  Instead routers, which are
	themselves TSs attached to the NVE, would handle multicast protocols
	for the application-specific multicast.  We refer to such routers as
	TS routers.</t>

	<t>
The NVE can act as a pseudo multicast router for the directly attached TSs and
support the mapping of IGMP/MLD messages to the messages needed by the
underlay IP multicast protocols.</t>

	</list>
	</t>

	<t>
With this method, there are none of the issues with the methods described in
Sections <xref target="section-3.2" format="counter"/> and <xref
target="section-3.3" format="counter"/> with respect to scaling and
congestion.  Instead, there are other issues described below.</t>

	<t>
With PIM-SM, the number of flows required would be (n*g), where n is the
number of source NVEs that source packets for the group, and g is the number
of groups.  Bidirectional PIM (BIDIR-PIM) would offer better scalability with
the number of flows required being g.  Unfortunately, many vendors still do
not fully support BIDIR or have limitations on its implementation.  <xref
target="RFC6831"/> describes the use of SSM as an alternative to BIDIR,
provided that the NVEs have a way to learn of each other's IP addresses so
that they can join all of the SSM Shortest Path Trees (SPTs) to
create/maintain an underlay SSM IP multicast tunnel solution.</t>

	<t>
  In the absence of any additional mechanism (e.g., using an NVA for address
  resolution), for optimal delivery, there would have to be a separate group
  for each VN for infrastructure multicast plus a separate group for each
  application-specific multicast address within a tenant.</t>

	<t>An additional consideration is that only the lower 23 bits of the
	IP address (regardless of whether IPv4 or IPv6 is in use) are mapped
	to the outer MAC address, and if there is equipment that prunes
	multicasts at Layer 2, there will be some aliasing.</t>

<t>Finally, a mechanism to efficiently provision such addresses for each group
would be required.</t>

	<t>
   There are additional optimizations that are possible, but they come
   with their own restrictions.  For example, a set of tenants may be
   restricted to some subset of NVEs, and they could all share the same
   outer IP multicast group address.  This, however, introduces a problem
   of suboptimal delivery (even if a particular tenant within the
   group of tenants doesn't have a presence on one of the NVEs that
   another one does, the multicast packets would still be delivered to
   that NVE).  It also introduces an additional network management
   burden to optimize which tenants should be part of the same tenant
   group (based on the NVEs they share), which somewhat dilutes the
   value proposition of NVO3 (to completely decouple the
   overlay and physical network design allowing complete freedom of
   placement of VMs anywhere within the DC).</t>

	<t>
   Multicast schemes such as Bit Indexed Explicit Replication (BIER)
   <xref target="RFC8279"/> may be able to provide optimizations by allowing the
   underlay network to provide optimum multicast delivery without
   requiring routers in the core of the network to maintain per-
   multicast group state.</t>

	</section>

	<section title="Other Schemes" anchor="section-3.5"><t>
   There are still other mechanisms that may be used that attempt to
   combine some of the advantages of the above methods by offering
   multiple replication points, each with a limited degree of
   replication <xref target="EDGE-REP"/>.  Such schemes offer a trade-off between the
   amount of replication at an intermediate node (e.g., router) versus
   performing all of the replication at the source NVE or all of the
   replication at a multicast service node.</t>

	</section>

	</section>

	<section title="Simultaneous Use of More Than One Mechanism" anchor="section-4"><t>
   While the mechanisms discussed in the previous section have been
   discussed individually, it is possible for implementations to rely
   on more than one of these.  For example, the method of <xref target="section-3.1"/>
   could be used for minimizing ARP/ND, while at the same time,
   multicast applications may be supported by one, or a combination, of
   the other methods.  For small multicast groups, the methods of
   source NVE replication or the use of a multicast service node may be
   attractive, while for larger multicast groups, the use of multicast
   in the underlay may be preferable.</t>

	</section>

	<section title="Other Issues" anchor="section-5"><section title="Multicast-Agnostic NVEs" anchor="section-5.1"><t>
   Some hypervisor-based NVEs do not process or recognize IGMP/MLD
   frames, i.e., those NVEs simply encapsulate the IGMP/MLD messages in
   the same way as they do for regular data frames.</t>

	<t>
 By default, a TS router periodically sends IGMP/MLD query messages to all the
 hosts in the subnet to trigger the hosts that are interested in the multicast
 stream to send back IGMP/MLD reports.  In order for the MSN to get the
 updated multicast group information, the MSN can also send the IGMP/MLD query
 message comprising a client-specific multicast address encapsulated in an
 overlay header to all of the NVEs to which the TSs in the VN are attached.
</t>

	<t>
   However, the MSN may not always be aware of the client-specific
   multicast addresses.  In order to perform multicast filtering, the
   MSN has to snoop the IGMP/MLD messages between TSs and their
   corresponding routers to maintain the multicast membership. In order
   for the MSN to snoop the IGMP/MLD messages between TSs and their
   router, the NVA needs to configure the NVE to send copies of the
   IGMP/MLD messages to the MSN in addition to the default behavior of
   sending them to the TSs' routers; e.g., the NVA has to inform the
   NVEs to encapsulate data frames with the Destination Address (DA) being
	224.0.0.2 (DA of IGMP report) to the TSs' router and MSN.</t>

	<t>
   This process is similar to "Source Replication" described in <xref
   target="section-3.2"/>, except the NVEs only replicate the message to the TSs' router and
   MSN.</t>

	</section>

	<section title="Multicast Membership Management for DC with VMs" anchor="section-5.2"><t>
   For DCs with virtualized servers, VMs can be added, deleted,
   or moved very easily. When VMs are added, deleted, or moved, the NVEs
   to which the VMs are attached are changed.</t>

	<t>
   When a VM is deleted from an NVE or a new VM is added to an NVE, the
   VM management system should notify the MSN to send the IGMP/MLD
   query messages to the relevant NVEs (as described in <xref target="section-3.3"/>)
   so that the multicast membership can be updated promptly.</t>

	<t>
   Otherwise, if there are changes of VMs attachment to NVEs (within
   the duration of the configured default time interval that the TSs
   routers use for IGMP/MLD queries), multicast data may not reach the
   VM(s) that moved.</t>

	</section>

	</section>

	<section title="Security Considerations" anchor="section-6"><t>
   This document does not introduce any new security considerations beyond
   what is described in the NVO3 Architecture document <xref target="RFC8014"/>.</t>

	</section>

<section title="IANA Considerations" anchor="section-7"><t>
This document does not require any IANA actions.</t>

</section>

	<section title="Summary" anchor="section-8"><t>
   This document has identified various mechanisms for supporting
   application-specific multicast in networks that use NVO3.  It
   highlights the basics of each mechanism and some of the issues with
   them.  As solutions are developed, the protocols would need to
   consider the use of these mechanisms, and coexistence may be a
   consideration.  It also highlights some of the requirements for
   supporting multicast applications in an NVO3 network.</t>

	</section>
	</middle>

	<back>
	<references title="Normative References">
	&RFC3376;
	&RFC6513;
	&RFC7364;
<!--[rfced] Please note that RFC 7365 appeared in both the Normative
and Informative References section. It was removed from the Informative
References.  Please let us know any objections.-->
	&RFC7365;
	&RFC8014;
	</references>
	<references title="Informative References">
	&RFC2131;  
	&RFC2710;
	&RFC3569;
	&RFC3819;
	&RFC4762;
	&RFC6040;
	&RFC6831;
	&RFC7117;
	&RFC7348;
	&RFC7637;

<!--draft-ietf-bier-architecture-08: Now RFC 8279-->
&RFC8279;

<!--draft-mcbride-armd-mcast-overview-02: I-D Expired-->
<reference anchor='DC-MC'>
<front>
<title>Multicast in the Data Center Overview</title>

<author initials='M' surname='McBride' fullname='Mike McBride'>
    <organization />
</author>

<author initials='H' surname='Liu' fullname='Hui Liu'>
    <organization />
</author>

<date month='July' year='2012' />

<abstract><t>There has been much interest in issues surrounding massive amounts of hosts in the data center.  These issues include the prevalent use of IP Multicast within the Data Center.  Its important to understand how IP Multicast is being deployed in the Data Center to be able to understand the surrounding issues with doing so.  This document provides a quick survey of uses of multicast in the data center and should serve as an aid to further discussion of issues related to large amounts of multicast in the data center.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-mcbride-armd-mcast-overview-02' />

</reference>


<!--draft-marques-l3vpn-mcast-edge-01: I-D Expired-->
<reference anchor='EDGE-REP'>
<front>
<title>Edge multicast replication for BGP IP VPNs.</title>

<author initials='P' surname='Marques' fullname='Pedro Marques'>
    <organization />
</author>

<author initials='L' surname='Fang' fullname='Luyuan Fang'>
    <organization />
</author>

<author initials='D' surname='Winkworth' fullname='Derick Winkworth'>
    <organization />
</author>

<author initials='Y' surname='Cai' fullname='Yiqun Cai'>
    <organization />
</author>

<author initials='P' surname='Lapukhov' fullname='Petr Lapukhov'>
    <organization />
</author>

<date month='June' year='2012' />

<abstract><t>In data-center networks it is common to use Clos network topologies [clos] in order to provide a non-blocking switched network.  In these topologies it is often not desirable to provide native IP multicast service.  This document defines a multicast replication algorithm along with its control and data forwarding procedures that provides a multicast service for a BGP IP VPN network without assuming that the underlying infrastructure supports IP multicast.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-marques-l3vpn-mcast-edge-01' />

</reference>

<!--draft-ietf-nvo3-geneve-05: I-D Version Updated from -01; I-D exists-->
<reference anchor='Geneve'>
<front>
<title>Geneve: Generic Network Virtualization Encapsulation</title>

<author initials='J' surname='Gross' fullname='Jesse Gross'>
    <organization />
</author>

<author initials='I' surname='Ganga' fullname='Ilango Ganga'>
    <organization />
</author>

<author initials='T' surname='Sridhar' fullname='T. Sridhar'>
    <organization />
</author>

<date month='September' year='2017' />

<abstract><t>Network virtualization involves the cooperation of devices with a wide variety of capabilities such as software and hardware tunnel endpoints, transit fabrics, and centralized control clusters.  As a result of their role in tying together different elements in the system, the requirements on tunnels are influenced by all of these components.  Flexibility is therefore the most important aspect of a tunnel protocol if it is to keep pace with the evolution of the system.  This draft describes Geneve, a protocol designed to recognize and accommodate these changing capabilities and needs.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-ietf-nvo3-geneve-05' />

</reference>

<reference anchor='GUE'>
<front>
<title>Generic UDP Encapsulation</title>

<author initials='T' surname='Herbert' fullname='Tom Herbert'>
    <organization />
</author>

<author initials='L' surname='Yong' fullname='Lucy Yong'>
    <organization />
</author>

<author initials='O' surname='Zia' fullname='Osama Zia'>
    <organization />
</author>

<date month='May' year='2017' />

<abstract><t>This specification describes Generic UDP Encapsulation (GUE), which is a scheme for using UDP to encapsulate packets of different IP protocols for transport across layer 3 networks. By encapsulating packets in UDP, specialized capabilities in networking hardware for efficient handling of UDP packets can be leveraged. GUE specifies basic encapsulation methods upon which higher level constructs, such as tunnels and overlay networks for network virtualization, can be constructed. GUE is extensible by allowing optional data fields as part of the encapsulation, and is generic in that it can encapsulate packets of various IP protocols.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-ietf-intarea-gue-04' />
<format type='TXT'
        target='http://www.ietf.org/internet-drafts/draft-ietf-intarea-gue-04.txt' />
</reference>


<!--draft-yong-isis-ext-4-distribution-tree-03: I-D Expired-->
<reference anchor='ISIS-Multicast'>
<front>
<title>IS-IS Protocol Extension For Building Distribution Trees</title>

<author initials='L' surname='Yong' fullname='Lucy Yong'>
    <organization />
</author>

<author initials='H' surname='Weiguo' fullname='Hao Weiguo'>
    <organization />
</author>

<author initials='D' surname='Eastlake' fullname='Donald Eastlake'>
    <organization />
</author>

<author initials='A' surname='Qu' fullname='Andrew Qu'>
    <organization />
</author>

<author initials='J' surname='Hudson' fullname='Jon Hudson'>
    <organization />
</author>

<author initials='U' surname='Chunduri' fullname='Uma Chunduri'>
    <organization />
</author>

<date month='October' year='2014' />

<abstract><t>This document proposes an IS-IS protocol extension to support IGP based multicast transport architecture and solution [IGP-MCAST].</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-yong-isis-ext-4-distribution-tree-03' />

</reference>

<reference anchor="LANE">
<front>
<title>LAN Emulation Over ATM: Version 1.0</title>
<author>
  <organization>ATM Forum</organization>
</author>
<date month='January' year='1995'/>
</front>
<seriesInfo name="ATM Forum Technical Committee," value="af-lane-0021.000"/>
</reference>

<!--draft-ietf-lisp-signal-free-multicast-07: I-D Updated from -01; -->
<reference anchor='LISP-Signal-Free'>
<front>
<title>Signal-Free LISP Multicast</title>

<author initials='V' surname='Moreno' fullname='Victor Moreno'>
    <organization />
</author>

<author initials='D' surname='Farinacci' fullname='Dino Farinacci'>
    <organization />
</author>

<date month='November' year='2017' />

<abstract><t>When multicast sources and receivers are active at LISP sites, the core network is required to use native multicast so packets can be delivered from sources to group members.  When multicast is not available to connect the multicast sites together, a signal-free mechanism can be used to allow traffic to flow between sites.  The mechanism within here uses unicast replication and encapsulation over the core network for the data-plane and uses the LISP mapping database system so encapsulators at the source LISP multicast site can find decapsulators at the receiver LISP multicast sites.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-ietf-lisp-signal-free-multicast-07' />

</reference>

<!--draft-ietf-nvo3-vxlan-gpe-05: I-D Updated from -02; I-D Exists-->
<reference anchor='VXLAN-GPE'>
<front>
<title>Generic Protocol Extension for VXLAN</title>

<author initials='F' surname='Maino' fullname='Fabio Maino'>
    <organization />
</author>

<author initials='L' surname='Kreeger' fullname='Larry Kreeger'>
    <organization />
</author>

<author initials='U' surname='Elzur' fullname='Uri Elzur'>
    <organization />
</author>

<date month='October' year='2017' />

<abstract><t>This draft describes extending Virtual eXtensible Local Area Network (VXLAN), via changes to the VXLAN header, with three new capabilities: support for multi-protocol encapsulation, operations, administration and management (OAM) signaling and explicit versioning.</t></abstract>

</front>

<seriesInfo name='Work in Progress,' value='draft-ietf-nvo3-vxlan-gpe-05' />

</reference>

	</references>
	<section title="Acknowledgments" numbered="no" anchor="acknowledgments"><t>
   Many thanks are due to Dino Farinacci, Erik Nordmark, Lucy Yong,
   Nicolas Bouliane, Saumya Dikshit, Joe Touch, Olufemi Komolafe, and
   Matthew Bocci for their valuable comments and suggestions.</t>

	</section>

	</back>



	</rfc>
	
