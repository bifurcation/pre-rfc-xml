<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [

<!ENTITY RFC2119 SYSTEM "reference.RFC.2119.xml">
<!ENTITY RFC3654 SYSTEM "reference.RFC.3654.xml">
<!ENTITY RFC3746 SYSTEM "reference.RFC.3746.xml">
<!ENTITY RFC5657 SYSTEM "reference.RFC.5657.xml">
<!ENTITY RFC5812 SYSTEM "reference.RFC.5812.xml">
<!ENTITY RFC5810 SYSTEM "reference.RFC.5810.xml">
<!ENTITY RFC5811 SYSTEM "reference.RFC.5811.xml">
<!ENTITY RFC7391 SYSTEM "reference.RFC.7391.xml">
<!ENTITY RFC7408 SYSTEM "reference.RFC.7408.xml">
<!ENTITY RFC6956 SYSTEM "reference.RFC.6956.xml">
<!ENTITY RFC2460 SYSTEM "reference.RFC.2460.xml">
]>
<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>

<?rfc strict="yes" ?>
<?rfc needLines="yes" ?> 
<?rfc toc="yes"?>
<?rfc tocdepth="4"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes" ?>
<?rfc compact="yes" ?>
<?rfc subcompact="no" ?>
<?rfc needLines="yes" ?>
<rfc number="8013" category="std" submissionType="IETF" consensus="yes" ipr="trust200902">

  <front>

    <title abbrev="ForCES Inter-FE LFB">Forwarding and Control
    Element&nbsp;Separation&nbsp;(ForCES) Inter&nbhy;FE&nbsp;Logical&nbsp;Functional Block (LFB)</title>


        <author fullname="Damascane M. Joachimpillai" initials="D.J." surname="Joachimpillai">
                <organization>Verizon</organization>
                <address>
                        <postal>
                                <street>60 Sylvan Rd</street>

                                <city>Waltham</city>
                                <region>MA</region>
                                <code>02451</code>
                                <country>United States of America </country>
                        </postal>
                        <email>damascene.joachimpillai@verizon.com</email>

                </address>
        </author>
 
        <author fullname="Jamal Hadi Salim" initials="J." surname="Hadi Salim">
                <organization>Mojatatu Networks</organization>
                <address>
                        <postal>
                                <street>Suite 200, 15 Fitzgerald Rd.</street>
                                <city>Ottawa</city>
                                <code>K2H 9G1</code>
                                <region>Ontario</region>
                                <country>Canada</country>
                        </postal>
                        <email>hadi@mojatatu.com</email>

                </address>
        </author>
      
    <date month="November" year="2016" />

    <area>Routing</area>

    <workgroup>Internet Engineering Task Force</workgroup>

    <keyword>ForCES</keyword>
    <keyword>Inter-FE</keyword>

    <abstract>
   <t>
    This document describes how to extend the Forwarding and Control Element
    Separation (ForCES) Logical Functional Block (LFB) topology across
    Forwarding Elements (FEs) by defining the inter-FE LFB class. The inter-FE LFB class
    provides the ability to pass data and metadata across FEs without
    needing any changes to the ForCES specification.
    The document focuses on Ethernet transport.
   </t>
    </abstract>
  </front>

  <middle>


<section title="Introduction">
    <t> 
    In the ForCES architecture, a packet service can be modeled by composing
    a graph of one or more LFB instances. The reader is referred to the details 
    in <xref target="RFC5812"> the ForCES model</xref>.
    </t>
   <t>
    The ForCES model describes the processing within a single
    Forwarding Element (FE) in terms of Logical Functional Blocks (LFBs),
    including provision for the Control Element (CE) to establish and
    modify that processing sequence, and the parameters of the
    individual LFBs.
   </t>
   <t>
   Under some circumstances, it would be beneficial to be able to extend
   this view and the resulting processing across more than one FE.
   This may be in order to achieve scale by splitting the processing
   across elements or to utilize specialized hardware available on
   specific FEs.
   </t>
   <t>
    Given that the ForCES inter-LFB architecture calls for the ability to pass
    metadata between LFBs, it is imperative to define mechanisms to 
    extend that existing feature and allow passing the metadata between LFBs
    across FEs.
   </t>
   <t>
    This document describes how to extend the LFB topology across
    FEs, i.e., inter-FE connectivity without needing any changes to the
    ForCES definitions. It focuses on using Ethernet as the interconnection
    between FEs.
   </t>
</section>

   <section title="Terminology and Conventions">
      <t/>
   
      <section title="Requirements Language">
        <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", 
        "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
         document are to be interpreted as described in
      <xref target="RFC2119"></xref>.</t>
      </section>

 <section title="Definitions">
        <t>This document depends on the terms (below) defined in several ForCES
        documents: <xref target="RFC3746"></xref>, <xref
        target="RFC5810"></xref>, <xref target="RFC5811"></xref>, 
	<xref target="RFC5812"></xref>, <xref target="RFC7391"></xref>, and <xref
	target="RFC7408"></xref>.</t>
      <t><list style="hanging">
<t>
Control Element (CE)
</t> 
<t>
Forwarding Element (FE)
</t> 
<t>
FE Model
</t> 
<t>
LFB (Logical Functional Block) Class (or type)
</t> 
<t>
LFB Instance
</t> 
<t>
LFB Model
</t> 
<t>
LFB Metadata
</t> 
<t>ForCES Component
</t> 
<t>
LFB Component
</t> 
<t>ForCES Protocol Layer (ForCES PL)
</t> 
<t>ForCES Protocol Transport Mapping Layer (ForCES TML)
</t> 
</list></t>

</section>
</section>

<section title="Problem Scope and Use Cases">
  <t> 
  The scope of this document is to solve the challenge of passing
  ForCES-defined metadata alongside packet data across FEs (be they physical
  or virtual) for the purpose of distributing the LFB processing.
  </t> 

  <section title="Assumptions">
   <t>
   <list style="symbols">
   <t>
           The FEs involved in the inter-FE LFB belong to the same Network
           Element (NE) and are within a single administrative private network
           that is in close proximity.
   </t>
   <t>
           The FEs are already interconnected using Ethernet.
           We focus on Ethernet because it is commonly used for FE interconnection.
           Other higher transports (such as UDP over IP) or lower transports
           could be defined to carry the data and metadata, but
           these cases are not addressed in this document.
   </t>
   </list>

   </t>
  </section>

  <section title="Sample Use Cases">
   <t>
  To illustrate the problem scope, we present two use cases where we start
  with a single FE running all the LFBs functionality and then split it into
  multiple FEs achieving the same end goals.
   </t>
  <section title="Basic IPv4 Router">
  <t> 
  A sample LFB topology depicted in <xref target="fig1"></xref> demonstrates
  a service graph for delivering a basic IPv4-forwarding service
  within one FE. For the purpose of illustration, the diagram shows 
  LFB classes as graph nodes instead of multiple LFB class instances.
  </t>
  <t>
  Since the purpose of the illustration in <xref target="fig1"></xref> is 
  to showcase how data
  and metadata are sent down or upstream on a graph of LFB instances,
  it abstracts out any ports in both directions
  and talks about a generic ingress and egress LFB. Again, for illustration
  purposes, the diagram does not show exception or error paths.
  Also left out are details on Reverse Path Filtering, ECMP, multicast
  handling, etc. In other words, this is not meant to be a complete description
  of an IPv4-forwarding application; for a more complete example, please refer 
  <xref target="RFC6956"> to the LFBLibrary document</xref>.

  </t>
  <t>
  The output of the ingress LFB(s) coming into the IPv4 Validator LFB 
  will have both the IPv4 packets and, depending on the implementation,
  a variety of ingress metadata such as offsets into the different headers, any 
  classification metadata, physical and virtual ports encountered,
  tunneling information, etc. These metadata are lumped together as
  "ingress metadata".
  </t>
  <t>
  Once the IPv4 validator vets the packet (for example, it ensures that there
  is no expired TTL), it feeds the packet and inherited metadata into the IPv4 unicast
  LPM (Longest-Prefix-Matching) LFB. 
  </t>
  <t>
   <figure title="Basic IPv4 Packet Service LFB Topology" align="left" anchor="fig1">
   <artwork><![CDATA[

                   +----+                           
                   |    |                           
        IPv4 pkt   |    | IPv4 pkt     +-----+             +---+    
    +------------->|    +------------->|     |             |   |    
    |  + ingress   |    | + ingress    |IPv4 |   IPv4 pkt  |   |    
    |   metadata   |    | metadata     |Ucast+------------>|   +--+ 
    |              +----+              |LPM  |  + ingress  |   |  | 
  +-+-+             IPv4               +-----+  + NHinfo   +---+  | 
  |   |             Validator                   metadata   IPv4   | 
  |   |             LFB                                    NextHop| 
  |   |                                                     LFB   | 
  |   |                                                           | 
  |   |                                                  IPv4 pkt |
  |   |                                               + {ingress  |
  +---+                                                  + NHdetails}
  Ingress                                                metadata |
   LFB                                +--------+                  | 
                                      | Egress |                  | 
                                   <--+        |<-----------------+ 
                                      |  LFB   |
                                      +--------+     
    ]]></artwork>
   </figure>
  </t>

<t>
The IPv4 unicast LPM LFB does an LPM lookup on the
IPv4 FIB using the destination IP address as a search key. The result is 
typically a next-hop selector, which is passed downstream as metadata.
</t>
<t>
The NextHop LFB receives the IPv4 packet with associated next-hop (NH)
information metadata. The NextHop LFB consumes the NH information metadata and 
derives a table index from it to look up the next-hop table in order to 
find the appropriate egress information. The lookup result
is used to build the next-hop details to be used
downstream on the egress. This information may include any source 
and destination information (for our purposes, which Media Access Control (MAC) addresses to use)
as well as egress ports.
(Note: It is also at this LFB where typically, the forwarding TTL-decrementing
and IP checksum recalculation occurs.)
</t>
<t>
The details of the egress LFB are considered out of scope for this discussion.
Suffice it to say that somewhere within or beyond the Egress LFB, the
IPv4 packet will be sent out a port (e.g., Ethernet, virtual or physical).
</t>
<section title="Distributing the Basic IPv4 Router">
<t>
<xref target="fig2"></xref> demonstrates one way that the router LFB topology
in <xref target="fig1"></xref> may be split across two FEs (e.g., two
Application-Specific Integrated Circuits (ASICs)).
<xref target="fig2"></xref> shows the LFB topology split across FEs after the
IPv4 unicast LPM LFB.
</t>
  <t>
   <figure title="Split IPv4 Packet Service LFB Topology" align="left" anchor="fig2">
   <artwork><![CDATA[
   FE1
 +-------------------------------------------------------------+
 |                            +----+                           |
 | +----------+               |    |                           |
 | | Ingress  |    IPv4 pkt   |    | IPv4 pkt     +-----+      |
 | |  LFB     +-------------->|    +------------->|     |      |
 | |          |  + ingress    |    | + ingress    |IPv4 |      |
 | +----------+    metadata   |    |   metadata   |Ucast|      |
 |      ^                     +----+              |LPM  |      |
 |      |                      IPv4               +--+--+      |
 |      |                     Validator              |         |
 |                             LFB                   |         |
 +---------------------------------------------------|---------+
                                                     |
                                                IPv4 packet +
                                              {ingress + NHinfo}
                                                  metadata
   FE2                                               |
 +---------------------------------------------------|---------+
 |                                                   V         | 
 |             +--------+                       +--------+     |
 |             | Egress |     IPv4 packet       | IPv4   |     |
 |       <-----+  LFB   |<----------------------+NextHop |     |
 |             |        |{ingress + NHdetails}  | LFB    |     |
 |             +--------+      metadata         +--------+     |
 +-------------------------------------------------------------+
    ]]></artwork>
   </figure>
   </t>

   <t>
   Some proprietary interconnections (for example, Broadcom HiGig over XAUI
   <xref target="brcm-higig"></xref>)
   are known to exist to carry both the IPv4 packet and the related
   metadata between the IPv4 Unicast LFB and IPv4NextHop LFB across the two
   FEs.
   </t>

   <t>
     This document defines the inter-FE LFB, a standard mechanism for 
   encapsulating, generating, receiving, and decapsulating 
   packets and associated metadata FEs over Ethernet.
   </t>

</section>
</section>

  <section title="Arbitrary Network Function">
  <t>
  In this section, we show an example of an arbitrary Network Function
  that is more coarsely grained in terms of functionality. Each
  Network Function may constitute more than one LFB.
   <figure title="A Network Function Service Chain within One FE" align="left" anchor="fig31">
   <artwork><![CDATA[
   FE1
 +-------------------------------------------------------------+
 |                            +----+                           |
 | +----------+               |    |                           |
 | | Network  |   pkt         |NF2 |    pkt       +-----+      |
 | | Function +-------------->|    +------------->|     |      |
 | |    1     |  + NF1        |    | + NF1/2      |NF3  |      |
 | +----------+    metadata   |    |   metadata   |     |      |
 |      ^                     +----+              |     |      |
 |      |                                         +--+--+      |
 |      |                                            |         |
 |                                                   |         |
 +---------------------------------------------------|---------+
                                                     V         
    ]]></artwork>
   </figure>
  </t>
  <t>
  The setup in <xref target="fig31"></xref> is typical of most packet processing
  boxes where we have functions like deep packet inspection (DPI), NAT, Routing, etc., connected in
  such a topology to deliver a packet processing service to flows.
  </t>
   <section title="Distributing the Arbitrary Network Function">
  <t>
  The setup in <xref target="fig31"></xref> can be split across
  three FEs instead of as demonstrated in <xref target="fig32"></xref>.
  This could be motivated by scale-out reasons or because different
  vendors provide different functionality, which is plugged-in to provide
  such functionality. The end result is having the same packet service 
  delivered to the different flows passing through.
  </t>
  <t>
   <figure title="A Network Function Service Chain Distributed across Multiple FEs" align="left" anchor="fig32">
   <artwork><![CDATA[

   FE1                        FE2
   +----------+               +----+               FE3             
   | Network  |   pkt         |NF2 |    pkt       +-----+       
   | Function +-------------->|    +------------->|     |       
   |    1     |  + NF1        |    | + NF1/2      |NF3  |       
   +----------+    metadata   |    |   metadata   |     |       
        ^                     +----+              |     |       
        |                                         +--+--+       
                                                     |          
                                                     V         
    ]]></artwork>
   </figure>
   </t>
    </section>

  </section>
  </section>
</section>

<section title="Inter-FE LFB Overview">
<t>
We address the inter-FE connectivity requirements by defining the 
inter-FE LFB class.
Using a standard LFB class definition implies no change to the basic 
ForCES architecture in the form of the core LFBs (FE Protocol or Object LFBs).
This design choice was made after considering an alternative approach that would have 
required changes to both the FE Object capabilities
(SupportedLFBs) and the LFBTopology component to describe the inter-FE
connectivity capabilities as well as the runtime topology of the LFB instances.
</t>

  <section title="Inserting the Inter-FE LFB">
   <t>
   The distributed LFB topology described in  <xref target="fig2"></xref> is
   re-illustrated in <xref target="fig3"></xref> to show the topology
   location where the inter-FE LFB would fit in.
   </t>

  <t>
  As can be observed in <xref target="fig3"></xref>, the same details
  passed between IPv4 unicast LPM LFB and the IPv4 NH LFB are passed
  to the egress side of the inter-FE LFB. This information is illustrated
  as multiplicity of inputs into the egress inter-FE LFB instance. Each
  input represents a unique set of selection information.
  </t> 
  <t>
   <figure title="Split IPv4-Forwarding Service with Inter-FE LFB" align="left" anchor="fig3">
   <artwork><![CDATA[
   FE1
 +-------------------------------------------------------------+
 | +----------+               +----+                           |
 | | Ingress  |    IPv4 pkt   |    | IPv4 pkt     +-----+      |
 | |  LFB     +-------------->|    +------------->|     |      |
 | |          |  + ingress    |    | + ingress    |IPv4 |      |
 | +----------+    metadata   |    |   metadata   |Ucast|      |
 |      ^                     +----+              |LPM  |      |
 |      |                      IPv4               +--+--+      |
 |      |                     Validator              |         |
 |      |                      LFB                   |         |
 |      |                                  IPv4 pkt + metadata |
 |      |                                   {ingress + NHinfo} |
 |      |                                            |         |
 |      |                                       +..--+..+      |
 |      |                                       |..| |  |      |
 |                                            +-V--V-V--V-+    |
 |                                            |   Egress  |    |
 |                                            |  Inter-FE |    |
 |                                            |   LFB     |    |
 |                                            +------+----+    |
 +---------------------------------------------------|---------+
                                                     |
                             Ethernet Frame with:    |
                             IPv4 packet data and metadata
                             {ingress + NHinfo + Inter-FE info}
  FE2                                                |
 +---------------------------------------------------|---------+
 |                                                +..+.+..+    |
 |                                                |..|.|..|    |
 |                                              +-V--V-V--V-+  |
 |                                              | Ingress   |  |
 |                                              | Inter-FE  |  |
 |                                              |   LFB     |  |
 |                                              +----+------+  |
 |                                                   |         | 
 |                                         IPv4 pkt + metadata |
 |                                          {ingress + NHinfo} |
 |                                                   |         | 
 |             +--------+                       +----V---+     |
 |             | Egress |     IPv4 packet       | IPv4   |     |
 |       <-----+  LFB   |<----------------------+NextHop |     |
 |             |        |{ingress + NHdetails}  | LFB    |     |
 |             +--------+      metadata         +--------+     |
 +-------------------------------------------------------------+
    ]]></artwork>
   </figure>
   </t>

  <t> 
  The egress of the inter-FE LFB uses the received packet and metadata
  to select details for encapsulation when sending 
  messages towards the selected neighboring FE.
  These details include what to communicate as the 
  source and destination FEs (abstracted as MAC addresses as described in
  <xref target="etherencap"></xref>); in addition, the original metadata 
  may be passed along with the original IPv4
  packet.
  </t> 
  <t> 
  On the ingress side of the inter-FE LFB, the received packet and its associated
  metadata are used to decide the packet graph continuation. This includes which
  of the original metadata and on which next
  LFB class instance to continue processing.
  In <xref target="fig3"></xref>, an IPv4NextHop LFB instance
  is selected and the appropriate metadata is passed to it.
  </t> 
  <t> 
  The ingress side of the inter-FE LFB consumes some of the information passed
  and passes it the IPv4 packet alongside with the ingress and NHinfo metadata
  to the IPv4NextHop LFB as was done earlier in both Figures 
   <xref target="fig1" format="counter" /> and <xref target="fig2"
   format="counter" />.
  </t> 
  </section>

 </section>


<section title="Inter-FE Ethernet Connectivity" anchor="ethercon">

   <t>
   <xref target="etherissues"></xref> describes some of the issues 
   related to using Ethernet as the transport and 
   how we mitigate them. 
   </t>
   <t>
<xref target="etherencap"></xref> defines a payload format that is to be used 
   over Ethernet. An existing implementation of this specification
   that runs on top of Linux Traffic Control <xref target="linux-tc"></xref> 
   is described in <xref target="tc-ife"></xref>.
   </t>

<section title="Inter-FE Ethernet Connectivity Issues" anchor="etherissues">
  <t>
  There are several issues that may occur due to using direct Ethernet
  encapsulation that need consideration.
  </t>

<section title="MTU Consideration" anchor="ethermissues">
           <t>
           Because we are adding data to existing Ethernet frames, MTU issues
           may arise. We recommend:
                   <list style="symbols">
                   <t>
                   Using large MTUs when possible (example with jumbo frames).
                   </t>
                   <t>
                   Limiting the amount of metadata that could be transmitted; our
                   definition allows for filtering of
                   select metadata to be encapsulated in the frame as described
                   in <xref target="det1"></xref>. 
<!-- [rfced] We are having trouble parsing this sentence.  Please clarify.

Original:
      We recommend
      sizing the egress port MTU so as to allow space for maximum size
      of the metadata total size to allow between FEs.
-->

                   We recommend sizing the egress port MTU so as to allow
                   space for maximum size of the metadata
                   total size to allow between FEs. In such a setup,
                   the port is configured to "lie" to the upper layers by
                   claiming to have a lower MTU than it is capable of.
                   Setting the MTU can be achieved by ForCES control of
                   the port LFB (or some other configuration. 
                   In essence, the control plane when explicitly making
                   a decision for the MTU settings of the egress port 
                   is implicitly deciding how much metadata will be allowed.
                   Caution needs to be exercised on how low the resulting 
                   reported link MTU could be:
                   for IPv4 packets, the minimum size is 64 octets
                   <xref target="RFC791"/> and for IPv6 the minimum size is 1280 octets
                   <xref target="RFC2460"></xref>.
                   </t>
                   </list>
           </t>
   </section>

<section title="Quality-of-Service Considerations" anchor="ethercos">
 <t>
         A raw packet arriving at the
         inter-FE LFB (from upstream LFB class instances) may have
         Class-of-Service (CoS) metadata indicating how it should be treated 
         from a Quality-of-Service perspective. 
 </t>
 <t>
         The resulting Ethernet frame will be eventually (preferentially) 
         treated by a downstream LFB (typically a port LFB instance)
         and their CoS marks will be honored in terms of priority. In other
         words, the presence of the inter-FE LFB does not change the
         CoS semantics.
 </t>

</section>

<section title="Congestion Considerations" anchor="ethercng">

<t>
Most of the traffic passing through FEs that utilize the inter-FE LFB is
expected to be IP based, which is generally assumed to be congestion
controlled <xref target="UDP-GUIDE"></xref>. 
For example, if congestion causes a TCP packet annotated with additional 
ForCES metadata to be dropped
between FEs, the sending TCP can be expected to react in the same
fashion as if that packet had been dropped at a different point
on its path where ForCES is not involved.   For this reason, additional
inter-FE congestion-control mechanisms are not specified.
</t>

<t>
   However, the increased packet size due to the addition of ForCES metadata
   is likely to require additional bandwidth on inter-FE links in comparison
   to what would be required to carry the same traffic without ForCES metadata.
   Therefore, traffic engineering SHOULD be done when deploying
   inter-FE encapsulation.
</t>
<t>

Furthermore, the inter-FE LFB MUST only be deployed within a single 
network (with a single network operator)
or networks of an adjacent set of cooperating network operators where
traffic is managed to avoid congestion. These are Controlled
Environments, as defined by Section 3.6 of
<xref target="UDP-GUIDE"></xref>.
Additional measures SHOULD be imposed to restrict the impact of 
inter-FE-encapsulated traffic on other traffic; for example:

<list style="symbols">
        <t>
        rate-limiting all inter-FE LFB traffic at an upstream LFB
        </t>
        <t>
managing circuit breaking <xref target="circuit-b"></xref>
        </t>
        <t>
Isolating the inter-FE traffic either via dedicated interfaces or VLANs
        </t>
</list>

 </t>

</section>


</section>

<section title="Inter-FE Ethernet Encapsulation" anchor="etherencap">
<t>
The Ethernet wire encapsulation is illustrated in <xref target="fige1"></xref>.
The process that leads to this encapsulation is described in 
        <xref target="det1"></xref>. The resulting frame is 32-bit aligned.
</t>

  <t>
   <figure title="Packet Format Definition" align="left" anchor="fige1">
   <artwork><![CDATA[
    0                   1                   2                   3
    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | Destination MAC Address                                       |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | Destination MAC Address       |   Source MAC Address          |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | Source MAC Address                                            |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | Inter-FE ethertype            | Metadata length               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | TLV encoded Metadata ~~~..............~~                      |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | TLV encoded Metadata ~~~..............~~                      |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | Original packet data ~~................~~                     |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    ]]></artwork>
   </figure>
   </t>

<t>
        The Ethernet header (illustrated in <xref target="fige1"></xref>)
        has the following semantics:

<list style="symbols">
<t>
The Destination MAC Address is used to identify the Destination FEID by the CE
policy (as described in <xref target="det1"></xref>).
</t>
<t>
The Source MAC Address is used to identify the Source FEID by the CE policy
(as described in <xref target="det1"></xref>).
</t>
<t>
The ethertype is used to identify the frame as inter-FE LFB type. 
Ethertype TBA1 is to be used.

</t>
<t>
The 16-bit metadata length is used to describe the total encoded metadata
length (including the 16 bits used to encode the metadata length).
</t>
<t>
One or more 16-bit TLV-encoded metadatum follows the Metadata length field.
The TLV type 
identifies the metadata ID. ForCES metadata IDs that have been registered with
IANA will be used.
All TLVs will be 32-bit-aligned.
We recognize that using a 16-bit TLV
restricts the metadata ID to 16 bits instead of a ForCES-defined component ID
space of 32 bits if an Index-Length-Value (ILV) is used. However, at the time of publication, we
believe this is sufficient to carry all the information we need; the TLV approach
has been selected because it saves us 4 bytes per metadatum transferred as
compared to the ILV approach.
</t>
<t>
The original packet data payload is appended at the end of the metadata as
shown.
</t>
</list>
</t>

 </section>
</section>

  <section title="Detailed Description of the Ethernet Inter-FE LFB" anchor="det1">
  <t>
  The Ethernet inter-FE LFB has two LFB input port groups and three
  LFB output ports as shown in <xref target="fig6"></xref>.
  </t>

  <t>   
  The inter-FE LFB defines two components used in aiding processing
 described in <xref target="data"></xref>.
    </t>
  <t>
   <figure title="Inter-FE LFB" align="left" anchor="fig6">
   <artwork><![CDATA[
                 +-----------------+
  Inter-FE LFB   |                 |
  Encapsulated   |             OUT2+--> Decapsulated Packet
  -------------->|IngressInGroup   |       + metadata 
  Ethernet Frame |                 |
                 |                 |
  raw Packet +   |             OUT1+--> Encapsulated Ethernet
  -------------->|EgressInGroup    |           Frame
  Metadata       |                 |
                 |    EXCEPTIONOUT +--> ExceptionID, packet
                 |                 |           + metadata
                 +-----------------+
    ]]></artwork>
   </figure>
   </t>
    <section anchor="data" title="Data Handling">
    <t>
    The inter-FE LFB (instance) can be positioned at the egress of a source FE.
    <xref target="fig3"></xref> illustrates an example source FE in the form
    of FE1.
    In such a case, an inter-FE LFB instance receives, via port group
EgressInGroup,
    a raw packet and associated metadata from the preceding LFB instances.
    The input information is used to produce a selection of how to generate
    and encapsulate the new frame. The set of all selections is stored in
    the LFB component IFETable described further below.

    The processed encapsulated Ethernet frame will go out on 
    OUT1 to a downstream LFB instance when processing succeeds 
    or to the EXCEPTIONOUT port in the case of failure.
    </t>
    <t> 
    The inter-FE LFB (instance) can be positioned at the ingress of a
receiving FE.
    <xref target="fig3"></xref> illustrates an example destination FE in
the form of FE1.
    In such a case, an inter-FE LFB receives, via an LFB port in the
    IngressInGroup, an encapsulated Ethernet frame.
    Successful processing of the packet will result in a raw packet with 
    associated metadata IDs going downstream to an LFB connected on OUT2.
    On failure, the data is sent out EXCEPTIONOUT.
    </t>

    <section title="Egress Processing">

    <t>
     The egress inter-FE LFB receives packet data and any accompanying
     metadatum at an LFB port of the LFB instance's input port group labeled EgressInGroup.
    </t>

    <t>
    The LFB implementation may use the incoming LFB port (within the LFB
    port group EgressInGroup) to map to a table
    index used to look up the IFETable table. 
    </t>

    <t>
    If the lookup is successful, a matched table row that has the IFEInfo
    details is retrieved with
    the tuple (optional IFETYPE, optional StatId,
    Destination MAC address (DSTFE), Source MAC address (SRCFE),
    and optional metafilters). 
    The metafilters lists define a whitelist of which metadatum
    are to be passed to the neighboring FE.
    The inter-FE LFB will perform the following actions using the resulting
    tuple: 
    </t>

    <t>
    <list style="symbols">

       <t>
       Increment statistics for packet and byte count observed at
       the corresponding IFEStats entry.
       </t>
       <t>
       When the MetaFilterList is present, walk each received metadatum and apply it against the MetaFilterList.
   If no legitimate metadata is found that needs to be passed downstream, then
   the processing stops and the packet and metadata are sent out the
   EXCEPTIONOUT port with the exceptionID of EncapTableLookupFailed <xref target="RFC6956"></xref>.
       </t>
       <t>
        Check that the additional overhead of the Ethernet header and encapsulated
        metadata will not exceed MTU. If it does, increment the error-packet-count statistics and send the packet and metadata out the EXCEPTIONOUT
        port with the exceptionID of FragRequired <xref target="RFC6956"></xref>.
       </t>

       <t>
         Create the Ethernet header.
       </t>
       <t>
       Set the Destination MAC address of the Ethernet header with the value
       found in the DSTFE field. 
       </t>
       <t>
       Set the Source MAC address of the Ethernet header with the value found in the 
       SRCFE field.
       </t>
       <t>
       If the optional IFETYPE is present, set the ethertype to the value
       found in IFETYPE.  If IFETYPE is absent, then the standard
       inter-FE LFB ethertype TBA1 is used.
       </t>
       <t>
       Encapsulate each allowed metadatum in a TLV. Use the 
       metaID as the "type" field in the TLV header.
       The TLV should be aligned to 32 bits. This means you may
       need to add a padding of zeroes at the end of the TLV to ensure alignment.
        </t>

       <t>
       Update the metadata length to the sum of each TLV's space
       plus 2 bytes (a 16-bit space for the Metadata length field).
        </t>

</list>
        The resulting packet is sent to the next LFB instance connected to the 
        OUT1 LFB-port, typically a port LFB.
    </t>

    <t>
        In the case of a failed lookup, the original packet and associated
        metadata is sent out the EXCEPTIONOUT port with the exceptionID of
        EncapTableLookupFailed  <xref target="RFC6956"></xref>.
        Note that the EXCEPTIONOUT LFB port is merely an abstraction
        and implementation may in fact drop packets as described above.
    </t>

    </section>

    <section title="Ingress Processing">
    <t>
    An ingressing inter-FE LFB packet is recognized by inspecting
    the ethertype, and optionally the destination and source MAC addresses.
    A matching packet is mapped to an LFB instance port in the IngressInGroup. 
    The IFETable table row entry matching the LFB instance port may have
    optionally programmed metadata filters. In such a case, the ingress 
    processing should use the metadata filters as a whitelist of what
    metadatum is to be allowed.
    <list style="symbols">
    <t>
    Increment statistics for packet and byte count observed.
    </t>
    <t>
    Look at the metadata length field and walk the packet data, extracting
    the metadata values from the TLVs. For each metadatum extracted, in the presence of metadata filters,
    the metaID is compared against the relevant IFETable row metafilter list.
    If the metadatum is recognized and allowed by the filter, 
    the corresponding implementation 
    Metadatum field is set. If an unknown metadatum ID is encountered
    or if the metaID is not in the allowed filter list, then
    the implementation is expected to ignore it, increment the packet error
    statistic, and proceed processing other metadatum.
    </t>
    <t>
    Upon completion of processing all the metadata, the inter-FE LFB instance 
    resets the data point to the original payload
    (i.e., skips the IFE header information). At this point, the original
    packet that was passed to the egress inter-FE LFB at the source FE is
    reconstructed. This data is then passed along with the reconstructed
    metadata downstream to the next LFB instance in the graph.
    </t>
</list>
    </t>

    <t>
    In the case of a processing failure of either ingress or
    egress positioning of the LFB, the packet and metadata are 
    sent out the EXCEPTIONOUT LFB port with the appropriate error ID.
    Note that the EXCEPTIONOUT LFB port is merely an abstraction
    and implementation may in fact drop packets as described above.
    </t>

    </section>

    </section>
    <section title="Components" anchor="comps">
    <t>
            There are two LFB components accessed by the CE. The reader is asked
            to refer to the definitions in <xref target="fig8"></xref>.
 </t>

    <t>
        The first component, populated by the CE, is an array known 
        as the "IFETable" table.
        The array rows are made up of IFEInfo structure.
        The IFEInfo structure constitutes the
        optional IFETYPE, the optionally present StatId, the
        Destination MAC address (DSTFE), the
        Source MAC address (SRCFE), and an optionally present
        array of allowed metaIDs (MetaFilterList).
    </t>
    <t>
    The second component (ID 2), populated by the FE and read by the CE,
    is an indexed array known as the "IFEStats" table.
    Each IFEStats row carries statistics information in
    the structure bstats. 
    </t>
    <t>
    A note about the StatId relationship between the IFETable table
    and the IFEStats table -- 
    an implementation may choose to map between an IFETable row and IFEStats 
    table row using the StatId entry in the matching IFETable row. In that
    case, the IFETable StatId must be present.
   An alternative implementation may map an IFETable row to an IFEStats table
   row at provisioning time. Yet another alternative implementation
    may choose not to use the IFETable row StatId and instead use
    the IFETable row index as the IFEStats index. For these reasons, the StatId
    component is optional.
    </t>
    </section>

    <section title="Inter-FE LFB XML Model">

  <t>
<figure title="Inter-FE LFB XML" align="left" anchor="fig8"><artwork><![CDATA[

<LFBLibrary xmlns="urn:ietf:params:xml:ns:forces:lfbmodel:1.1"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       provides="IFE">
  <frameDefs>

     <frameDef>
         <name>PacketAny</name>
          <synopsis>Arbitrary Packet</synopsis>
     </frameDef>
     <frameDef>
         <name>InterFEFrame</name>
         <synopsis>
                 Ethernet frame with encapsulated IFE information
         </synopsis>
     </frameDef>

  </frameDefs>

  <dataTypeDefs>

    <dataTypeDef>
       <name>bstats</name>
       <synopsis>Basic stats</synopsis>
    <struct>
        <component componentID="1">
         <name>bytes</name>
         <synopsis>The total number of bytes seen</synopsis>
         <typeRef>uint64</typeRef>
        </component>

        <component componentID="2">
         <name>packets</name>
         <synopsis>The total number of packets seen</synopsis>
         <typeRef>uint32</typeRef>
        </component>

        <component componentID="3">
         <name>errors</name>
         <synopsis>The total number of packets with errors</synopsis>
         <typeRef>uint32</typeRef>
        </component>
    </struct>

   </dataTypeDef>

     <dataTypeDef>
        <name>IFEInfo</name>
        <synopsis>Describing IFE table row Information</synopsis>
        <struct>
           <component componentID="1">
             <name>IFETYPE</name>
             <synopsis>
                 The ethertype to be used for outgoing IFE frame
             </synopsis>
             <optional/>
             <typeRef>uint16</typeRef>
           </component>
           <component componentID="2">
             <name>StatId</name>
             <synopsis>
                 The Index into the stats table
             </synopsis>
             <optional/>
             <typeRef>uint32</typeRef>
           </component>
           <component componentID="3">
             <name>DSTFE</name>
             <synopsis>
                     The destination MAC address of the destination FE
             </synopsis>
             <typeRef>byte[6]</typeRef>
           </component>
           <component componentID="4">
             <name>SRCFE</name>
             <synopsis>
                     The source MAC address used for the source FE
             </synopsis>
             <typeRef>byte[6]</typeRef>
           </component>
           <component componentID="5">
             <name>MetaFilterList</name>
             <synopsis>
                     The allowed metadata filter table
             </synopsis>
             <optional/>
             <array type="variable-size">
               <typeRef>uint32</typeRef>
             </array>
            </component>


        </struct>
     </dataTypeDef>

  </dataTypeDefs>

  <LFBClassDefs>
    <LFBClassDef LFBClassID="18">
      <name>IFE</name>
      <synopsis>
         This LFB describes IFE connectivity parameterization
      </synopsis>
      <version>1.0</version>

        <inputPorts>

          <inputPort group="true">
           <name>EgressInGroup</name>
           <synopsis>
                   The input port group of the egress side.
                   It expects any type of Ethernet frame.
           </synopsis>
           <expectation>
                <frameExpected>
                <ref>PacketAny</ref>
                </frameExpected>
           </expectation>
          </inputPort>

          <inputPort  group="true">
           <name>IngressInGroup</name>
           <synopsis>
                   The input port group of the ingress side.
                   It expects an interFE-encapsulated Ethernet frame.
            </synopsis>
           <expectation>
                <frameExpected>
                <ref>InterFEFrame</ref>
                </frameExpected>
           </expectation>
        </inputPort>

       </inputPorts>

       <outputPorts>

         <outputPort>
           <name>OUT1</name>
           <synopsis>
                The output port of the egress side
           </synopsis>
           <product>
              <frameProduced>
                <ref>InterFEFrame</ref>
              </frameProduced>
           </product>
        </outputPort>

        <outputPort>
          <name>OUT2</name>
          <synopsis>
              The output port of the Ingress side
          </synopsis>
          <product>
             <frameProduced>
               <ref>PacketAny</ref>
             </frameProduced>
          </product>
       </outputPort>

       <outputPort>
         <name>EXCEPTIONOUT</name>
         <synopsis>
            The exception handling path
         </synopsis>
         <product>
            <frameProduced>
              <ref>PacketAny</ref>
            </frameProduced>
            <metadataProduced>
              <ref>ExceptionID</ref>
            </metadataProduced>
         </product>
      </outputPort>

   </outputPorts>

   <components>

      <component componentID="1" access="read-write">
         <name>IFETable</name>
         <synopsis>
            The table of all inter-FE relations
         </synopsis>
         <array type="variable-size">
            <typeRef>IFEInfo</typeRef>
         </array>
      </component>

     <component componentID="2" access="read-only">
       <name>IFEStats</name>
       <synopsis>
        The stats corresponding to the IFETable table
       </synopsis>
       <typeRef>bstats</typeRef>
     </component>

  </components>

 </LFBClassDef>
</LFBClassDefs>

</LFBLibrary>
]]></artwork></figure>
   </t>

    </section>

  </section>


    <section anchor="IANA" title="IANA Considerations">
    <t>
    IANA has registered the following LFB class name in the the "Logical
    Functional Block (LFB) Class Names and Class Identifiers" subregistry of
    the "Forwarding and Control Element Separation (ForCES)" registry &lt;https://www.iana.org/assignments/forces&gt;.  
    </t>

<texttable title="Logical Functional Block (LFB) Class Names and Class Identifiers">

<ttcol align="center">LFB Class Identifier</ttcol>

<ttcol align="center">LFB Class Name</ttcol>

<ttcol align="center">LFB Version</ttcol>

<ttcol align="center">Description</ttcol>

<ttcol align="center">Reference</ttcol>

<c>18</c>

<c>IFE</c>

<c>1.0</c>

<c>
An IFE LFB to standardize inter-FE LFB for ForCES Network Elements
</c>

<c>This document</c>
</texttable>

</section>

    <section anchor="IEEE" title="IEEE Assignment Considerations">

<!-- [rfced] update TBA with value assigned by IEEE -->

    <t>
    This memo includes a request for a new Ethernet protocol type
    as described in <xref target="etherencap"/>.
    </t>
    </section>

    <section anchor="Security" title="Security Considerations">

   <t>
    The FEs involved in the inter-FE LFB belong to the same NE
    and are within the scope of a single administrative Ethernet
    LAN private network. While trust of policy in the control and its
    treatment in the datapath exists already, an inter-FE LFB implementation
    SHOULD support security services provided by Media Access Control
    Security (MACsec) <xref target="ieee8021ae"></xref>. MACsec is not 
    currently sufficiently widely deployed in traditional packet processing
    hardware although it is present in newer versions of the Linux kernel (which
    will be widely deployed) <xref target="linux-macsec"></xref>.
    Over time, we expect that most FEs will be able to support MACsec. 
  </t>

    <t>
    MACsec provides security services such as a message authentication
    service and an optional confidentiality service. The services can 
    be configured manually or automatically using the MACsec Key Agreement (MKA)
    over the IEEE 802.1x <xref target="ieee8021x"></xref> Extensible
    Authentication Protocol (EAP) framework.  It is expected
    that FE implementations are going to start with shared keys configured
    from the control plane
    but progress to automated key management.
  </t>
  
    <t>
   The following are the MACsec security mechanisms that need to be in
   place for the inter-FE LFB:

<list style="symbols">
    <t>
     Security mechanisms are NE-wide for all FEs. Once the
      security is turned on, depending upon the chosen security level
      (e.g., Authentication, Confidentiality), it will be in effect for the
      inter-FE LFB for the entire duration of the session.
    </t>

   <t>
     An operator SHOULD configure the same security policies for all
      participating FEs in the NE cluster.  This will ensure
      uniform operations and avoid unnecessary complexity in policy
      configuration. In other words, the Security Association Keys (SAKs)
      should be pre-shared. When using MKA, FEs must identify themselves
      with a shared Connectivity Association Key (CAK) and Connectivity
      Association Key Name (CKN). EAP-TLS SHOULD be used as the EAP
      method.
   </t>

    <t>
     An operator SHOULD configure the strict validation mode, i.e., 
      all non-protected, invalid, or non-verifiable frames MUST be
      dropped.
   </t>
</list>
  </t>

 <t>
  It should be noted that given the above choices, if an FE
  is compromised, an entity running on the FE would be able to
  fake inter-FE or modify its content, causing bad outcomes.

  </t>
</section>

  </middle>


  <back>

    <references title="Normative References">
      &RFC2119;
      &RFC5810;
      &RFC5811;
      &RFC5812;
      &RFC7391;
      &RFC7408;


<reference anchor="ieee8021ae" target="http://ieeexplore.ieee.org/document/1678345/">
<front>
  <title>IEEE Standard for Local and metropolitan area networks
        Media Access Control (MAC) Security
  </title>
  <author><organization>IEEE</organization></author>
  <date />
</front>
<seriesInfo name="IEEE" value="802.1AE-2006" />
<seriesInfo name="DOI" value="10.1109/IEEESTD.2006.245590" />
</reference>

<reference anchor="ieee8021x" target="http://ieeexplore.ieee.org/document/5409813/">
<front>
  <title>IEEE Standard for Local and metropolitan
area networks - Port-Based Network Access Control.
  </title>
  <author><organization>IEEE</organization></author>
  <date />
</front>
<seriesInfo name="IEEE" value="802.1X-2010" />
<seriesInfo name="DOI" value="10.1109/IEEESTD.2010.5409813" />

 </reference>

</references>

    <references title="Informative References">
    &RFC6956;
    &RFC3746;

<!-- draft-ietf-tsvwg-rfc5405bis-18 IESG state: IESG Evaluation -->
<reference anchor='UDP-GUIDE'>
<front>
<title>UDP Usage Guidelines</title>

<author initials='L' surname='Eggert' fullname='Lars Eggert'>
    <organization />
</author>

<author initials='G' surname='Fairhurst' fullname='Gorry Fairhurst'>
    <organization />
</author>

<author initials='G' surname='Shepherd' fullname='Greg Shepherd'>
    <organization />
</author>

<date month='October' year='2016' />

</front>

<seriesInfo name='Work in Progress,' value='draft-ietf-tsvwg-rfc5405bis-19' />
<format type='TXT'
        target='http://www.ietf.org/internet-drafts/draft-ietf-tsvwg-rfc5405bis-18.txt' />
</reference>

    <reference anchor="brcm-higig" target="http://www.broadcom.com/products/ethernet-communication-and-switching/switching/bcm56720">
        <front>
        <title>HiGig</title>
        <author><organization>Broadcom</organization></author>
        <date/>
        </front>
    </reference>

<!-- draft-fairhurst-tsvwg-circuit-breaker-15 IANA Review state: Version
Changed - Review Needed -->

<reference anchor="circuit-b">
<front>
<title>Network Transport Circuit Breakers</title>

<author initials='G' surname='Fairhurst' fullname='Gorry Fairhurst'>
    <organization />
</author>

<date month='April' day='4' year='2016' />

</front>

<seriesInfo name='Work in Progress,' value='draft-ietf-tsvwg-circuit-breaker-15' />
<format type='TXT'
        target='http://www.ietf.org/internet-drafts/draft-ietf-tsvwg-circuit-breaker-15.txt' />
</reference>

<reference anchor="linux-tc" >
<front>
        <title>Linux Traffic Control Classifier-Action Subsystem Architecture
        </title>
<author fullname="Jamal Hadi Salim" initials="J." surname="Hadi Salim">
        <organization />
  </author>
  <date month="Feb" year="2015" />
</front>
<seriesInfo name="Netdev" value="01" />
</reference>

<reference anchor="tc-ife">
<front>
        <title>Distributing Linux Traffic Control Classifier-Action Subsystem
        </title>
<author fullname="Jamal Hadi Salim" initials="J." surname="Hadi Salim">
  </author>
        <author fullname="Damascane M. Joachimpillai" initials="D.J." surname="Joachimpillai">
  </author>
  <date month="Feb" year="2015" />
</front>
<seriesInfo name="Netdev" value="01" />
</reference>

<reference anchor="linux-macsec" >
<front>
        <title>MACsec: Encryption for the wired LAN </title>
<author fullname="Sabrina Dubroca" initials="S." surname="Dubroca">
  </author>
  <date month="Feb" year="2016" />
</front>
<seriesInfo name="Netdev" value="11" />
</reference>

    &RFC2460;

<reference  anchor='RFC791' target='http://www.rfc-editor.org/info/rfc791'>
<front>
<title>Internet Protocol</title>
<author initials='J.' surname='Postel' fullname='J. Postel'><organization /></author>
<date year='1981' month='September' />
</front>
<seriesInfo name='STD' value='5'/>
<seriesInfo name='RFC' value='791'/>
<seriesInfo name='DOI' value='10.17487/RFC0791'/>
</reference>




    </references>

  <section anchor="Acknowledgements" title="Acknowledgements" numbered="no">
          <t>
          The authors would like to thank Joel Halpern and Dave Hood for
          the stimulating discussions. Evangelos Haleplidis shepherded
          and contributed to improving this document. Alia Atlas was the 
          AD sponsor of this document and did a tremendous job of critiquing
          it. The authors are grateful to Joel Halpern and Sue Hares in
          their roles
          as the Routing Area reviewers for shaping the content of this document.
          David Black put in a lot of effort to make sure the congestion-control
          considerations are sane. Russ Housley did the Gen-ART review, 
          Joe Touch did the TSV area review, and Shucheng LIU (Will) did the OPS review.
          Suresh Krishnan helped us provide clarity during the IESG review.
          The authors are appreciative of the efforts Stephen Farrell put
          in to fixing the security section.
          </t>
  </section>

  </back>
</rfc>
