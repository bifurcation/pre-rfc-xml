<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt"?>

<?rfc toc="yes"?>
<?rfc tocompact="yes"?>
<?rfc tocdepth="3"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes"?>
<?rfc comments="yes"?>
<?rfc inline="yes"?>
<?rfc compact="yes"?>
<?rfc subcompact="no"?>

<rfc category="info" submissionType="independent" number="8604" ipr="trust200902">

  <front>
    <title abbrev="Large-Scale Segment Routing">Interconnecting Millions of
 Endpoints with Segment Routing</title>

    <author fullname="Clarence Filsfils" initials="C." role="editor"
            surname="Filsfils">
      <organization>Cisco Systems, Inc.</organization>
      <address>
        <postal>
          <street/>
          <city>Brussels</city>
          <region/>
          <code/>
          <country>Belgium</country>
        </postal>
        <email>cfilsfil@cisco.com</email>
      </address>
    </author>

    <author fullname="Stefano Previdi" initials="S." surname="Previdi">
      <organization>Huawei Technologies</organization>
      <address>
        <postal>
          <street></street>
          <city></city>
          <code></code>
          <country></country>
        </postal>
        <email>stefano@previdi.net</email>
      </address>
    </author>

    <author fullname="Gaurav Dawra" initials="G." role="editor"
            surname="Dawra">
      <organization>LinkedIn</organization>
      <address>
        <postal>
          <street/>
          <city/>
          <code/>
          <country>United States of America</country>
        </postal>
        <email>gdawra.ietf@gmail.com</email>
      </address>
    </author>

    <author fullname="Wim Henderickx" initials="W." surname="Henderickx">
      <organization>Nokia</organization>
      <address>
        <postal>
          <street>Copernicuslaan 50</street>
          <city>Antwerp</city>
          <code>2018</code>
          <country>Belgium</country>
        </postal>
        <email>wim.henderickx@nokia.com</email>
      </address>
    </author>

    <author fullname="Dave Cooper" initials="D." surname="Cooper">
      <organization>CenturyLink</organization>
      <address>
        <email>Dave.Cooper@centurylink.com</email>
      </address>
    </author>

    <date month="June" year="2019"/>

    <abstract>
      <t>This document describes an application of Segment Routing to scale
      the network to support hundreds of thousands of network nodes, and tens
      of millions of physical underlay endpoints. This use case can be applied
      to the interconnection of massive-scale Data Centers (DCs) and/or large
      aggregation networks. Forwarding tables of midpoint and leaf nodes only
      require a few tens of thousands of entries. This may be achieved by the
      inherently scaleable nature of Segment Routing and the design proposed
      in this document.</t>
    </abstract>
  </front>

  <middle>
    <section anchor="INTRO" title="Introduction">
      <t>This document describes how Segment Routing (SR) can be used to
      interconnect millions of endpoints.</t>
    </section>

    <section anchor="TERMINOLOGY" title="Terminology">
      <t>The following terms and abbreviations are used in this
      document:

       <figure suppress-title="true">
          <artwork>   Term          Definition
   ---------------------------------------------------------
   Agg           Aggregation
   BGP           Border Gateway Protocol
   DC            Data Center
   DCI           Data Center Interconnect
   ECMP          Equal-Cost Multipath
   FIB           Forwarding Information Base
   LDP           Label Distribution Protocol
   LFIB          Label Forwarding Information Base
   MPLS          Multiprotocol Label Switching
   PCE           Path Computation Element
   PCEP          Path Computation Element Communication Protocol
   PW            Pseudowire
   SLA           Service Level Agreement
   SR            Segment Routing
   SRTE Policy   Segment Routing Traffic Engineering Policy
   TE            Traffic Engineering
   TI-LFA        Topology Independent Loop-Free Alternate</artwork>
        </figure></t>
    </section>

    <section anchor="REFDESIGN" title="Reference Design">
      <t>The network diagram below illustrates the reference network
      topology used in this document:

        <figure anchor="REFTOPO" title="Reference Topology">
       <artwork>        +-------+ +--------+ +--------+ +-------+ +-------+
        A       DCI1       Agg1       Agg3      DCI3      Z
        |  DC1  | |   M1   | |   C    | |   M2  | |  DC2  |
        |       DCI2       Agg2       Agg4      DCI4      |
        +-------+ +--------+ +--------+ +-------+ +-------+</artwork>
        </figure></t>

      <t>The following apply to the reference topology above:
         <list style="symbols">
          <t>Independent ISIS-OSPF/SR instance in core (C) region.</t>

          <t>Independent ISIS-OSPF/SR instance in Metro1 (M1) region.</t>

          <t>Independent ISIS-OSPF/SR instance in Metro2 (M2) region.</t>

          <t>BGP/SR in DC1.</t>

          <t>BGP/SR in DC2.</t>

          <t>Agg routes (Agg1, Agg2, Agg3, Agg4) are redistributed from C to M
          (M1 and M2) and from M to DC domains.</t>

          <t>No other route is advertised or redistributed between
          regions.</t>

          <t>The same homogeneous Segment Routing Global Block (SRGB) is used
          throughout the domains (e.g., 16000-23999).</t>

          <t>Unique SRGB sub-ranges are allocated to each metro (M) and core
          (C) domain:
            <list style="symbols">
             <t>The 16000-16999 range is allocated to the core (C)
             domain&wj;/region.</t>

             <t>The 17000-17999 range is allocated to the M1 domain/region.</t>

             <t>The 18000-18999 range is allocated to the M2 domain/region.</t>

             <t>Specifically, the Agg1 router has Segment Identifier (SID)
             16001 allocated, and the Agg2 router has SID 16002 allocated.</t>

              <t>Specifically, the Agg3 router has SID 16003 allocated, and the
              anycast SID for Agg3 and Agg4 is 16006.</t>

              <t>Specifically, the DCI3 router has SID 18003 allocated, and the
              anycast SID for DCI3 and DCI4 is 18006.</t>

              <t>Specifically, at the Agg1 router, the binding SID 4001 leads
              to DCI pair (DCI3, DCI4) via a specific low-latency path {16002,
              16003, 18006}.</t>
            </list></t>

          <t>The same SRGB sub-range is reused within each DC (DC1 and DC2)
          region for each DC (e.g., 20000-23999). Specifically, nodes A
          and&nbsp;Z both have SID 20001 allocated to them.</t>
        </list></t>
    </section>

    <section anchor="CONTROLPLANE" title="Control Plane">
      <t>This section provides a high-level description of how a control
      plane could be implemented using protocol components already defined in
      other RFCs.</t>

      <t>The mechanism through which SRTE Policies are defined, computed, and
      programmed in the source nodes is outside the scope of this
      document.</t>

      <t>Typically, a controller or a service orchestration system programs
      node A with a PW to a remote next-hop node Z with a given
      SLA contract (e.g., low-latency path, disjointness from a specific core
      plane, disjointness from a different PW service).</t>

      <t>Node A automatically detects that node Z is not reachable.
      It then automatically sends a PCEP request to an SR PCE for an SRTE
      policy that provides reachability information for node Z with the
      requested&nbsp;SLA.</t>

      <t>The SR PCE <xref target="RFC4655"/> is made of two components: a
      multi-domain topology and a computation engine. The multi-domain
      topology is continuously refreshed through BGP - Link State (BGP-LS)
      feeds <xref target="RFC7752"/> from each domain. The computation engine
      is designed to implement TE algorithms and provide output in
      SR Path format. Upon receiving the PCEP request <xref
      target="RFC5440"/>, the SR PCE computes the requested path. The path
      is expressed through a list of segments (e.g., {16003, 18006, 20001})
      and provided to node A.</t>

      <t>The SR PCE logs the request as a stateful query and hence is
      able to recompute the path at each network topology change.</t>

      <t>Node A receives the PCEP reply with the path (expressed as a segment
      list). Node A installs the received SRTE policy in the data plane. Node A
      then automatically steers the PW into that SRTE policy.</t>
    </section>

    <section anchor="ILLUSTRATION" title="Illustration of the Scale">
      <t>According to the reference topology shown in <xref
      target="REFTOPO"/>, the following assumptions are made:
        <list style="symbols">
          <t>There is one core domain, and there are 100 leaf (metro)
          domains.</t>

          <t>The core domain includes 200 nodes.</t>

          <t>Two nodes connect each leaf (metro) domain. Each node connecting
          a leaf domain has a SID allocated. Each pair of nodes connecting a
          leaf domain also has a common anycast SID. This yields up to
          300 prefix segments in total.</t>

          <t>A core node connects only one leaf domain.</t>

          <t>Each leaf domain has 6,000 leaf-node segments. Each leaf node has
          500 endpoints attached and thus 500 adjacency segments.
          This yields a total of 3&nbsp;million endpoints for a leaf
          domain.</t>
        </list></t>

      <t>Based on the above, the network scaling numbers are as follows:
        <list style="symbols">
          <t>6,000 leaf-node segments multiplied by 100 leaf domains:
          600,000&nbsp;nodes.</t>

          <t>600,000 nodes multiplied by 500 endpoints: 300 million
           endpoints.</t>
        </list></t>

      <t>The node scaling numbers are as follows:
        <list style="symbols">
          <t>Leaf-node segment scale: 6,000 leaf-node segments + 300 core-node
          segments + 500 adjacency segments = 6,800 segments.</t>

          <t>Core-node segment scale: 6,000 leaf-domain segments +
          300&nbsp;core&nbhy;domain segments = 6,300 segments.</t>
        </list></t>

      <t>In the above calculations, the link-adjacency segments are not taken
      into account. These are local segments and, typically, less than 100 per
      node.</t>

      <t>It has to be noted that, depending on leaf-node FIB capabilities,
      leaf domains could be split into multiple smaller domains. In the above
      example, the leaf domains could be split into six smaller domains so that
      each leaf node only needs to learn 1,000 leaf-node segments + 300
      core&nbhy;node segments + 500 adjacency segments, yielding a total of
      1,800 segments.</t>

      <t/>
    </section>

    <section anchor="OPTIONS" title="Design Options">
      <t>This section describes multiple design options to illustrate scale
      as described in the previous section.</t>

      <section anchor="SRGBSIZE"
               title="Segment Routing Global Block (SRGB) Size">
        <t>In the simplified illustrations in this document, we picked a small
        homogeneous SRGB range of 16000-23999. In practice, a large-scale
        design would use a bigger range, such as 16000-80000 or even larger.
        A larger range provides allocations for various TE applications
        within a given domain.</t>
      </section>

      <section anchor="REDISTAGG" title="Redistribution of Routes for Agg Nodes">
        <t>The operator might choose to not redistribute the routes for Agg
        nodes into the Metro/DC domains. In that case, more segments are
        required in order to express an inter-domain path.</t>

        <t>For example, node A would use an SRTE Policy {DCI1, Agg1, Agg3,
        DCI3,&nbsp;Z} in order to reach Z instead of {Agg3, DCI3, Z} in the
        reference design.</t>
      </section>

      <section anchor="SIZING" title="Sizing and Hierarchy">
        <t>The operator is free to choose among a small number of larger leaf
        domains, a large number of small leaf domains, or a mix of small and
        large core/leaf domains.</t>

        <t>The operator is free to use a two-tier (Core/Metro) or 
        three-tier (Core/Metro/DC) design.</t>
      </section>

      <section anchor="LOCALSEGMENTS" title="Local Segments to Hosts/Servers">
        <t>Local segments can be programmed at any leaf node (e.g., node Z) in
        order to identify locally attached hosts (or Virtual Machines (VMs)).
        For example, if
        node Z has bound a local segment 40001 to a local host ZH1, then node
        A uses the following SRTE Policy in order to reach that host: {16006,
        18006, 20001, 40001}. &nbsp;Such a local segment could represent the
        NID (Network Interface Device) in the context of the service provider
        access network, or a VM in the context of the DC network.</t>
      </section>

      <section anchor="COMPRESSSRTE" title="Compressed SRTE Policies">
        <t>As an example and according to <xref target="REFDESIGN"/>, we
        assume that node A can reach node Z (e.g., with a low-latency SLA
        contract) via the SRTE policy that consists of the path
        Agg1, Agg2, Agg3, DCI3/4(anycast), Z. The path is represented by the
        segment list&nbsp;{16001, 16002, 16003, 18006, 20001}.</t>

        <t>It is clear that the control-plane solution can install an SRTE
        Policy {16002, 16003, 18006} at Agg1, collect the binding SID
        allocated by Agg1 to that policy (e.g., 4001), and hence program
        node&nbsp;A with the compressed SRTE Policy {16001, 4001, 20001}.</t>

        <t>From node A, 16001 leads to Agg1. Once at Agg1, 4001 leads to the
        DCI pair (DCI3, DCI4) via a specific low-latency path {16002, 16003,
        18006}. &nbsp;Once at that DCI pair, 20001 leads to Z.</t>

        <t>Binding SIDs allocated to "intermediate" SRTE Policies achieve
        the compression of end-to-end SRTE Policies.</t>

        <t>The segment list {16001, 4001, 20001} expresses the same path as
        {16001, 16002, 16003, 18006, 20001} but with two less segments.</t>

        <t>The binding SID also provides for inherent churn protection.</t>

        <t>When the core topology changes, the control plane can update the
        low&nbhy;latency SRTE Policy from Agg1 to the DCI pair to DC2 without
        updating the SRTE Policy from A to Z.</t>
      </section>
    </section>

    <section anchor="DEPLOYMENT" title="Deployment Model">
      <t>It is expected that this design will be used in "green field"
      deployments as well as interworking ("brown field") deployments
      with an MPLS design across multiple domains.</t>
    </section>

    <section anchor="BENEFITS" title="Benefits">
      <t>The design options illustrated in this document allow 
      interconnections on a very large scale. Millions of endpoints across
      different domains can be interconnected.</t>

      <section title="Simplified Operations">
        <t>Two control-plane protocols not needed in this design are LDP
        and RSVP-TE. No new protocol has been introduced. The design leverages
        the core IP protocols ISIS, OSPF, BGP, and PCEP with straightforward
        SR extensions.</t>
      </section>

      <section title="Inter-domain SLAs">
        <t>Fast reroute and resiliency are provided by TI-LFA with
        sub&nbhy;50&nbhy;ms fast reroute upon failure of a link, node, or
        Shared Risk Link Group (SRLG). TI-LFA is described
        in <xref target="SR-TI-LFA"/>.</t>

        <t>The use of anycast SIDs also provides improved availability and
        resiliency.</t>

        <t>Inter-domain SLAs can be delivered (e.g., latency
        vs.&nbsp;cost-optimized paths, disjointness from backbone planes,
        disjointness from other services, disjointness between primary and
        backup paths).</t>

        <t>Existing inter-domain solutions do not provide any support for SLA
        contracts. They just provide best-effort reachability across
        domains.</t>
      </section>

      <section title="Scale">
        <t>In addition to having eliminated the need for LDP and RSVP-TE,
        per&nbhy;service midpoint states have also been removed from the
        network.</t>
      </section>

      <section title="ECMP">
        <t>Each policy (intra-domain or inter-domain, with or without TE) is
        expressed as a list of segments. Since each segment is optimized for
        ECMP, the entire policy is optimized for ECMP. The benefit of an
        anycast prefix segment optimized for ECMP should also be considered
        (e.g., 16001 load&nbhy;shares across any gateway from the M1 leaf
        domain to the Core and 16002 load&nbhy;shares across any gateway from
        the Core to the M1 leaf domain).</t>
      </section>
    </section>

    <section anchor="IANA" title="IANA Considerations">
     <t>This document has no IANA actions.</t>
    </section>

    <section anchor="Manageability" title="Manageability Considerations">
      <t>This document describes an application of SR over the
      MPLS data plane. SR does not introduce any changes in the
      MPLS data plane. The manageability considerations described in <xref
      target="RFC8402"/> apply to the MPLS data plane when used with SR.</t>
    </section>

    <section anchor="Security" title="Security Considerations">
      <t>This document does not introduce additional security requirements and
      mechanisms other than those described in <xref target="RFC8402"/>.</t>
    </section>

  </middle>

  <back>
    <references title="Informative References">

<!-- draft-bashandy-rtgwg-segment-routing-ti-lfa
     (Replaced by draft-ietf-rtgwg-segment-routing-ti-lfa (I-D Exists)) -->
<reference anchor='SR-TI-LFA'>
<front>
<title>Topology Independent Fast Reroute using Segment Routing</title>
<author initials='S' surname='Litkowski' fullname='Stephane Litkowski'>
    <organization />
</author>
<author initials='A' surname='Bashandy' fullname='Ahmed Bashandy'>
    <organization />
</author>
<author initials='C' surname='Filsfils' fullname='Clarence Filsfils'>
    <organization />
</author>
<author initials='B' surname='Decraene' fullname='Bruno Decraene'>
    <organization />
</author>
<author initials='P' surname='Francois' fullname='Pierre Francois'>
    <organization />
</author>
<author initials='D' surname='Voyer' fullname='Daniel Voyer'>
    <organization />
</author>
<author initials='F' surname='Clad' fullname='Francois Clad'>
    <organization />
</author>
<author initials='P' surname='Camarillo' fullname='Pablo Camarillo'>
    <organization />
</author>
<date month='March' year='2019' />
</front>
<seriesInfo name='Work in Progress,' value='draft-ietf-rtgwg-segment-routing-ti-lfa-01' />
</reference>

      <?rfc include='reference.RFC.8402.xml'?>

      <?rfc include='reference.RFC.4655.xml'?>

      <?rfc include='reference.RFC.5440.xml'?>

      <?rfc include='reference.RFC.7752.xml'?>
    </references>

    <section anchor="Acknowledgements" title="Acknowledgements" numbered="no">
      <t>We would like to thank Giles Heron, Alexander Preusche, Steve Braaten,
      and Francis Ferguson for their contributions to the content of this
      document.</t>
    </section>

    <section anchor="Contributors" title="Contributors" numbered="no">
      <t>The following people substantially contributed to the editing of
      this document:

   <figure><artwork>
Dennis Cai
Individual

Tim Laberge
Individual

Steven Lin
Google Inc.

Bruno Decraene
Orange

Luay Jalil
Verizon

Jeff Tantsura
Individual

Rob Shakir
Google Inc.
    </artwork></figure>
   </t>
  </section>
 </back>
</rfc>
