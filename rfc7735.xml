<?xml version="1.0" encoding="US-ASCII"?>

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
  <!ENTITY rfc6385 PUBLIC '' 'http://xml.resource.org/public/rfc/bibxml/reference.RFC.6385.xml'>
]>

<?rfc toc="yes" ?>
<?rfc compact="yes" ?>
<?rfc subcompact="no" ?>
<?rfc sortrefs="yes" ?>
<?rfc strict="yes" ?>
<?rfc inline="yes" ?>
<?rfc symrefs="yes" ?>

<rfc number="7735"
     ipr="trust200902"  
     category="info" 
     submissionType="IETF"
     consensus="yes">


<front>

<title abbrev="Review Tracking">
Tracking Reviews of Documents
</title>

<author initials="R." surname="Sparks" fullname="Robert Sparks" >
 <organization>Oracle</organization>
 <address>
 <postal>
 <street>7460 Warren Parkway</street>
 <street>Suite 300</street>
 <city>Frisco</city>
 <region>Texas</region>
 <code>75034</code>
 <country>United States</country>
 </postal>
 <email>rjsparks@nostrum.com</email>
 </address>
</author>
<author initials='T.' surname='Kivinen' fullname='Tero Kivinen'>
  <organization>INSIDE Secure</organization>
  <address>
    <postal>
      <street>Eerikinkatu 28</street>
      <code>FI-00180</code>
      <city>Helsinki</city>
      <country>Finland</country>
    </postal>
    <email>kivinen@iki.fi</email>
  </address>
</author>

<date month="January" year="2016" />

<!-- [rfced] Please insert any keywords (beyond those that appear in the
title) for use on http://www.rfc-editor.org/search -->
<keyword>example</keyword>

<abstract>
<t>
Several review teams ensure specific types of review are performed on Internet-Drafts as they progress towards becoming RFCs. The tools used by these teams to assign and track reviews would benefit from tighter integration to the Datatracker. This document discusses requirements for improving those tools without disrupting current work flows.
</t>
</abstract>

</front>

<middle>

<section anchor="intro" title="Introduction">

<!-- [rfced] Throughout the text, the following terminology appears to be 
used inconsistently. Please review these occurrences and let us know if/how
they may be made consistent.

Last Call vs. last call (we recommend the former)
Telechat vs. telechat
-->


<t>
As Internet-Drafts are processed, reviews are requested from several review
teams. For example, the General Area Review Team (Gen-ART) and the Security
Directorate (SecDir) perform reviews of documents that are in IETF Last
Call. Gen-ART always performs a follow-up review when the document is
scheduled for an IESG Telechat. SecDir usually performs a follow-up review,
but the SecDir secretary may choose not to request that follow-up if any issues identified at Last Call are addressed and there are otherwise no major changes to the document. These teams also perform earlier reviews of documents on demand. There are several other teams that perform similar services, often focusing on specific areas of expertise.
</t>
<t>
The secretaries of these teams manage a pool of volunteer reviewers. Documents are assigned to reviewers, taking various factors into account. For instance, a reviewer will not be assigned a document for which he is an author or shepherd. 
Reviewers are given a deadline, usually driven by the end of Last Call or an
IESG Telechat date. The reviewer sends each completed review to the team's
mailing list and to any other lists that are relevant for the document being reviewed. 
Often, a thread ensues on one or more of those lists to resolve any issues found in the review.
</t>
<t>
The secretaries and reviewers from several teams are using a tool developed
and maintained by Tero Kivinen. Much of its design predates the modern
Datatracker. The application currently keeps its own data store and learns
about documents needing review by inspecting Datatracker and tools.ietf.org
pages. Most of those pages are easy to parse, but the Last Call pages, in
particular, require some effort. Tighter integration with the Datatracker
would simplify the logic used to identify documents that are ready for review,
make it simpler for the Datatracker to associate reviews with documents, and
allow users to reuse their Datatracker credentials. It would also make it
easier to detect other potential review-triggering events, such as a document
entering Working Group (WG) Last Call or when an RFC's standard level is being changed without revising the RFC. Tero currently believes this integration is best achieved by a new implementation of the tool. This document captures requirements for that reimplementation, with a focus on the workflows that the new implementation must take care not to disrupt. It also discusses new features, including changes suggested for the existing tool at its issue tracker <xref target="art-trac"/>.
</t>

<t>For more information about the various review teams, see the following references.</t>

<!-- [rfced] In this table, we suggest putting the full names so that
readers don't have to go to the references section if they are not
familiar with the short names. Please let us know if you agree. 
(We note the OPS-dir page uses the name "Operational Directorate".)

Current:
                  | Gen-ART      | [Gen-ART] [RFC6385] |
                  | SecDir       | [SecDir]            |
                  | AppsDir      | [AppsDir]           |
                  | OPS-dir      | [OPS-dir]           |
                  | RTG-dir      | [RTG-dir]           |
                  | MIB Doctors  | [MIBdoctors]        |
                  | YANG Doctors | [YANGdoctors]       |

Perhaps:
          | General Area Review Team      | [Gen-ART] [RFC6385] |
          | Security Directorate          | [SecDir]            |
          | Applications Area Directorate | [AppsDir]           |
          | Operations Area Directorate   | [OPS-dir]           |
          | Routing Area Directorate      | [RTG-dir]           |
          | MIB Doctors                   | [MIBdoctors]        |
          | YANG Doctors                  | [YANGdoctors]       |
-->
<texttable>
<ttcol/><ttcol/>
<c>General Area Review Team</c><c><xref target="Gen-ART"/> <xref target="RFC6385"/></c>
<c>Security Directorate</c><c><xref target="SecDir"/></c>
<c>Applications Area Directorate</c><c><xref target="AppsDir"/></c>
<c>Operations Area Directorate</c><c><xref target="OPS-dir"/></c>
<c>Routing Area Directorate</c><c><xref target="RTG-dir"/></c>
<c>MIB Doctors</c><c><xref target="MIBdoctors"/></c>
<c>YANG Doctors</c><c><xref target="YANGdoctors"/></c>
</texttable>
</section>

<section title="Overview of Current Workflows">

<t> This section gives a high-level overview of how the review team secretaries and reviewers use the existing tool. It is not intended to be comprehensive documentation of how review teams operate. Please see the references for those details.</t>

<t>
For many teams, the team's secretary periodically (typically once a week)
checks the tool for documents it has identified as ready for review. The tool
compiles a list from Last Call announcements and IESG Telechat agendas. The
secretary creates a set of assignments from this list and enters them into the
reviewer pool, choosing the reviewers in roughly a round-robin order. That
order can be perturbed by several factors. Reviewers have different levels of
availability. Some are willing to review multiple documents a month. Others
may only be willing to review a document every other month. The assignment
process takes exceptional conditions such as reviewer vacations into
account. Furthermore, secretaries are careful not to assign a document to a
reviewer that is an author, shepherd, responsible WG chair, or has some other already existing association with the document. The preference is to get a reviewer with a fresh perspective. The secretary may discover reasons to change assignments while going through the list of documents. In order to not cause a reviewer to make a false start on a review, the secretaries complete the full list of assignments before sending notifications to anyone. This assignment process can take several minutes and it is possible for new Last Calls to be issued while the secretary is making assignments. The secretary typically checks to see if new documents are ready for review just before issuing the assignments and updates the assignments if necessary. 
</t>
<t>
Some teams operate in more of a review-on-demand model. The Routing
Area Directorate (RTG-dir), for example, primarily initiates reviews at the request
of a Routing AD. They may also
start an early review at the request of a WG chair. In either case, the reviewers are chosen manually from the pool of available reviewers driven by context rather than using a round-robin ordering.
</t>
<t>The issued assignments are either sent to the review team's email list or are emailed directly to the assigned reviewer. The assignments are reflected in the tool. For those teams handling different types of reviews (Last Call vs. Telechat, for example), the secretary typically processes the documents for each type of review separately, and potentially with different assignment criteria. In Gen-ART, for example, the Last Call reviewer for a document will almost always get the follow-up Telechat review assignment. Similarly, SecDir assigns any re-reviews of a document to the same reviewer. Other teams may choose to assign a different reviewer.
</t>
<t>
Reviewers discover their assignments through email or by looking at their
queue in the tool. The secretaries for some teams (such as the OPS-dir and RTG-dir) insulate their team members from using the tool directly. These reviewers only work through the review team's email list or through direct email. On teams that have the reviewers use the tool directly, most reviewers only check the tool when they see they have an assignment via the team's email list. A reviewer has the opportunity to reject the assignment for any reason. While the tool provides a way to reject assignments, reviewers typically use email to coordinate rejections with the team secretary. The secretary will find another volunteer for any rejected assignments. The reviewer can indicate that the assignment is accepted in the tool before starting the review, but this feature is rarely used.</t>
<t>
The reviewer sends a completed review to the team's email list or secretary,
as well as any other lists relevant to the review, and usually the draft's primary email alias. For instance, many Last Call reviews are also sent to the IETF general list. The teams typically have a template format for the review. Those templates usually start with a summary of the conclusion of the review. Typical summaries are "ready for publication" or "on the right track but has open issues". The reviewer (or in the case of teams that insulate their reviewers, the secretary) uses the tool to indicate that the review is complete, provides the summary, and has an opportunity to provide a link to the review in the archives. Note, however, that having to wait for the document to appear in the archive to know the link to paste into the tool is a significant enough impedance that this link is often not provided by the reviewer. The SecDir secretary manually collects these links from the team's email list and adds them to the tool.
</t>
<t>
Occasionally, a document is revised between when a review assignment is made and when the reviewer starts the review. Different teams can have different policies about whether the reviewer should review the assigned version or the current version.
</t>

</section>

<section title="Requirements">
<section title="Secretariat Focused">
<t><list style="hanging">
<t hangText="o">The Secretariat must be able to configure secretaries and reviewers for review teams (by managing Role records).</t>
<t hangText="o">The Secretariat must be able to perform any secretary action on behalf of a review team secretary (and thus, must be able to perform any reviewer action on behalf of the reviewer).</t>
</list></t>
</section>

<section title="Review-Team Secretary Focused">
<t><list style="hanging">
<t hangText="o">A secretary must be able to see what documents are ready for a
review of a given type (such as a Last Call review).</t>
<t hangText="o">A secretary must be able to assign reviews for documents that may not have been automatically identified as ready for a review of a given type. (In addition to being the primary assignment method for teams that only initiate reviews on demand, this allows the secretary to work around errors and handle special cases, including early review requests.)</t>
<t hangText="o">A secretary must be able to work on and issue a set of assignments as an atomic unit. No assignment should be issued until the secretary declares the set of assignments complete.</t>
<t hangText="o">The tool must support teams that have multiple secretaries. The tool should warn secretaries that are simultaneously working on assignments and protect against conflicting assignments being made.</t>
<t hangText="o">It must be easy for the secretary to discover that more
documents have become ready for review while working on an assignment set.</t>
<t hangText="o">The tool should make preparing the assignment email to the team's email list easy. For instance, the tool could prepare the message, give the secretary an opportunity to edit it, and handle sending it to the team's email list.</t>
<t hangText="o">It must be possible for a secretary to indicate that the
review team will not provide a review for a document (or a given version of a
document). This indication should be taken into account when presenting the
documents that are ready for a review of a given type. This will also make it possible to show on a document's page that no review is expected from this team.</t>
<t hangText="o">A secretary must be able to easily see who the next available reviewers are, in order.</t>
<t hangText="o">A secretary must be able to edit a reviewer's availability,
both in terms of frequency, not-available-until-date, and
skip-next-n-assignments. (See the description of these settings in <xref target="rev_focus" />.)</t>
<t hangText="o">The tool should make it easy for the secretary to see any team members that have requested to review a given document when it becomes available for review.</t>
<t hangText="o">The tool should make it easy for the secretary to identify that a reviewer is already involved with a document. The current tool allows the secretary to provide a regular expression to match against the document name. If the expression matches, the document is not available for assignment to this reviewer. For example, Tero will not be assigned documents matching '^draft-(kivinen|ietf-tcpinc)-.*$'. The tool should also take any roles, such as document shepherd, that the Datatracker knows about into consideration.</t>
<t hangText="o">The tool should make it easy for the secretary to see key features of a document ready for assignment, such as its length, its authors, the group and area it is associated with, its title and abstract, its states (such as IESG or WG states), and any other personnel (such as the shepherd and reviewers already assigned from other teams) involved in the draft. </t>
<t hangText="o">The tool must make it easy for the secretary to detect and process re-review requests on the same version of a document (such as when a document has an additional Last Call only to deal with new IPR information). </t>
<t hangText="o">Common operations to groups of documents should be easy for
the secretary to process as a group with a minimum amount of interaction with
the tool. For instance, it should be possible to process all of the documents described by the immediately preceding bullet with one action. Similarly, for teams that assign re-reviews to the same reviewer, issuing all re-review requests should be a simple action.</t>
<t hangText="o">A secretary must be able to see which reviewers have outstanding assignments.</t>
<t hangText="o">The tool must make it easy for the secretary to see the result of previous reviews from this team for a given document. In SecDir, for example, if the request is for a revision that has only minor differences and the previous review result was "Ready", a new assignment will not be made. If the given document replaces one or more other prior documents, the tool must make it easy for the secretary to see the results of previous reviews of the replaced documents.</t>
<t hangText="o">The tool must make it easy for the secretary to see the result
of previous reviews from this team for all documents across configurable,
recent periods of time (such as the last 12 months). A secretary of the RTG-dir, for example, would use this result to aid in the manual selection of the next reviewer.</t>
<t hangText="o">The tools must make it easy for the secretary to see the recent performance of a reviewer while making an assignment (see <xref target="statistics"/>). This allows the secretary to detect overburdened or unresponsive volunteers earlier in the process. </t>
<t hangText="o">A secretary must be able to configure the tool to remind them
to follow up when actions are due. (For instance, a secretary could receive email when a review is about to become overdue.)</t>
<t hangText="o">A secretary must be able to assign multiple reviewers to a given draft at any time. In particular, a secretary must be able to assign an additional reviewer when an original reviewer indicates their review is likely to be only partially complete.</t>
<t hangText="o">A secretary must be able to withdraw a review assignment.</t>
<t hangText="o">A secretary must be able to perform any reviewer action on behalf of the reviewer.</t>
<t hangText="o">A secretary must be able to configure the review team's set of reviewers (by managing Role records for the team).</t>
<t hangText="o">Information about a reviewer must not be lost when a reviewer is removed from a team. (Frequently, reviewers come back to teams later.)</t>
<t hangText="o">A secretary must be able to delegate secretary capabilities in the tool (similar to how a working group chair can assign a Delegate). This allows review teams to self-manage secretary vacations.</t>

</list></t>
</section>
<section title="Reviewer Focused" anchor="rev_focus">
<t><list style="hanging">
<t hangText="o">A reviewer must be able to indicate availability, both in frequency of reviews and as "not available until this date".
The current tool speaks of frequency in these terms:
<list style="hanging">
<t hangText="-">Assign at maximum one new review per week</t>
<t hangText="-">Assign at maximum one new review per fortnight</t>
<t hangText="-">Assign at maximum one new review per month</t>
<t hangText="-">Assign at maximum one new review per two months</t>
<t hangText="-">Assign at maximum one new review per quarter</t>
</list>
</t>
<t hangText="o">Reviewers must be able to indicate hiatus periods. Each period may be either "soft" or "hard".
<list style="hanging">
<t hangText="-">A hiatus must have a start date. It may have an end date or it
may be indefinite. </t>
<t hangText="-">During a hiatus, the reviewer will not be included in the normal review rotation. When a provided end date is reached, the reviewer will automatically be included in the rotation in their usual order.</t>
<t hangText="-">During a "soft" hiatus, the reviewer must not be assigned new reviews but is expected to complete existing assignments and do follow-up reviews.</t>
<t hangText="-">During a "hard" hiatus, the reviewer must not be assigned any new reviews and the secretary must be prompted to reassign any outstanding or follow-up reviews.</t>
</list>
</t>
<t hangText="o">Reviewers must be able to indicate that they should be skipped
the next "n" times they would normally have received an assignment.</t>


<t hangText="o">Reviewers must be able to indicate that they are transitioning
to inactive and provide a date for the end of the transition period. During this transition time, the reviewer must not be assigned new reviews but is expected to complete outstanding assignments and follow-up reviews. At the end of the transition period, the secretary must be prompted to reassign any outstanding or follow-up reviews. (This allows review-team members that are taking on AD responsibility to transition gracefully to an inactive state for the team.)</t>
<t hangText="o">Both the reviewer and the secretary will be notified by email of any modifications to a reviewer's availability.</t>
<t hangText="o">A reviewer must be able to easily discover new review assignments. (The tool might send email directly to an assigned reviewer in addition to sending the set of assignments to the team's email list. The tool might also use the Django Message framework to let a reviewer that's logged into the Datatracker know a new review assignment has been made.)</t>
<t hangText="o">Reviewers must be able to see their current set of outstanding
assignments, completed assignments, and rejected assignments. The presentation of those sets should either be separate or, if combined, the sets should be visually distinct.</t>
<t hangText="o">A reviewer should be able to request to review a particular document.  The draft may be in any state: available and unassigned; already assigned to another reviewer; or not yet available.</t>

<t hangText="o">A reviewer must be able to reject a review assignment, optionally providing the secretary with an explanation for the rejection. The tool will notify the secretary of the rejection by email.</t>
<t hangText="o">A reviewer must be able to indicate that they have accepted and are working on an assignment.</t>
<t hangText="o">A reviewer must be able to indicate that a review is only
partially completed and ask the secretary to assign an additional reviewer. The tool will notify the secretary of this condition by email.</t>
<t hangText="o">It should be possible for a reviewer to reject or accept a review either by using the tool's web interface or by replying to the review assignment email.</t>
<t hangText="o">It must be easy for a reviewer to see when each assigned review is due.</t>
<t hangText="o">A reviewer must be able to configure the tool to remind them when actions are due. (For instance, a reviewer could receive email when a review is about to become overdue).</t>
<t hangText="o">A reviewer must be able to indicate that a review is complete, capturing where the review is in the archives and the high-level, review-result summary.</t>
<t hangText="o">It must be possible for a reviewer to clearly indicate which version of a document was reviewed. Documents are sometimes revised between when a review was assigned and when it is due. The tool should note the current version of the document and highlight when the review is not for the current version.</t>
<t hangText="o">It must be easy for a reviewer to submit a completed review.
<list style="hanging">
<t hangText="-">The current workflow, where the reviewer sends email to the team's email list (possibly copying other lists) and then indicates where to find that review, must continue to be supported. The tool should make it easier to capture the link to review in the team's email list archives (perhaps by suggesting links based on a search into the archives).</t>
<t hangText="-">The tool should allow the reviewer to enter the review into
the tool via a web form (either as directly provided text or through a file-upload mechanism). The tool will ensure the review is posted to the appropriate lists and will construct the links to those posts in the archives.</t>
<t hangText="-">The tool could also allow the reviewer to submit the review to the tool by email (perhaps by replying to the assignment). The tool would then ensure the review is posted to the appropriate lists.</t>
</list></t>
</list></t>
</section>
<section title="Review Requester and Consumer Focused">
<t><list style="hanging">
<t hangText="o">It should be easy for an AD or group chair to request any type of review, but particularly an early review, from a review team.</t>
<t hangText="o">It should be possible for that person to withdraw a review request.</t>
<t hangText="o">It must be easy to find all reviews of a document when looking at the document's main page in the Datatracker. The reference to the review must make it easy to see any responses to the review on the email lists it was sent to. If a document "replaces" one or more other documents, reviews of the replaced documents should be included in the results.</t>
<t hangText="o">It must be easy to find all reviews of a document when looking at search result pages and other lists of documents, such as the documents on an IESG Telechat agenda.</t>
</list></t>
</section>
<section title="Statistics Focused" anchor="statistics">
<t><list style="hanging">
<t hangText="o">It must be easy to see the following across all teams, a given team, or a given reviewer, and independently across all time or across configurable recent periods of time:
<list style="hanging">
<t hangText="-">how many reviews have been completed</t>
<t hangText="-">how many reviews are in progress</t>
<t hangText="-">how many in progress reviews are late</t>
<t hangText="-">how many completed reviews were late</t>
<t hangText="-">how many reviews were not completed at all</t>
<t hangText="-">average time to complete reviews (from assignment to completion)</t>
</list></t>
<t hangText="o">It must be easy to see the following for all teams, for a given team, or for a given reviewer, across all time or across configurable recent periods:
<list style="hanging">
<t hangText="-">total counts of reviews in each review state (done, rejected, etc.)</t>
<t hangText="-">total counts of completed reviews by result (ready, ready with nits, etc.)</t>
</list></t>
<t hangText="o">The above statistics should also be calculated reflecting the size of the documents being reviewed (such as using the number of pages or words in the documents).</t>
<t hangText="o">Where applicable, statistics should take reviewer hiatus periods into account.</t>
<t hangText="o">Access to the above statistics must be easy to configure. Access will be initially limited as follows:
<list style="hanging">
<t hangText="-">The Secretariat and ADs can see any statistic.</t>
<t hangText="-">A team secretary can see any statistics for that team.</t>
<t hangText="-">A reviewer can see any team aggregate statistics or their own reviewer-specific statistics.</t>
</list></t>
<t hangText="o">Where possible, the above statistics should be visible as a time-series graph.</t>
<t hangText="o">The implementation should anticipate future enhancements that would allow ADs to indicate their position was informed by a given review. Such enhancements would allow reporting correlations between reviews and documents that receive one or more "discusses". However, implementing these enhancements is not part of the current project. </t>
</list></t>
</section>
</section>

<section title="Security Considerations">
<t>
This document discusses requirements for tools that assist review teams. These requirements do not affect the security of the Internet in any significant fashion. The tools themselves have authentication and authorization considerations (team secretaries will be able to do different things than reviewers). 
</t>
</section>
</middle>
<back>


<references title="Informative References">

&rfc6385;

<reference anchor="Gen-ART" target="http://trac.tools.ietf.org/area/gen/trac/wiki">
 <front>
  <title>General Area: General Area Review Team (GEN-ART) Guidelines</title>
  <author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

<reference anchor="SecDir" target="http://trac.tools.ietf.org/area/sec/trac/wiki/SecurityDirectorate">
 <front>
  <title>Security Directorate</title>
  <author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

<reference anchor="RTG-dir" target="http://trac.tools.ietf.org/area/rtg/trac/wiki/RtgDir">
 <front>
  <title>Routing Area Directorate Wiki Page</title>
 <author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

<reference anchor="AppsDir" target="http://trac.tools.ietf.org/area/app/trac/wiki/AppsDirReview">
 <front>
  <title>Applications Area Directorate Review Process</title>
<author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

<reference anchor="OPS-dir" target="https://svn.tools.ietf.org/area/ops/trac/wiki/Directorates">
 <front>
  <title>OPS Directorate (OPS-DIR)</title>
<author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

<reference anchor="MIBdoctors" target="http://www.ietf.org/iesg/directorate/mib-doctors.html">
 <front>
  <title>MIB Doctors</title>
<author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

<reference anchor="YANGdoctors" target="http://www.ietf.org/iesg/directorate/yang-doctors.html">
 <front>
  <title>YANG Doctors</title>
<author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

<reference anchor="art-trac" target="https://wiki.tools.ietf.org/tools/art/trac/report/1">
 <front>
  <title>Area Review Team Tool: {1} Active Tickets</title>
<author><organization>IETF</organization></author>
  <date/>
 </front>
</reference>

</references>

<section title="A Starting Point for Django Models Supporting the Review Tool">

<!-- [rfced] Per the IESG statement on formal languages
(https://www.ietf.org/iesg/statement/pseudocode-guidelines.html),
please provide a reference to the Django lanugage 
specification if Appendix A contains Django.
-->
<figure><artwork>
from django.db import models
from ietf.doc.models import Document
from ietf.person.models import Email
from ietf.group.models import Group, Role

from ietf.name.models import NameModel

class ReviewRequestStateName(NameModel):
    """ Requested, Accepted, Rejected, Withdrawn, Overtaken By Events, 
        No Response , Completed  """

class ReviewTypeName(NameModel):
    """ Early Review, Last Call, Telechat """

class ReviewResultName(NameModel):
    """Almost ready, Has issues, Has nits, Not Ready, 
       On the right track, Ready, Ready with issues, 
       Ready with nits, Serious Issues"""

class Reviewer(models.Model):
    """
    These records associate reviewers with review teams and keep track
    of admin data associated with the reviewer in the particular team.
    There will be one record for each combination of reviewer and team.
    """
    role        = models.ForeignKey(Role)
    frequency   = models.IntegerField(help_text=
                                  "Can review every N days")
    available   = models.DateTimeField(blank=True,null=True, help_text=
                        "When will this reviewer be available again")
    filter_re   = models.CharField(blank=True)
    skip_next   = models.IntegerField(help_text=
                         "Skip the next N review assignments")

class ReviewResultSet(models.Model):
    """
    This table provides a way to point out a set of ReviewResultName 
    entries that are valid for a given team in order to be able to 
    limit the result choices that can be set for a given review as a 
    function of which team it is related to.
    """
    team        = models.ForeignKey(Group)
    valid       = models.ManyToManyField(ReviewResultName)

class ReviewRequest(models.Model):
    """
    There should be one ReviewRequest entered for each combination of 
    document, rev, and reviewer.  
    """
    # Fields filled in on the initial record creation:
    time          = models.DateTimeField(auto_now_add=True)
    type          = models.ReviewTypeName()
    doc           = models.ForeignKey(Document, 
                           related_name='review_request_set')
    team          = models.ForeignKey(Group)
    deadline      = models.DateTimeField()
    requested_rev = models.CharField(verbose_name="requested_revision",
                            max_length=16, blank=True)
    state         = models.ForeignKey(ReviewRequestStateName)
    # Fields filled in as reviewer is assigned, and as the review 
    # is uploaded
    reviewer      = models.ForeignKey(Reviewer, null=True, blank=True)
    review        = models.OneToOneField(Document, null=True, 
                                                   blank=True)
    reviewed_rev  = models.CharField(verbose_name="reviewed_revision", 
                                     max_length=16, blank=True)
    result        = models.ForeignKey(ReviewResultName)

</artwork></figure>
</section>

<section title="Suggested Features Deferred for Future Work">
<t>
Brian Carpenter suggested a set of author/editor-focused requirements that were
deferred for another iteration of improvement. These include providing a way for
the editors to acknowledge receipt of the review, potentially
tracking the email conversation between the reviewer and document editor, and
indicating which review topics the editor believes a new revision addresses.
</t>
</section>

<section title="Acknowledgements" numbered="no">
<t>Tero Kivinen and Henrik Levkowetz were instrumental in forming this
set of requirements and in developing the initial Django models in
the appendix.</t>

<t>The following people provided reviews of this document: David Black,
Deborah Brungard, Brian Carpenter, Elwyn Davies, Stephen Farrell,
Joel Halpern, Jonathan Hardwick, Russ Housley, Barry Leiba, Jean
Mahoney, Randy Presuhn, Gunter Van De Velde, and Martin Vigoureux.</t>

</section>

</back>
	
</rfc>
